{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAeG440fB7y3"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_767SpEGD2zJ"
      },
      "source": [
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "**How to Train [Detectron2](https://github.com/facebookresearch/detectron2) Segmentation on a Custom Dataset**\n",
        "\n",
        "The notebook is based on official Detectron2 [colab notebook](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) and it covers:\n",
        "- Python environment setup\n",
        "- Inference using pre-trained models\n",
        "- Download, register and visualize COCO Format Dataset\n",
        "- Configure, train and evaluate model using custom COCO Format Dataset\n",
        "\n",
        "**Preparing a Custom Dataset**\n",
        "\n",
        "In this tutorial, we will utilize an open source computer vision dataset from one of the 100,000+ available on [Roboflow Universe](https://universe.roboflow.com).\n",
        "\n",
        "If you already have your own images (and, optionally, annotations), you can convert your dataset using [Roboflow](https://roboflow.com), a set of tools developers use to build better computer vision models quickly and accurately. 150k+ developers use roboflow for (automatic) annotation, converting dataset formats (like to Detectron2), training, deploying, and improving their datasets/models.\n",
        "\n",
        "Follow [the getting started guide here](https://docs.roboflow.com/quick-start) to create and prepare your own custom dataset. Make sure to select **Instance Segmentation** Option, If you want to create your own dataset on roboflow\n",
        "\n",
        "Useful Dataset Links\n",
        "\n",
        "* [Helmet Instace Segmentation ](https://universe.roboflow.com/computer-vision-hx9i9/helmet_polygon_v2/dataset/4)\n",
        "\n",
        "* [PCB Board Instance Segmentation](https://universe.roboflow.com/chip/pcb_segmentation_yolov7/dataset/17)\n",
        "\n",
        "* [Fire Segmentation Instance Segmentation](https://universe.roboflow.com/fire-instance-segmentation/fire-detection-pr6nj/dataset/1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5DwluqC5ID2"
      },
      "source": [
        "## Before you start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZBUwM3tyFWS"
      },
      "source": [
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator` and set it to `GPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iil0J4nTHHb_",
        "outputId": "5cb62412-fd04-472f-cd51-c1327558c481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Oct 22 10:42:28 2024       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   56C    P8     4W /  N/A |   3636MiB /  4096MiB |     15%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A     10632    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     15132      C   ...\\detectron_env\\python.exe    N/A      |\n",
            "|    0   N/A  N/A     23588      C   ...ltralytics-env\\python.exe    N/A      |\n",
            "|    0   N/A  N/A     26164      C   ...\\detectron_env\\python.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqCNglJXRro5",
        "outputId": "0aa2b469-8b0b-4feb-c2ee-959cda50ea73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "torch:  2.4 ; cuda:  cu118\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DIEKfPKFmW54"
      },
      "outputs": [],
      "source": [
        "# COMMON LIBRARIES\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# DATA SET PREPARATION AND LOADING\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "# VISUALIZATION\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "# CONFIGURATION\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "# EVALUATION\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "# TRAINING\n",
        "from detectron2.engine import DefaultTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFkJOTWvxu6G"
      },
      "source": [
        "## COCO Format Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP3vux5vnCVn"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaohq2orBkCC"
      },
      "source": [
        "We use `football-pitch-segmentation` dataset as example. Feel free to visit [Roboflow Universe](https://universe.roboflow.com/), and select any other Instance Segmentation dataset. Make sure to download the dataset in correct - `COCO Segmentation` format. \n",
        "\n",
        "Structure of your dataset should look like this:\n",
        "\n",
        "```\n",
        "dataset-directory/\n",
        "├─ README.dataset.txt\n",
        "├─ README.roboflow.txt\n",
        "├─ train\n",
        "│  ├─ train-image-1.jpg\n",
        "│  ├─ train-image-1.jpg\n",
        "│  ├─ ...\n",
        "│  └─ _annotations.coco.json\n",
        "├─ test\n",
        "│  ├─ test-image-1.jpg\n",
        "│  ├─ test-image-1.jpg\n",
        "│  ├─ ...\n",
        "│  └─ _annotations.coco.json\n",
        "└─ valid\n",
        "   ├─ valid-image-1.jpg\n",
        "   ├─ valid-image-1.jpg\n",
        "   ├─ ...\n",
        "   └─ _annotations.coco.json\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grFIdy8ynP-7",
        "outputId": "d05be873-4ac3-4d10-d88b-fa053a263c90"
      },
      "outputs": [],
      "source": [
        "dataset = r'E:\\TA AINGGGG\\1. INI PALING FIXX\\Detectron-Mask_RCNN\\dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoB31yi4AoYs"
      },
      "source": [
        "### Register"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HopUGOyW853G"
      },
      "source": [
        "When you use Detectron2, before you actually train the model you need to [register it](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#register-a-coco-format-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KbI2PNEZF3sU"
      },
      "outputs": [],
      "source": [
        "data_dir = \"E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jntOI8GJG2ks"
      },
      "outputs": [],
      "source": [
        "train_json = os.path.join(data_dir, \"train\", \"_annotations.coco.json\")\n",
        "test_json = os.path.join(data_dir, \"test\", \"_annotations.coco.json\")\n",
        "valid_json = os.path.join(data_dir, \"valid\", \"_annotations.coco.json\")\n",
        "\n",
        "DATA_SET_NAME = \"my_dataset\"\n",
        "\n",
        "register_coco_instances(f\"{DATA_SET_NAME}_train\", {}, train_json, os.path.join(data_dir, \"train\"))\n",
        "register_coco_instances(f\"{DATA_SET_NAME}_test\", {}, test_json, os.path.join(data_dir, \"test\"))\n",
        "register_coco_instances(f\"{DATA_SET_NAME}_valid\", {}, valid_json, os.path.join(data_dir, \"valid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Metadata: Metadata(name='my_dataset_train', json_file='E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\\\train\\\\_annotations.coco.json', image_root='E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\\\train', evaluator_type='coco', thing_classes=['defect-crack', 'crack'], thing_dataset_id_to_contiguous_id={0: 0, 1: 1})\n",
            "Number of training samples: 693\n"
          ]
        }
      ],
      "source": [
        "train_metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n",
        "\n",
        "print(\"Train Metadata:\", train_metadata)\n",
        "print(\"Number of training samples:\", len(dataset_dicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOCY1UWNCtnq"
      },
      "source": [
        "We can now confirm that our custom dataset was correctly registered using [MetadataCatalog](https://detectron2.readthedocs.io/en/latest/modules/data.html#detectron2.data.MetadataCatalog)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR8ha4EHCkA-",
        "outputId": "6506603a-3742-43ee-af5d-7f842426d25d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered datasets: ['my_dataset_train', 'my_dataset_test', 'my_dataset_valid']\n"
          ]
        }
      ],
      "source": [
        "registered_datasets = [\n",
        "    data_set for data_set in MetadataCatalog.list()\n",
        "    if data_set.startswith(DATA_SET_NAME)\n",
        "]\n",
        "\n",
        "print(\"Registered datasets:\", registered_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDpU2L3UL922"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Bd_-oCA90a"
      },
      "source": [
        "Let's take a look at single entry from out train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "eE0anblvMGJx",
        "outputId": "b5db94b3-faf0-4c64-9717-fa906c4b76db"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    cv2.imshow(\"Sample\", vis.get_image()[:, :, ::-1])\n",
        "    cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GavGRHy2M7Hb"
      },
      "source": [
        "## Train Model Using Custom COCO Format Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ3g-l56NMOY"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "krCm2L_lNC83"
      },
      "outputs": [],
      "source": [
        "ARCHITECTURE = \"cascade_mask_rcnn_R_50_FPN_3x\"\n",
        "CONFIG_FILE_PATH = f\"Misc/{ARCHITECTURE}.yaml\"\n",
        "BATCH_SIZE = 2\n",
        "IMAGE_SIZE = 640\n",
        "WORKERS = 8\n",
        "BASE_LR = 0.01\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Setting MAX_ITER to 12000\n",
        "MAX_ITER = 12000\n",
        "\n",
        "# OUTPUT DIR\n",
        "OUTPUT_DIR_PATH = os.path.join(\n",
        "    DATA_SET_NAME, \n",
        "    ARCHITECTURE, \n",
        "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
        ")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lxQU8JrgOD73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration set with MAX_ITER: 12000\n",
            "Evaluation will happen every 200 iterations using validation dataset\n"
          ]
        }
      ],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_valid\",)  # Use the validation dataset\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
        "cfg.TEST.EVAL_PERIOD = 200  # Evaluate every 200 iterations\n",
        "cfg.DATALOADER.NUM_WORKERS = WORKERS\n",
        "cfg.SOLVER.IMS_PER_BATCH = BATCH_SIZE\n",
        "cfg.INPUT.MASK_FORMAT = 'bitmask'\n",
        "cfg.INPUT.MIN_SIZE_TEST = IMAGE_SIZE\n",
        "cfg.INPUT.MIN_SIZE_TRAIN = IMAGE_SIZE\n",
        "cfg.SOLVER.BASE_LR = BASE_LR\n",
        "cfg.SOLVER.MAX_ITER = MAX_ITER\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
        "cfg.OUTPUT_DIR = OUTPUT_DIR_PATH\n",
        "\n",
        "print(f\"Configuration set with MAX_ITER: {MAX_ITER}\")\n",
        "print(f\"Evaluation will happen every 200 iterations using validation dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch-_5aCuXWj9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S8y2W2AQvJq",
        "outputId": "36fe2d26-1118-4748-fff1-18a86d873970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/22 11:18:42 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): CascadeROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): ModuleList(\n",
            "      (0-2): 3 x FastRCNNConvFCHead(\n",
            "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "        (fc_relu1): ReLU()\n",
            "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (fc_relu2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (box_predictor): ModuleList(\n",
            "      (0-2): 3 x FastRCNNOutputLayers(\n",
            "        (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:18:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:18:42 d2.data.datasets.coco]: \u001b[0mLoaded 693 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\train\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:18:42 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 693 images left.\n",
            "\u001b[32m[10/22 11:18:42 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
            "\u001b[36m|   category   | #instances   |  category  | #instances   |\n",
            "|:------------:|:-------------|:----------:|:-------------|\n",
            "| defect-crack | 0            |   crack    | 1255         |\n",
            "|              |              |            |              |\n",
            "|    total     | 1255         |            |              |\u001b[0m\n",
            "\u001b[32m[10/22 11:18:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[10/22 11:18:42 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[10/22 11:18:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:18:42 d2.data.common]: \u001b[0mSerializing 693 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:18:42 d2.data.common]: \u001b[0mSerialized dataset takes 2.41 MiB\n",
            "\u001b[32m[10/22 11:18:42 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:18:42 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[10/22 11:18:42 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/Misc/cascade_mask_rcnn_R_50_FPN_3x/144998488/model_final_480dd8.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model_final_480dd8.pkl: 288MB [01:17, 3.70MB/s]                              \n",
            "Skip loading parameter 'roi_heads.box_predictor.0.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.0.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.1.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.1.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.2.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.2.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.0.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.1.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.2.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/22 11:20:00 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\anaconda3\\envs\\detectron_env\\lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/22 11:20:26 d2.utils.events]: \u001b[0m eta: 2:30:59  iter: 19  total_loss: 4.169  loss_cls_stage0: 0.7285  loss_box_reg_stage0: 0.1746  loss_cls_stage1: 0.7218  loss_box_reg_stage1: 0.1358  loss_cls_stage2: 0.7284  loss_box_reg_stage2: 0.1313  loss_mask: 0.6764  loss_rpn_cls: 0.6492  loss_rpn_loc: 0.1412    time: 0.7788  last_time: 0.7364  data_time: 0.3826  last_data_time: 0.0014   lr: 0.00019981  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:20:42 d2.utils.events]: \u001b[0m eta: 2:33:48  iter: 39  total_loss: 2.934  loss_cls_stage0: 0.4319  loss_box_reg_stage0: 0.4257  loss_cls_stage1: 0.3548  loss_box_reg_stage1: 0.4215  loss_cls_stage2: 0.2566  loss_box_reg_stage2: 0.2509  loss_mask: 0.5435  loss_rpn_cls: 0.1384  loss_rpn_loc: 0.07067    time: 0.7774  last_time: 0.7916  data_time: 0.0016  last_data_time: 0.0016   lr: 0.00039961  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:20:57 d2.utils.events]: \u001b[0m eta: 2:35:05  iter: 59  total_loss: 2.881  loss_cls_stage0: 0.3867  loss_box_reg_stage0: 0.4834  loss_cls_stage1: 0.3238  loss_box_reg_stage1: 0.4832  loss_cls_stage2: 0.236  loss_box_reg_stage2: 0.3192  loss_mask: 0.37  loss_rpn_cls: 0.129  loss_rpn_loc: 0.09398    time: 0.7815  last_time: 0.7648  data_time: 0.0016  last_data_time: 0.0014   lr: 0.00059941  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:21:13 d2.utils.events]: \u001b[0m eta: 2:35:17  iter: 79  total_loss: 2.977  loss_cls_stage0: 0.308  loss_box_reg_stage0: 0.5655  loss_cls_stage1: 0.2785  loss_box_reg_stage1: 0.5961  loss_cls_stage2: 0.2189  loss_box_reg_stage2: 0.3868  loss_mask: 0.319  loss_rpn_cls: 0.06845  loss_rpn_loc: 0.09335    time: 0.7820  last_time: 0.7582  data_time: 0.0016  last_data_time: 0.0013   lr: 0.00079921  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:21:29 d2.utils.events]: \u001b[0m eta: 2:35:10  iter: 99  total_loss: 2.88  loss_cls_stage0: 0.2476  loss_box_reg_stage0: 0.4596  loss_cls_stage1: 0.263  loss_box_reg_stage1: 0.7066  loss_cls_stage2: 0.2283  loss_box_reg_stage2: 0.4906  loss_mask: 0.3181  loss_rpn_cls: 0.06558  loss_rpn_loc: 0.09962    time: 0.7822  last_time: 0.7725  data_time: 0.0017  last_data_time: 0.0014   lr: 0.00099901  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:21:45 d2.utils.events]: \u001b[0m eta: 2:35:12  iter: 119  total_loss: 2.982  loss_cls_stage0: 0.25  loss_box_reg_stage0: 0.5535  loss_cls_stage1: 0.2604  loss_box_reg_stage1: 0.6785  loss_cls_stage2: 0.2437  loss_box_reg_stage2: 0.5528  loss_mask: 0.2721  loss_rpn_cls: 0.06171  loss_rpn_loc: 0.08472    time: 0.7836  last_time: 0.8098  data_time: 0.0015  last_data_time: 0.0020   lr: 0.0011988  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:22:00 d2.utils.events]: \u001b[0m eta: 2:35:06  iter: 139  total_loss: 3.307  loss_cls_stage0: 0.2613  loss_box_reg_stage0: 0.5541  loss_cls_stage1: 0.2441  loss_box_reg_stage1: 0.7224  loss_cls_stage2: 0.2435  loss_box_reg_stage2: 0.6081  loss_mask: 0.2872  loss_rpn_cls: 0.06627  loss_rpn_loc: 0.08156    time: 0.7843  last_time: 0.7744  data_time: 0.0015  last_data_time: 0.0020   lr: 0.0013986  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:22:16 d2.utils.events]: \u001b[0m eta: 2:34:55  iter: 159  total_loss: 2.675  loss_cls_stage0: 0.1776  loss_box_reg_stage0: 0.5825  loss_cls_stage1: 0.1935  loss_box_reg_stage1: 0.7203  loss_cls_stage2: 0.1869  loss_box_reg_stage2: 0.4903  loss_mask: 0.252  loss_rpn_cls: 0.04152  loss_rpn_loc: 0.04723    time: 0.7846  last_time: 0.7719  data_time: 0.0019  last_data_time: 0.0018   lr: 0.0015984  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:22:32 d2.utils.events]: \u001b[0m eta: 2:34:57  iter: 179  total_loss: 2.784  loss_cls_stage0: 0.1858  loss_box_reg_stage0: 0.5663  loss_cls_stage1: 0.2025  loss_box_reg_stage1: 0.7373  loss_cls_stage2: 0.1966  loss_box_reg_stage2: 0.546  loss_mask: 0.2487  loss_rpn_cls: 0.04396  loss_rpn_loc: 0.09068    time: 0.7855  last_time: 0.8048  data_time: 0.0016  last_data_time: 0.0016   lr: 0.0017982  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:22:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:22:48 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:22:48 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
            "\u001b[36m|   category   | #instances   |  category  | #instances   |\n",
            "|:------------:|:-------------|:----------:|:-------------|\n",
            "| defect-crack | 0            |   crack    | 209          |\n",
            "|              |              |            |              |\n",
            "|    total     | 209          |            |              |\u001b[0m\n",
            "\u001b[32m[10/22 11:22:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:22:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:22:48 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:22:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:22:48 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:22:48 d2.utils.events]: \u001b[0m eta: 2:34:49  iter: 199  total_loss: 2.915  loss_cls_stage0: 0.2236  loss_box_reg_stage0: 0.5784  loss_cls_stage1: 0.2265  loss_box_reg_stage1: 0.6966  loss_cls_stage2: 0.1948  loss_box_reg_stage2: 0.5006  loss_mask: 0.212  loss_rpn_cls: 0.02285  loss_rpn_loc: 0.08117    time: 0.7861  last_time: 0.7858  data_time: 0.0016  last_data_time: 0.0021   lr: 0.001998  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:23:04 d2.utils.events]: \u001b[0m eta: 2:34:39  iter: 219  total_loss: 3.331  loss_cls_stage0: 0.2274  loss_box_reg_stage0: 0.5812  loss_cls_stage1: 0.2212  loss_box_reg_stage1: 0.7635  loss_cls_stage2: 0.2254  loss_box_reg_stage2: 0.632  loss_mask: 0.2136  loss_rpn_cls: 0.05476  loss_rpn_loc: 0.07864    time: 0.7873  last_time: 0.7956  data_time: 0.0018  last_data_time: 0.0019   lr: 0.0021978  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:23:20 d2.utils.events]: \u001b[0m eta: 2:34:33  iter: 239  total_loss: 2.379  loss_cls_stage0: 0.1957  loss_box_reg_stage0: 0.3973  loss_cls_stage1: 0.183  loss_box_reg_stage1: 0.461  loss_cls_stage2: 0.1618  loss_box_reg_stage2: 0.3537  loss_mask: 0.2437  loss_rpn_cls: 0.06475  loss_rpn_loc: 0.08192    time: 0.7891  last_time: 0.8136  data_time: 0.0025  last_data_time: 0.0022   lr: 0.0023976  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:23:36 d2.utils.events]: \u001b[0m eta: 2:34:26  iter: 259  total_loss: 2.832  loss_cls_stage0: 0.1942  loss_box_reg_stage0: 0.5434  loss_cls_stage1: 0.1896  loss_box_reg_stage1: 0.6109  loss_cls_stage2: 0.2036  loss_box_reg_stage2: 0.6121  loss_mask: 0.2254  loss_rpn_cls: 0.03926  loss_rpn_loc: 0.07038    time: 0.7897  last_time: 0.7787  data_time: 0.0019  last_data_time: 0.0013   lr: 0.0025974  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:23:52 d2.utils.events]: \u001b[0m eta: 2:34:08  iter: 279  total_loss: 2.549  loss_cls_stage0: 0.1561  loss_box_reg_stage0: 0.5155  loss_cls_stage1: 0.172  loss_box_reg_stage1: 0.6219  loss_cls_stage2: 0.1764  loss_box_reg_stage2: 0.5355  loss_mask: 0.1745  loss_rpn_cls: 0.03246  loss_rpn_loc: 0.06126    time: 0.7896  last_time: 0.7710  data_time: 0.0016  last_data_time: 0.0024   lr: 0.0027972  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:24:08 d2.utils.events]: \u001b[0m eta: 2:33:57  iter: 299  total_loss: 2.779  loss_cls_stage0: 0.1884  loss_box_reg_stage0: 0.5851  loss_cls_stage1: 0.2111  loss_box_reg_stage1: 0.6568  loss_cls_stage2: 0.1668  loss_box_reg_stage2: 0.509  loss_mask: 0.2101  loss_rpn_cls: 0.02776  loss_rpn_loc: 0.06226    time: 0.7917  last_time: 0.7903  data_time: 0.0015  last_data_time: 0.0014   lr: 0.002997  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:24:24 d2.utils.events]: \u001b[0m eta: 2:33:50  iter: 319  total_loss: 3.077  loss_cls_stage0: 0.1914  loss_box_reg_stage0: 0.5833  loss_cls_stage1: 0.2108  loss_box_reg_stage1: 0.816  loss_cls_stage2: 0.1931  loss_box_reg_stage2: 0.568  loss_mask: 0.2244  loss_rpn_cls: 0.02843  loss_rpn_loc: 0.09008    time: 0.7922  last_time: 0.7876  data_time: 0.0021  last_data_time: 0.0015   lr: 0.0031968  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:24:40 d2.utils.events]: \u001b[0m eta: 2:33:38  iter: 339  total_loss: 3.031  loss_cls_stage0: 0.2504  loss_box_reg_stage0: 0.5938  loss_cls_stage1: 0.1932  loss_box_reg_stage1: 0.6778  loss_cls_stage2: 0.2097  loss_box_reg_stage2: 0.6095  loss_mask: 0.2131  loss_rpn_cls: 0.03839  loss_rpn_loc: 0.07413    time: 0.7931  last_time: 0.7837  data_time: 0.0023  last_data_time: 0.0012   lr: 0.0033966  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:24:57 d2.utils.events]: \u001b[0m eta: 2:33:39  iter: 359  total_loss: 2.954  loss_cls_stage0: 0.2175  loss_box_reg_stage0: 0.5651  loss_cls_stage1: 0.2143  loss_box_reg_stage1: 0.6933  loss_cls_stage2: 0.1825  loss_box_reg_stage2: 0.5537  loss_mask: 0.1961  loss_rpn_cls: 0.04878  loss_rpn_loc: 0.07652    time: 0.7946  last_time: 0.8201  data_time: 0.0020  last_data_time: 0.0023   lr: 0.0035964  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:25:13 d2.utils.events]: \u001b[0m eta: 2:33:31  iter: 379  total_loss: 2.93  loss_cls_stage0: 0.1457  loss_box_reg_stage0: 0.4926  loss_cls_stage1: 0.1383  loss_box_reg_stage1: 0.7483  loss_cls_stage2: 0.163  loss_box_reg_stage2: 0.7447  loss_mask: 0.2012  loss_rpn_cls: 0.02474  loss_rpn_loc: 0.1151    time: 0.7959  last_time: 0.8050  data_time: 0.0034  last_data_time: 0.0057   lr: 0.0037962  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:25:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:25:29 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:25:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:25:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:25:29 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:25:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:25:29 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:25:29 d2.utils.events]: \u001b[0m eta: 2:33:22  iter: 399  total_loss: 2.599  loss_cls_stage0: 0.1279  loss_box_reg_stage0: 0.4612  loss_cls_stage1: 0.1328  loss_box_reg_stage1: 0.7436  loss_cls_stage2: 0.152  loss_box_reg_stage2: 0.6413  loss_mask: 0.2254  loss_rpn_cls: 0.02929  loss_rpn_loc: 0.07669    time: 0.7961  last_time: 0.7913  data_time: 0.0016  last_data_time: 0.0020   lr: 0.003996  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:25:45 d2.utils.events]: \u001b[0m eta: 2:33:07  iter: 419  total_loss: 2.634  loss_cls_stage0: 0.1238  loss_box_reg_stage0: 0.5378  loss_cls_stage1: 0.146  loss_box_reg_stage1: 0.6636  loss_cls_stage2: 0.1568  loss_box_reg_stage2: 0.6442  loss_mask: 0.1854  loss_rpn_cls: 0.02564  loss_rpn_loc: 0.08277    time: 0.7966  last_time: 0.7978  data_time: 0.0021  last_data_time: 0.0021   lr: 0.0041958  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:26:01 d2.utils.events]: \u001b[0m eta: 2:32:51  iter: 439  total_loss: 2.788  loss_cls_stage0: 0.1586  loss_box_reg_stage0: 0.4856  loss_cls_stage1: 0.1696  loss_box_reg_stage1: 0.7106  loss_cls_stage2: 0.157  loss_box_reg_stage2: 0.7674  loss_mask: 0.1889  loss_rpn_cls: 0.03364  loss_rpn_loc: 0.05689    time: 0.7965  last_time: 0.7869  data_time: 0.0029  last_data_time: 0.0031   lr: 0.0043956  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:26:17 d2.utils.events]: \u001b[0m eta: 2:32:38  iter: 459  total_loss: 2.672  loss_cls_stage0: 0.1723  loss_box_reg_stage0: 0.4502  loss_cls_stage1: 0.1663  loss_box_reg_stage1: 0.679  loss_cls_stage2: 0.1884  loss_box_reg_stage2: 0.613  loss_mask: 0.2129  loss_rpn_cls: 0.02901  loss_rpn_loc: 0.09701    time: 0.7967  last_time: 0.7748  data_time: 0.0018  last_data_time: 0.0016   lr: 0.0045954  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:26:33 d2.utils.events]: \u001b[0m eta: 2:32:22  iter: 479  total_loss: 2.65  loss_cls_stage0: 0.187  loss_box_reg_stage0: 0.4545  loss_cls_stage1: 0.1847  loss_box_reg_stage1: 0.6769  loss_cls_stage2: 0.1768  loss_box_reg_stage2: 0.6347  loss_mask: 0.184  loss_rpn_cls: 0.02828  loss_rpn_loc: 0.06549    time: 0.7970  last_time: 0.8077  data_time: 0.0017  last_data_time: 0.0015   lr: 0.0047952  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:26:49 d2.utils.events]: \u001b[0m eta: 2:32:06  iter: 499  total_loss: 2.93  loss_cls_stage0: 0.1614  loss_box_reg_stage0: 0.5182  loss_cls_stage1: 0.1497  loss_box_reg_stage1: 0.7383  loss_cls_stage2: 0.1718  loss_box_reg_stage2: 0.585  loss_mask: 0.2275  loss_rpn_cls: 0.03879  loss_rpn_loc: 0.08185    time: 0.7969  last_time: 0.7907  data_time: 0.0017  last_data_time: 0.0014   lr: 0.004995  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:27:06 d2.utils.events]: \u001b[0m eta: 2:31:52  iter: 519  total_loss: 2.916  loss_cls_stage0: 0.158  loss_box_reg_stage0: 0.5214  loss_cls_stage1: 0.1623  loss_box_reg_stage1: 0.7253  loss_cls_stage2: 0.1605  loss_box_reg_stage2: 0.7516  loss_mask: 0.2041  loss_rpn_cls: 0.03084  loss_rpn_loc: 0.06987    time: 0.7984  last_time: 0.8197  data_time: 0.0026  last_data_time: 0.0031   lr: 0.0051948  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:27:23 d2.utils.events]: \u001b[0m eta: 2:31:41  iter: 539  total_loss: 2.409  loss_cls_stage0: 0.1834  loss_box_reg_stage0: 0.4569  loss_cls_stage1: 0.1798  loss_box_reg_stage1: 0.5634  loss_cls_stage2: 0.1635  loss_box_reg_stage2: 0.4894  loss_mask: 0.2073  loss_rpn_cls: 0.03599  loss_rpn_loc: 0.1193    time: 0.7996  last_time: 0.7872  data_time: 0.0028  last_data_time: 0.0022   lr: 0.0053946  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:27:39 d2.utils.events]: \u001b[0m eta: 2:31:26  iter: 559  total_loss: 2.618  loss_cls_stage0: 0.1705  loss_box_reg_stage0: 0.5014  loss_cls_stage1: 0.1851  loss_box_reg_stage1: 0.6899  loss_cls_stage2: 0.1604  loss_box_reg_stage2: 0.5194  loss_mask: 0.2072  loss_rpn_cls: 0.041  loss_rpn_loc: 0.06894    time: 0.7997  last_time: 0.7955  data_time: 0.0018  last_data_time: 0.0010   lr: 0.0055944  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:27:55 d2.utils.events]: \u001b[0m eta: 2:31:13  iter: 579  total_loss: 2.659  loss_cls_stage0: 0.1779  loss_box_reg_stage0: 0.5129  loss_cls_stage1: 0.169  loss_box_reg_stage1: 0.6507  loss_cls_stage2: 0.1552  loss_box_reg_stage2: 0.594  loss_mask: 0.2021  loss_rpn_cls: 0.03932  loss_rpn_loc: 0.1011    time: 0.8005  last_time: 0.8554  data_time: 0.0035  last_data_time: 0.0084   lr: 0.0057942  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:28:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:28:11 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:28:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:28:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:28:11 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:28:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:28:11 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:28:11 d2.utils.events]: \u001b[0m eta: 2:30:58  iter: 599  total_loss: 2.642  loss_cls_stage0: 0.1525  loss_box_reg_stage0: 0.464  loss_cls_stage1: 0.1537  loss_box_reg_stage1: 0.6718  loss_cls_stage2: 0.1911  loss_box_reg_stage2: 0.6609  loss_mask: 0.2071  loss_rpn_cls: 0.03149  loss_rpn_loc: 0.0638    time: 0.8008  last_time: 0.7660  data_time: 0.0020  last_data_time: 0.0026   lr: 0.005994  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:28:27 d2.utils.events]: \u001b[0m eta: 2:30:40  iter: 619  total_loss: 2.697  loss_cls_stage0: 0.142  loss_box_reg_stage0: 0.5357  loss_cls_stage1: 0.1831  loss_box_reg_stage1: 0.6953  loss_cls_stage2: 0.1703  loss_box_reg_stage2: 0.6796  loss_mask: 0.2362  loss_rpn_cls: 0.04759  loss_rpn_loc: 0.0749    time: 0.8004  last_time: 0.7728  data_time: 0.0018  last_data_time: 0.0020   lr: 0.0061938  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:28:43 d2.utils.events]: \u001b[0m eta: 2:30:22  iter: 639  total_loss: 2.99  loss_cls_stage0: 0.2039  loss_box_reg_stage0: 0.5636  loss_cls_stage1: 0.1915  loss_box_reg_stage1: 0.7343  loss_cls_stage2: 0.1725  loss_box_reg_stage2: 0.6022  loss_mask: 0.2102  loss_rpn_cls: 0.03524  loss_rpn_loc: 0.04852    time: 0.8002  last_time: 0.7898  data_time: 0.0016  last_data_time: 0.0014   lr: 0.0063936  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:28:59 d2.utils.events]: \u001b[0m eta: 2:30:06  iter: 659  total_loss: 3.048  loss_cls_stage0: 0.1775  loss_box_reg_stage0: 0.5448  loss_cls_stage1: 0.1866  loss_box_reg_stage1: 0.7888  loss_cls_stage2: 0.1955  loss_box_reg_stage2: 0.6402  loss_mask: 0.1828  loss_rpn_cls: 0.03044  loss_rpn_loc: 0.06081    time: 0.8000  last_time: 0.7887  data_time: 0.0017  last_data_time: 0.0020   lr: 0.0065934  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:29:15 d2.utils.events]: \u001b[0m eta: 2:29:52  iter: 679  total_loss: 2.377  loss_cls_stage0: 0.1632  loss_box_reg_stage0: 0.4067  loss_cls_stage1: 0.1546  loss_box_reg_stage1: 0.6147  loss_cls_stage2: 0.1617  loss_box_reg_stage2: 0.599  loss_mask: 0.1882  loss_rpn_cls: 0.02867  loss_rpn_loc: 0.07311    time: 0.8001  last_time: 0.7991  data_time: 0.0019  last_data_time: 0.0010   lr: 0.0067932  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:29:30 d2.utils.events]: \u001b[0m eta: 2:29:30  iter: 699  total_loss: 2.59  loss_cls_stage0: 0.1585  loss_box_reg_stage0: 0.4503  loss_cls_stage1: 0.1716  loss_box_reg_stage1: 0.6623  loss_cls_stage2: 0.1603  loss_box_reg_stage2: 0.547  loss_mask: 0.2296  loss_rpn_cls: 0.04824  loss_rpn_loc: 0.1238    time: 0.7993  last_time: 0.7706  data_time: 0.0016  last_data_time: 0.0013   lr: 0.006993  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:29:46 d2.utils.events]: \u001b[0m eta: 2:29:11  iter: 719  total_loss: 2.637  loss_cls_stage0: 0.1314  loss_box_reg_stage0: 0.4903  loss_cls_stage1: 0.1132  loss_box_reg_stage1: 0.739  loss_cls_stage2: 0.1623  loss_box_reg_stage2: 0.6951  loss_mask: 0.1886  loss_rpn_cls: 0.03376  loss_rpn_loc: 0.08784    time: 0.7984  last_time: 0.7657  data_time: 0.0014  last_data_time: 0.0012   lr: 0.0071928  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:30:01 d2.utils.events]: \u001b[0m eta: 2:28:53  iter: 739  total_loss: 2.235  loss_cls_stage0: 0.1439  loss_box_reg_stage0: 0.4294  loss_cls_stage1: 0.1475  loss_box_reg_stage1: 0.5245  loss_cls_stage2: 0.1536  loss_box_reg_stage2: 0.564  loss_mask: 0.1937  loss_rpn_cls: 0.03775  loss_rpn_loc: 0.08576    time: 0.7978  last_time: 0.7737  data_time: 0.0015  last_data_time: 0.0015   lr: 0.0073926  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:30:17 d2.utils.events]: \u001b[0m eta: 2:28:30  iter: 759  total_loss: 2.783  loss_cls_stage0: 0.1388  loss_box_reg_stage0: 0.4863  loss_cls_stage1: 0.1609  loss_box_reg_stage1: 0.7278  loss_cls_stage2: 0.2036  loss_box_reg_stage2: 0.6316  loss_mask: 0.1701  loss_rpn_cls: 0.0426  loss_rpn_loc: 0.06657    time: 0.7971  last_time: 0.7855  data_time: 0.0016  last_data_time: 0.0015   lr: 0.0075924  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:30:33 d2.utils.events]: \u001b[0m eta: 2:28:11  iter: 779  total_loss: 2.687  loss_cls_stage0: 0.1764  loss_box_reg_stage0: 0.5031  loss_cls_stage1: 0.1565  loss_box_reg_stage1: 0.724  loss_cls_stage2: 0.1622  loss_box_reg_stage2: 0.6285  loss_mask: 0.19  loss_rpn_cls: 0.03546  loss_rpn_loc: 0.06403    time: 0.7968  last_time: 0.8086  data_time: 0.0019  last_data_time: 0.0055   lr: 0.0077922  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:30:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:30:48 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:30:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:30:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:30:48 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:30:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:30:48 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:30:48 d2.utils.events]: \u001b[0m eta: 2:27:54  iter: 799  total_loss: 2.488  loss_cls_stage0: 0.1654  loss_box_reg_stage0: 0.4254  loss_cls_stage1: 0.1823  loss_box_reg_stage1: 0.5879  loss_cls_stage2: 0.1691  loss_box_reg_stage2: 0.5275  loss_mask: 0.223  loss_rpn_cls: 0.04458  loss_rpn_loc: 0.1031    time: 0.7963  last_time: 0.7614  data_time: 0.0027  last_data_time: 0.0017   lr: 0.007992  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:31:04 d2.utils.events]: \u001b[0m eta: 2:27:34  iter: 819  total_loss: 2.452  loss_cls_stage0: 0.1421  loss_box_reg_stage0: 0.4642  loss_cls_stage1: 0.1309  loss_box_reg_stage1: 0.613  loss_cls_stage2: 0.1454  loss_box_reg_stage2: 0.6201  loss_mask: 0.225  loss_rpn_cls: 0.037  loss_rpn_loc: 0.07536    time: 0.7957  last_time: 0.7722  data_time: 0.0017  last_data_time: 0.0018   lr: 0.0081918  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:31:19 d2.utils.events]: \u001b[0m eta: 2:27:16  iter: 839  total_loss: 2.437  loss_cls_stage0: 0.1567  loss_box_reg_stage0: 0.4426  loss_cls_stage1: 0.1525  loss_box_reg_stage1: 0.5576  loss_cls_stage2: 0.1741  loss_box_reg_stage2: 0.5609  loss_mask: 0.1887  loss_rpn_cls: 0.03498  loss_rpn_loc: 0.06601    time: 0.7955  last_time: 0.8462  data_time: 0.0036  last_data_time: 0.0408   lr: 0.0083916  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:31:35 d2.utils.events]: \u001b[0m eta: 2:26:55  iter: 859  total_loss: 2.396  loss_cls_stage0: 0.1246  loss_box_reg_stage0: 0.4839  loss_cls_stage1: 0.1691  loss_box_reg_stage1: 0.6213  loss_cls_stage2: 0.1645  loss_box_reg_stage2: 0.5552  loss_mask: 0.2096  loss_rpn_cls: 0.03187  loss_rpn_loc: 0.09453    time: 0.7950  last_time: 0.7870  data_time: 0.0016  last_data_time: 0.0017   lr: 0.0085914  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:31:50 d2.utils.events]: \u001b[0m eta: 2:26:36  iter: 879  total_loss: 2.569  loss_cls_stage0: 0.1612  loss_box_reg_stage0: 0.5504  loss_cls_stage1: 0.156  loss_box_reg_stage1: 0.6196  loss_cls_stage2: 0.1677  loss_box_reg_stage2: 0.6502  loss_mask: 0.2127  loss_rpn_cls: 0.03884  loss_rpn_loc: 0.08204    time: 0.7946  last_time: 0.7809  data_time: 0.0018  last_data_time: 0.0014   lr: 0.0087912  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:32:06 d2.utils.events]: \u001b[0m eta: 2:26:12  iter: 899  total_loss: 2.653  loss_cls_stage0: 0.1386  loss_box_reg_stage0: 0.4935  loss_cls_stage1: 0.1369  loss_box_reg_stage1: 0.6952  loss_cls_stage2: 0.1536  loss_box_reg_stage2: 0.6932  loss_mask: 0.2136  loss_rpn_cls: 0.02339  loss_rpn_loc: 0.05663    time: 0.7942  last_time: 0.7736  data_time: 0.0018  last_data_time: 0.0013   lr: 0.008991  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:32:21 d2.utils.events]: \u001b[0m eta: 2:25:49  iter: 919  total_loss: 2.368  loss_cls_stage0: 0.1782  loss_box_reg_stage0: 0.4471  loss_cls_stage1: 0.1772  loss_box_reg_stage1: 0.609  loss_cls_stage2: 0.156  loss_box_reg_stage2: 0.568  loss_mask: 0.183  loss_rpn_cls: 0.04421  loss_rpn_loc: 0.0702    time: 0.7937  last_time: 0.7634  data_time: 0.0015  last_data_time: 0.0017   lr: 0.0091908  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:32:37 d2.utils.events]: \u001b[0m eta: 2:25:31  iter: 939  total_loss: 2.78  loss_cls_stage0: 0.1716  loss_box_reg_stage0: 0.5241  loss_cls_stage1: 0.1567  loss_box_reg_stage1: 0.6939  loss_cls_stage2: 0.1798  loss_box_reg_stage2: 0.5594  loss_mask: 0.2094  loss_rpn_cls: 0.05106  loss_rpn_loc: 0.1008    time: 0.7935  last_time: 0.7640  data_time: 0.0022  last_data_time: 0.0013   lr: 0.0093906  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:32:53 d2.utils.events]: \u001b[0m eta: 2:25:14  iter: 959  total_loss: 2.372  loss_cls_stage0: 0.152  loss_box_reg_stage0: 0.4641  loss_cls_stage1: 0.1352  loss_box_reg_stage1: 0.4803  loss_cls_stage2: 0.1241  loss_box_reg_stage2: 0.4942  loss_mask: 0.2101  loss_rpn_cls: 0.04775  loss_rpn_loc: 0.1225    time: 0.7935  last_time: 0.7790  data_time: 0.0031  last_data_time: 0.0014   lr: 0.0095904  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:33:09 d2.utils.events]: \u001b[0m eta: 2:24:57  iter: 979  total_loss: 2.527  loss_cls_stage0: 0.1355  loss_box_reg_stage0: 0.4856  loss_cls_stage1: 0.1236  loss_box_reg_stage1: 0.6642  loss_cls_stage2: 0.1425  loss_box_reg_stage2: 0.5058  loss_mask: 0.1723  loss_rpn_cls: 0.05793  loss_rpn_loc: 0.06603    time: 0.7933  last_time: 0.7554  data_time: 0.0019  last_data_time: 0.0013   lr: 0.0097902  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:33:24 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:33:24 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:33:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:33:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:33:24 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:33:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:33:24 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:33:24 d2.utils.events]: \u001b[0m eta: 2:24:37  iter: 999  total_loss: 2.399  loss_cls_stage0: 0.1446  loss_box_reg_stage0: 0.4155  loss_cls_stage1: 0.1523  loss_box_reg_stage1: 0.5887  loss_cls_stage2: 0.1461  loss_box_reg_stage2: 0.5669  loss_mask: 0.1782  loss_rpn_cls: 0.0473  loss_rpn_loc: 0.08072    time: 0.7927  last_time: 0.7765  data_time: 0.0016  last_data_time: 0.0013   lr: 0.00999  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:33:39 d2.utils.events]: \u001b[0m eta: 2:24:19  iter: 1019  total_loss: 2.621  loss_cls_stage0: 0.1227  loss_box_reg_stage0: 0.5116  loss_cls_stage1: 0.1431  loss_box_reg_stage1: 0.6335  loss_cls_stage2: 0.1574  loss_box_reg_stage2: 0.7204  loss_mask: 0.1823  loss_rpn_cls: 0.02538  loss_rpn_loc: 0.07805    time: 0.7923  last_time: 0.7717  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:33:55 d2.utils.events]: \u001b[0m eta: 2:24:02  iter: 1039  total_loss: 2.762  loss_cls_stage0: 0.1574  loss_box_reg_stage0: 0.5425  loss_cls_stage1: 0.1685  loss_box_reg_stage1: 0.7002  loss_cls_stage2: 0.1995  loss_box_reg_stage2: 0.5963  loss_mask: 0.2031  loss_rpn_cls: 0.0318  loss_rpn_loc: 0.06504    time: 0.7919  last_time: 0.7743  data_time: 0.0015  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:34:10 d2.utils.events]: \u001b[0m eta: 2:23:43  iter: 1059  total_loss: 2.561  loss_cls_stage0: 0.1476  loss_box_reg_stage0: 0.5259  loss_cls_stage1: 0.1828  loss_box_reg_stage1: 0.627  loss_cls_stage2: 0.1794  loss_box_reg_stage2: 0.606  loss_mask: 0.2108  loss_rpn_cls: 0.02951  loss_rpn_loc: 0.09693    time: 0.7915  last_time: 0.7812  data_time: 0.0015  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:34:26 d2.utils.events]: \u001b[0m eta: 2:23:21  iter: 1079  total_loss: 2.67  loss_cls_stage0: 0.1563  loss_box_reg_stage0: 0.4923  loss_cls_stage1: 0.1623  loss_box_reg_stage1: 0.5648  loss_cls_stage2: 0.1719  loss_box_reg_stage2: 0.6152  loss_mask: 0.1935  loss_rpn_cls: 0.03626  loss_rpn_loc: 0.09353    time: 0.7911  last_time: 0.7879  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:34:41 d2.utils.events]: \u001b[0m eta: 2:23:04  iter: 1099  total_loss: 2.508  loss_cls_stage0: 0.1429  loss_box_reg_stage0: 0.4782  loss_cls_stage1: 0.1236  loss_box_reg_stage1: 0.5611  loss_cls_stage2: 0.1451  loss_box_reg_stage2: 0.6711  loss_mask: 0.1614  loss_rpn_cls: 0.0384  loss_rpn_loc: 0.09787    time: 0.7908  last_time: 0.7635  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:34:57 d2.utils.events]: \u001b[0m eta: 2:22:45  iter: 1119  total_loss: 2.513  loss_cls_stage0: 0.1176  loss_box_reg_stage0: 0.4364  loss_cls_stage1: 0.1337  loss_box_reg_stage1: 0.6415  loss_cls_stage2: 0.1435  loss_box_reg_stage2: 0.6677  loss_mask: 0.167  loss_rpn_cls: 0.04608  loss_rpn_loc: 0.09524    time: 0.7904  last_time: 0.7635  data_time: 0.0015  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:35:12 d2.utils.events]: \u001b[0m eta: 2:22:26  iter: 1139  total_loss: 2.697  loss_cls_stage0: 0.1425  loss_box_reg_stage0: 0.4681  loss_cls_stage1: 0.1612  loss_box_reg_stage1: 0.6582  loss_cls_stage2: 0.1795  loss_box_reg_stage2: 0.5476  loss_mask: 0.2103  loss_rpn_cls: 0.0386  loss_rpn_loc: 0.07392    time: 0.7901  last_time: 0.7738  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:35:28 d2.utils.events]: \u001b[0m eta: 2:22:09  iter: 1159  total_loss: 2.314  loss_cls_stage0: 0.1102  loss_box_reg_stage0: 0.4915  loss_cls_stage1: 0.1202  loss_box_reg_stage1: 0.5642  loss_cls_stage2: 0.1386  loss_box_reg_stage2: 0.589  loss_mask: 0.1878  loss_rpn_cls: 0.03799  loss_rpn_loc: 0.1121    time: 0.7904  last_time: 0.9299  data_time: 0.0025  last_data_time: 0.0066   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:35:45 d2.utils.events]: \u001b[0m eta: 2:21:55  iter: 1179  total_loss: 2.265  loss_cls_stage0: 0.1242  loss_box_reg_stage0: 0.4017  loss_cls_stage1: 0.1175  loss_box_reg_stage1: 0.6052  loss_cls_stage2: 0.1196  loss_box_reg_stage2: 0.5185  loss_mask: 0.186  loss_rpn_cls: 0.03971  loss_rpn_loc: 0.07915    time: 0.7911  last_time: 0.7734  data_time: 0.0048  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:36:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:36:01 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:36:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:36:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:36:01 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:36:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:36:01 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:36:01 d2.utils.events]: \u001b[0m eta: 2:21:36  iter: 1199  total_loss: 2.855  loss_cls_stage0: 0.1207  loss_box_reg_stage0: 0.5454  loss_cls_stage1: 0.1404  loss_box_reg_stage1: 0.7305  loss_cls_stage2: 0.174  loss_box_reg_stage2: 0.6768  loss_mask: 0.2285  loss_rpn_cls: 0.0357  loss_rpn_loc: 0.07165    time: 0.7911  last_time: 0.7947  data_time: 0.0020  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:36:17 d2.utils.events]: \u001b[0m eta: 2:21:17  iter: 1219  total_loss: 2.642  loss_cls_stage0: 0.1184  loss_box_reg_stage0: 0.4703  loss_cls_stage1: 0.1362  loss_box_reg_stage1: 0.6676  loss_cls_stage2: 0.1384  loss_box_reg_stage2: 0.6539  loss_mask: 0.1882  loss_rpn_cls: 0.02639  loss_rpn_loc: 0.08683    time: 0.7914  last_time: 0.8180  data_time: 0.0024  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:36:33 d2.utils.events]: \u001b[0m eta: 2:21:00  iter: 1239  total_loss: 2.595  loss_cls_stage0: 0.1186  loss_box_reg_stage0: 0.4718  loss_cls_stage1: 0.1462  loss_box_reg_stage1: 0.6367  loss_cls_stage2: 0.1637  loss_box_reg_stage2: 0.6179  loss_mask: 0.2045  loss_rpn_cls: 0.02422  loss_rpn_loc: 0.09291    time: 0.7916  last_time: 0.8128  data_time: 0.0019  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:36:49 d2.utils.events]: \u001b[0m eta: 2:20:38  iter: 1259  total_loss: 2.27  loss_cls_stage0: 0.1456  loss_box_reg_stage0: 0.4175  loss_cls_stage1: 0.1489  loss_box_reg_stage1: 0.4707  loss_cls_stage2: 0.1465  loss_box_reg_stage2: 0.5  loss_mask: 0.169  loss_rpn_cls: 0.03935  loss_rpn_loc: 0.06516    time: 0.7914  last_time: 0.7714  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:37:04 d2.utils.events]: \u001b[0m eta: 2:20:19  iter: 1279  total_loss: 2.54  loss_cls_stage0: 0.1256  loss_box_reg_stage0: 0.4769  loss_cls_stage1: 0.1323  loss_box_reg_stage1: 0.6456  loss_cls_stage2: 0.1337  loss_box_reg_stage2: 0.6724  loss_mask: 0.1863  loss_rpn_cls: 0.03434  loss_rpn_loc: 0.08958    time: 0.7911  last_time: 0.7732  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:37:20 d2.utils.events]: \u001b[0m eta: 2:19:57  iter: 1299  total_loss: 2.536  loss_cls_stage0: 0.1271  loss_box_reg_stage0: 0.438  loss_cls_stage1: 0.1368  loss_box_reg_stage1: 0.6255  loss_cls_stage2: 0.1615  loss_box_reg_stage2: 0.7153  loss_mask: 0.1681  loss_rpn_cls: 0.02955  loss_rpn_loc: 0.07041    time: 0.7911  last_time: 0.7980  data_time: 0.0024  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:37:35 d2.utils.events]: \u001b[0m eta: 2:19:33  iter: 1319  total_loss: 2.409  loss_cls_stage0: 0.07712  loss_box_reg_stage0: 0.4409  loss_cls_stage1: 0.09983  loss_box_reg_stage1: 0.661  loss_cls_stage2: 0.1253  loss_box_reg_stage2: 0.6611  loss_mask: 0.1825  loss_rpn_cls: 0.02292  loss_rpn_loc: 0.0936    time: 0.7909  last_time: 0.7705  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:37:51 d2.utils.events]: \u001b[0m eta: 2:19:16  iter: 1339  total_loss: 2.218  loss_cls_stage0: 0.1102  loss_box_reg_stage0: 0.4046  loss_cls_stage1: 0.1102  loss_box_reg_stage1: 0.5265  loss_cls_stage2: 0.1287  loss_box_reg_stage2: 0.5793  loss_mask: 0.177  loss_rpn_cls: 0.0289  loss_rpn_loc: 0.07086    time: 0.7910  last_time: 0.7671  data_time: 0.0026  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:38:07 d2.utils.events]: \u001b[0m eta: 2:18:55  iter: 1359  total_loss: 2.802  loss_cls_stage0: 0.1409  loss_box_reg_stage0: 0.4401  loss_cls_stage1: 0.1853  loss_box_reg_stage1: 0.7131  loss_cls_stage2: 0.1946  loss_box_reg_stage2: 0.662  loss_mask: 0.175  loss_rpn_cls: 0.03333  loss_rpn_loc: 0.08276    time: 0.7909  last_time: 0.7791  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:38:23 d2.utils.events]: \u001b[0m eta: 2:18:32  iter: 1379  total_loss: 2.794  loss_cls_stage0: 0.1578  loss_box_reg_stage0: 0.5759  loss_cls_stage1: 0.183  loss_box_reg_stage1: 0.6732  loss_cls_stage2: 0.162  loss_box_reg_stage2: 0.6122  loss_mask: 0.2114  loss_rpn_cls: 0.02976  loss_rpn_loc: 0.08041    time: 0.7910  last_time: 0.8008  data_time: 0.0020  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:38:39 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:38:39 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:38:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:38:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:38:39 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:38:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:38:39 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:38:39 d2.utils.events]: \u001b[0m eta: 2:18:16  iter: 1399  total_loss: 2.37  loss_cls_stage0: 0.1079  loss_box_reg_stage0: 0.4142  loss_cls_stage1: 0.09887  loss_box_reg_stage1: 0.6304  loss_cls_stage2: 0.1237  loss_box_reg_stage2: 0.6873  loss_mask: 0.1784  loss_rpn_cls: 0.02465  loss_rpn_loc: 0.08552    time: 0.7911  last_time: 0.7677  data_time: 0.0023  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:38:55 d2.utils.events]: \u001b[0m eta: 2:17:52  iter: 1419  total_loss: 2.106  loss_cls_stage0: 0.1483  loss_box_reg_stage0: 0.4176  loss_cls_stage1: 0.1535  loss_box_reg_stage1: 0.5108  loss_cls_stage2: 0.1563  loss_box_reg_stage2: 0.5328  loss_mask: 0.1536  loss_rpn_cls: 0.03432  loss_rpn_loc: 0.05882    time: 0.7910  last_time: 0.7686  data_time: 0.0018  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:39:10 d2.utils.events]: \u001b[0m eta: 2:17:29  iter: 1439  total_loss: 2.721  loss_cls_stage0: 0.1333  loss_box_reg_stage0: 0.5254  loss_cls_stage1: 0.1207  loss_box_reg_stage1: 0.6592  loss_cls_stage2: 0.1372  loss_box_reg_stage2: 0.7333  loss_mask: 0.1835  loss_rpn_cls: 0.03502  loss_rpn_loc: 0.1034    time: 0.7908  last_time: 0.7813  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:39:26 d2.utils.events]: \u001b[0m eta: 2:17:09  iter: 1459  total_loss: 2.354  loss_cls_stage0: 0.09462  loss_box_reg_stage0: 0.4039  loss_cls_stage1: 0.1156  loss_box_reg_stage1: 0.6228  loss_cls_stage2: 0.1433  loss_box_reg_stage2: 0.6661  loss_mask: 0.1684  loss_rpn_cls: 0.01997  loss_rpn_loc: 0.06398    time: 0.7906  last_time: 0.8020  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:39:41 d2.utils.events]: \u001b[0m eta: 2:16:48  iter: 1479  total_loss: 2.33  loss_cls_stage0: 0.1047  loss_box_reg_stage0: 0.4433  loss_cls_stage1: 0.1118  loss_box_reg_stage1: 0.6399  loss_cls_stage2: 0.1334  loss_box_reg_stage2: 0.594  loss_mask: 0.1401  loss_rpn_cls: 0.02146  loss_rpn_loc: 0.05798    time: 0.7903  last_time: 0.7728  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:39:57 d2.utils.events]: \u001b[0m eta: 2:16:25  iter: 1499  total_loss: 2.342  loss_cls_stage0: 0.1089  loss_box_reg_stage0: 0.4131  loss_cls_stage1: 0.1191  loss_box_reg_stage1: 0.6207  loss_cls_stage2: 0.1068  loss_box_reg_stage2: 0.7064  loss_mask: 0.1843  loss_rpn_cls: 0.01945  loss_rpn_loc: 0.0798    time: 0.7900  last_time: 0.7651  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:40:12 d2.utils.events]: \u001b[0m eta: 2:16:04  iter: 1519  total_loss: 2.808  loss_cls_stage0: 0.1126  loss_box_reg_stage0: 0.4495  loss_cls_stage1: 0.1598  loss_box_reg_stage1: 0.6906  loss_cls_stage2: 0.1742  loss_box_reg_stage2: 0.729  loss_mask: 0.1849  loss_rpn_cls: 0.02411  loss_rpn_loc: 0.09326    time: 0.7897  last_time: 0.7714  data_time: 0.0014  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:40:28 d2.utils.events]: \u001b[0m eta: 2:15:39  iter: 1539  total_loss: 2.177  loss_cls_stage0: 0.08881  loss_box_reg_stage0: 0.4044  loss_cls_stage1: 0.1005  loss_box_reg_stage1: 0.596  loss_cls_stage2: 0.1198  loss_box_reg_stage2: 0.595  loss_mask: 0.1948  loss_rpn_cls: 0.02634  loss_rpn_loc: 0.07141    time: 0.7896  last_time: 0.7746  data_time: 0.0020  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:40:44 d2.utils.events]: \u001b[0m eta: 2:15:21  iter: 1559  total_loss: 2.763  loss_cls_stage0: 0.1169  loss_box_reg_stage0: 0.4633  loss_cls_stage1: 0.1206  loss_box_reg_stage1: 0.7145  loss_cls_stage2: 0.141  loss_box_reg_stage2: 0.7805  loss_mask: 0.1818  loss_rpn_cls: 0.02985  loss_rpn_loc: 0.08619    time: 0.7896  last_time: 0.7770  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:41:00 d2.utils.events]: \u001b[0m eta: 2:15:05  iter: 1579  total_loss: 2.451  loss_cls_stage0: 0.08512  loss_box_reg_stage0: 0.491  loss_cls_stage1: 0.1258  loss_box_reg_stage1: 0.6675  loss_cls_stage2: 0.1298  loss_box_reg_stage2: 0.6137  loss_mask: 0.1648  loss_rpn_cls: 0.02742  loss_rpn_loc: 0.07608    time: 0.7900  last_time: 0.7802  data_time: 0.0042  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:41:16 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:41:16 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:41:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:41:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:41:16 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:41:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:41:16 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:41:16 d2.utils.events]: \u001b[0m eta: 2:14:46  iter: 1599  total_loss: 2.325  loss_cls_stage0: 0.08636  loss_box_reg_stage0: 0.4279  loss_cls_stage1: 0.1117  loss_box_reg_stage1: 0.5116  loss_cls_stage2: 0.1117  loss_box_reg_stage2: 0.537  loss_mask: 0.2136  loss_rpn_cls: 0.03347  loss_rpn_loc: 0.06887    time: 0.7901  last_time: 0.7851  data_time: 0.0022  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:41:31 d2.utils.events]: \u001b[0m eta: 2:14:25  iter: 1619  total_loss: 2.498  loss_cls_stage0: 0.1249  loss_box_reg_stage0: 0.4339  loss_cls_stage1: 0.1402  loss_box_reg_stage1: 0.6155  loss_cls_stage2: 0.1558  loss_box_reg_stage2: 0.6673  loss_mask: 0.1855  loss_rpn_cls: 0.03625  loss_rpn_loc: 0.08725    time: 0.7899  last_time: 0.7659  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:41:47 d2.utils.events]: \u001b[0m eta: 2:14:06  iter: 1639  total_loss: 2.42  loss_cls_stage0: 0.1333  loss_box_reg_stage0: 0.4519  loss_cls_stage1: 0.1146  loss_box_reg_stage1: 0.6082  loss_cls_stage2: 0.125  loss_box_reg_stage2: 0.6677  loss_mask: 0.1878  loss_rpn_cls: 0.0418  loss_rpn_loc: 0.07594    time: 0.7898  last_time: 0.7741  data_time: 0.0019  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:42:02 d2.utils.events]: \u001b[0m eta: 2:13:44  iter: 1659  total_loss: 2.366  loss_cls_stage0: 0.1708  loss_box_reg_stage0: 0.4024  loss_cls_stage1: 0.1632  loss_box_reg_stage1: 0.5936  loss_cls_stage2: 0.1437  loss_box_reg_stage2: 0.5608  loss_mask: 0.1828  loss_rpn_cls: 0.05182  loss_rpn_loc: 0.06618    time: 0.7896  last_time: 0.7806  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:42:18 d2.utils.events]: \u001b[0m eta: 2:13:21  iter: 1679  total_loss: 2.658  loss_cls_stage0: 0.1447  loss_box_reg_stage0: 0.5494  loss_cls_stage1: 0.1315  loss_box_reg_stage1: 0.6766  loss_cls_stage2: 0.1276  loss_box_reg_stage2: 0.6306  loss_mask: 0.1728  loss_rpn_cls: 0.03173  loss_rpn_loc: 0.059    time: 0.7893  last_time: 0.7822  data_time: 0.0014  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:42:33 d2.utils.events]: \u001b[0m eta: 2:13:05  iter: 1699  total_loss: 2.243  loss_cls_stage0: 0.1025  loss_box_reg_stage0: 0.3875  loss_cls_stage1: 0.0908  loss_box_reg_stage1: 0.5877  loss_cls_stage2: 0.1095  loss_box_reg_stage2: 0.6335  loss_mask: 0.1767  loss_rpn_cls: 0.02949  loss_rpn_loc: 0.08461    time: 0.7891  last_time: 0.7796  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:42:49 d2.utils.events]: \u001b[0m eta: 2:12:52  iter: 1719  total_loss: 2.33  loss_cls_stage0: 0.08727  loss_box_reg_stage0: 0.4623  loss_cls_stage1: 0.1206  loss_box_reg_stage1: 0.5371  loss_cls_stage2: 0.1402  loss_box_reg_stage2: 0.6692  loss_mask: 0.1636  loss_rpn_cls: 0.02438  loss_rpn_loc: 0.06254    time: 0.7890  last_time: 0.7704  data_time: 0.0020  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:43:04 d2.utils.events]: \u001b[0m eta: 2:12:35  iter: 1739  total_loss: 2.645  loss_cls_stage0: 0.08663  loss_box_reg_stage0: 0.4766  loss_cls_stage1: 0.1149  loss_box_reg_stage1: 0.71  loss_cls_stage2: 0.1228  loss_box_reg_stage2: 0.761  loss_mask: 0.1683  loss_rpn_cls: 0.01849  loss_rpn_loc: 0.07425    time: 0.7888  last_time: 0.7716  data_time: 0.0015  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:43:21 d2.utils.events]: \u001b[0m eta: 2:12:23  iter: 1759  total_loss: 2.424  loss_cls_stage0: 0.1112  loss_box_reg_stage0: 0.4768  loss_cls_stage1: 0.1243  loss_box_reg_stage1: 0.6645  loss_cls_stage2: 0.1385  loss_box_reg_stage2: 0.6443  loss_mask: 0.191  loss_rpn_cls: 0.021  loss_rpn_loc: 0.08662    time: 0.7890  last_time: 0.7756  data_time: 0.0019  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:43:36 d2.utils.events]: \u001b[0m eta: 2:12:10  iter: 1779  total_loss: 2.299  loss_cls_stage0: 0.0944  loss_box_reg_stage0: 0.3873  loss_cls_stage1: 0.09933  loss_box_reg_stage1: 0.5871  loss_cls_stage2: 0.13  loss_box_reg_stage2: 0.7186  loss_mask: 0.1655  loss_rpn_cls: 0.02245  loss_rpn_loc: 0.06638    time: 0.7889  last_time: 0.7825  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:43:52 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:43:52 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:43:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:43:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:43:52 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:43:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:43:52 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:43:52 d2.utils.events]: \u001b[0m eta: 2:11:55  iter: 1799  total_loss: 2.385  loss_cls_stage0: 0.09098  loss_box_reg_stage0: 0.4199  loss_cls_stage1: 0.09225  loss_box_reg_stage1: 0.6122  loss_cls_stage2: 0.1383  loss_box_reg_stage2: 0.7777  loss_mask: 0.1669  loss_rpn_cls: 0.02034  loss_rpn_loc: 0.07321    time: 0.7887  last_time: 0.7720  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:44:07 d2.utils.events]: \u001b[0m eta: 2:11:41  iter: 1819  total_loss: 1.993  loss_cls_stage0: 0.0869  loss_box_reg_stage0: 0.3503  loss_cls_stage1: 0.08336  loss_box_reg_stage1: 0.5105  loss_cls_stage2: 0.1218  loss_box_reg_stage2: 0.6337  loss_mask: 0.1931  loss_rpn_cls: 0.02154  loss_rpn_loc: 0.05144    time: 0.7886  last_time: 0.7432  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:44:23 d2.utils.events]: \u001b[0m eta: 2:11:26  iter: 1839  total_loss: 2.514  loss_cls_stage0: 0.08649  loss_box_reg_stage0: 0.4439  loss_cls_stage1: 0.09288  loss_box_reg_stage1: 0.6646  loss_cls_stage2: 0.1113  loss_box_reg_stage2: 0.7754  loss_mask: 0.1704  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.06103    time: 0.7886  last_time: 0.7777  data_time: 0.0019  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:44:39 d2.utils.events]: \u001b[0m eta: 2:11:10  iter: 1859  total_loss: 2.442  loss_cls_stage0: 0.1027  loss_box_reg_stage0: 0.4728  loss_cls_stage1: 0.09138  loss_box_reg_stage1: 0.6539  loss_cls_stage2: 0.1266  loss_box_reg_stage2: 0.7924  loss_mask: 0.1742  loss_rpn_cls: 0.01904  loss_rpn_loc: 0.07862    time: 0.7886  last_time: 0.7704  data_time: 0.0020  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:44:54 d2.utils.events]: \u001b[0m eta: 2:10:53  iter: 1879  total_loss: 2.222  loss_cls_stage0: 0.1049  loss_box_reg_stage0: 0.3955  loss_cls_stage1: 0.1051  loss_box_reg_stage1: 0.5581  loss_cls_stage2: 0.1259  loss_box_reg_stage2: 0.5937  loss_mask: 0.1725  loss_rpn_cls: 0.02399  loss_rpn_loc: 0.06982    time: 0.7883  last_time: 0.7582  data_time: 0.0015  last_data_time: 0.0011   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:45:10 d2.utils.events]: \u001b[0m eta: 2:10:38  iter: 1899  total_loss: 2.46  loss_cls_stage0: 0.1204  loss_box_reg_stage0: 0.4773  loss_cls_stage1: 0.1139  loss_box_reg_stage1: 0.6276  loss_cls_stage2: 0.1127  loss_box_reg_stage2: 0.5723  loss_mask: 0.1697  loss_rpn_cls: 0.02396  loss_rpn_loc: 0.06033    time: 0.7882  last_time: 0.7714  data_time: 0.0013  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:45:25 d2.utils.events]: \u001b[0m eta: 2:10:23  iter: 1919  total_loss: 2.666  loss_cls_stage0: 0.1329  loss_box_reg_stage0: 0.4798  loss_cls_stage1: 0.1097  loss_box_reg_stage1: 0.6595  loss_cls_stage2: 0.1412  loss_box_reg_stage2: 0.7667  loss_mask: 0.1894  loss_rpn_cls: 0.03809  loss_rpn_loc: 0.07908    time: 0.7881  last_time: 0.8223  data_time: 0.0017  last_data_time: 0.0029   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:45:41 d2.utils.events]: \u001b[0m eta: 2:10:09  iter: 1939  total_loss: 2.525  loss_cls_stage0: 0.1411  loss_box_reg_stage0: 0.4821  loss_cls_stage1: 0.1363  loss_box_reg_stage1: 0.6804  loss_cls_stage2: 0.1389  loss_box_reg_stage2: 0.7251  loss_mask: 0.1799  loss_rpn_cls: 0.02722  loss_rpn_loc: 0.0728    time: 0.7883  last_time: 0.7809  data_time: 0.0019  last_data_time: 0.0029   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:45:57 d2.utils.events]: \u001b[0m eta: 2:09:58  iter: 1959  total_loss: 2.44  loss_cls_stage0: 0.08958  loss_box_reg_stage0: 0.3856  loss_cls_stage1: 0.08765  loss_box_reg_stage1: 0.6188  loss_cls_stage2: 0.1023  loss_box_reg_stage2: 0.7291  loss_mask: 0.1792  loss_rpn_cls: 0.0329  loss_rpn_loc: 0.0634    time: 0.7884  last_time: 0.7875  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:46:13 d2.utils.events]: \u001b[0m eta: 2:09:45  iter: 1979  total_loss: 2.259  loss_cls_stage0: 0.06873  loss_box_reg_stage0: 0.4445  loss_cls_stage1: 0.08442  loss_box_reg_stage1: 0.6063  loss_cls_stage2: 0.09647  loss_box_reg_stage2: 0.6767  loss_mask: 0.1508  loss_rpn_cls: 0.02089  loss_rpn_loc: 0.06649    time: 0.7885  last_time: 0.8040  data_time: 0.0018  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:46:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:46:29 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:46:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:46:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:46:29 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:46:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:46:29 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:46:29 d2.utils.events]: \u001b[0m eta: 2:09:36  iter: 1999  total_loss: 2.423  loss_cls_stage0: 0.1152  loss_box_reg_stage0: 0.4358  loss_cls_stage1: 0.1037  loss_box_reg_stage1: 0.668  loss_cls_stage2: 0.1253  loss_box_reg_stage2: 0.7261  loss_mask: 0.161  loss_rpn_cls: 0.01802  loss_rpn_loc: 0.05518    time: 0.7886  last_time: 0.7902  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:46:45 d2.utils.events]: \u001b[0m eta: 2:09:25  iter: 2019  total_loss: 2.359  loss_cls_stage0: 0.09637  loss_box_reg_stage0: 0.4857  loss_cls_stage1: 0.111  loss_box_reg_stage1: 0.6356  loss_cls_stage2: 0.1425  loss_box_reg_stage2: 0.7001  loss_mask: 0.1842  loss_rpn_cls: 0.02304  loss_rpn_loc: 0.08867    time: 0.7886  last_time: 0.8065  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:47:01 d2.utils.events]: \u001b[0m eta: 2:09:16  iter: 2039  total_loss: 2.285  loss_cls_stage0: 0.09938  loss_box_reg_stage0: 0.4579  loss_cls_stage1: 0.0871  loss_box_reg_stage1: 0.6168  loss_cls_stage2: 0.154  loss_box_reg_stage2: 0.6501  loss_mask: 0.1826  loss_rpn_cls: 0.02119  loss_rpn_loc: 0.0749    time: 0.7888  last_time: 0.7770  data_time: 0.0020  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:47:17 d2.utils.events]: \u001b[0m eta: 2:09:06  iter: 2059  total_loss: 2.482  loss_cls_stage0: 0.1096  loss_box_reg_stage0: 0.4555  loss_cls_stage1: 0.1145  loss_box_reg_stage1: 0.6002  loss_cls_stage2: 0.147  loss_box_reg_stage2: 0.7173  loss_mask: 0.1904  loss_rpn_cls: 0.03305  loss_rpn_loc: 0.05672    time: 0.7888  last_time: 0.7862  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:47:33 d2.utils.events]: \u001b[0m eta: 2:08:57  iter: 2079  total_loss: 2.359  loss_cls_stage0: 0.09177  loss_box_reg_stage0: 0.4355  loss_cls_stage1: 0.1175  loss_box_reg_stage1: 0.6431  loss_cls_stage2: 0.1168  loss_box_reg_stage2: 0.6685  loss_mask: 0.1655  loss_rpn_cls: 0.02302  loss_rpn_loc: 0.06161    time: 0.7889  last_time: 0.7886  data_time: 0.0020  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:47:49 d2.utils.events]: \u001b[0m eta: 2:08:46  iter: 2099  total_loss: 2.092  loss_cls_stage0: 0.07481  loss_box_reg_stage0: 0.4268  loss_cls_stage1: 0.08998  loss_box_reg_stage1: 0.5706  loss_cls_stage2: 0.0981  loss_box_reg_stage2: 0.657  loss_mask: 0.1703  loss_rpn_cls: 0.01605  loss_rpn_loc: 0.07438    time: 0.7890  last_time: 0.7963  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:48:05 d2.utils.events]: \u001b[0m eta: 2:08:36  iter: 2119  total_loss: 2.569  loss_cls_stage0: 0.07379  loss_box_reg_stage0: 0.4353  loss_cls_stage1: 0.08883  loss_box_reg_stage1: 0.6353  loss_cls_stage2: 0.1137  loss_box_reg_stage2: 0.7155  loss_mask: 0.1818  loss_rpn_cls: 0.02501  loss_rpn_loc: 0.06123    time: 0.7891  last_time: 0.7993  data_time: 0.0019  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:48:21 d2.utils.events]: \u001b[0m eta: 2:08:32  iter: 2139  total_loss: 2.417  loss_cls_stage0: 0.09325  loss_box_reg_stage0: 0.3895  loss_cls_stage1: 0.09988  loss_box_reg_stage1: 0.6004  loss_cls_stage2: 0.1013  loss_box_reg_stage2: 0.639  loss_mask: 0.1919  loss_rpn_cls: 0.02209  loss_rpn_loc: 0.08553    time: 0.7892  last_time: 0.8052  data_time: 0.0022  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:48:37 d2.utils.events]: \u001b[0m eta: 2:08:20  iter: 2159  total_loss: 2.29  loss_cls_stage0: 0.07145  loss_box_reg_stage0: 0.387  loss_cls_stage1: 0.07725  loss_box_reg_stage1: 0.5709  loss_cls_stage2: 0.109  loss_box_reg_stage2: 0.7065  loss_mask: 0.1589  loss_rpn_cls: 0.01314  loss_rpn_loc: 0.0661    time: 0.7893  last_time: 0.8208  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:48:53 d2.utils.events]: \u001b[0m eta: 2:08:05  iter: 2179  total_loss: 2.523  loss_cls_stage0: 0.1026  loss_box_reg_stage0: 0.4243  loss_cls_stage1: 0.08482  loss_box_reg_stage1: 0.6435  loss_cls_stage2: 0.1091  loss_box_reg_stage2: 0.7127  loss_mask: 0.1796  loss_rpn_cls: 0.01991  loss_rpn_loc: 0.1025    time: 0.7894  last_time: 0.7896  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:49:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:49:09 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:49:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:49:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:49:09 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:49:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:49:09 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:49:09 d2.utils.events]: \u001b[0m eta: 2:07:51  iter: 2199  total_loss: 2.661  loss_cls_stage0: 0.09822  loss_box_reg_stage0: 0.4659  loss_cls_stage1: 0.1074  loss_box_reg_stage1: 0.716  loss_cls_stage2: 0.1514  loss_box_reg_stage2: 0.7829  loss_mask: 0.1797  loss_rpn_cls: 0.03119  loss_rpn_loc: 0.05862    time: 0.7895  last_time: 0.7931  data_time: 0.0018  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:49:25 d2.utils.events]: \u001b[0m eta: 2:07:36  iter: 2219  total_loss: 2.218  loss_cls_stage0: 0.07574  loss_box_reg_stage0: 0.3901  loss_cls_stage1: 0.07276  loss_box_reg_stage1: 0.5508  loss_cls_stage2: 0.1021  loss_box_reg_stage2: 0.6809  loss_mask: 0.1542  loss_rpn_cls: 0.01933  loss_rpn_loc: 0.04943    time: 0.7896  last_time: 0.8071  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:49:41 d2.utils.events]: \u001b[0m eta: 2:07:20  iter: 2239  total_loss: 2.181  loss_cls_stage0: 0.09572  loss_box_reg_stage0: 0.3972  loss_cls_stage1: 0.1004  loss_box_reg_stage1: 0.6239  loss_cls_stage2: 0.1054  loss_box_reg_stage2: 0.7383  loss_mask: 0.149  loss_rpn_cls: 0.02397  loss_rpn_loc: 0.0519    time: 0.7897  last_time: 0.7873  data_time: 0.0020  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:49:57 d2.utils.events]: \u001b[0m eta: 2:07:15  iter: 2259  total_loss: 2.379  loss_cls_stage0: 0.08572  loss_box_reg_stage0: 0.4387  loss_cls_stage1: 0.08528  loss_box_reg_stage1: 0.6508  loss_cls_stage2: 0.1223  loss_box_reg_stage2: 0.7091  loss_mask: 0.1858  loss_rpn_cls: 0.02711  loss_rpn_loc: 0.07598    time: 0.7897  last_time: 0.7902  data_time: 0.0018  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:50:13 d2.utils.events]: \u001b[0m eta: 2:07:05  iter: 2279  total_loss: 2.545  loss_cls_stage0: 0.09967  loss_box_reg_stage0: 0.4393  loss_cls_stage1: 0.1126  loss_box_reg_stage1: 0.6459  loss_cls_stage2: 0.1229  loss_box_reg_stage2: 0.6827  loss_mask: 0.1501  loss_rpn_cls: 0.01917  loss_rpn_loc: 0.05932    time: 0.7898  last_time: 0.8229  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:50:29 d2.utils.events]: \u001b[0m eta: 2:06:53  iter: 2299  total_loss: 2.279  loss_cls_stage0: 0.104  loss_box_reg_stage0: 0.3976  loss_cls_stage1: 0.0997  loss_box_reg_stage1: 0.6366  loss_cls_stage2: 0.1248  loss_box_reg_stage2: 0.7381  loss_mask: 0.1696  loss_rpn_cls: 0.01763  loss_rpn_loc: 0.08253    time: 0.7899  last_time: 0.8105  data_time: 0.0016  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:50:45 d2.utils.events]: \u001b[0m eta: 2:06:47  iter: 2319  total_loss: 2.26  loss_cls_stage0: 0.1256  loss_box_reg_stage0: 0.4572  loss_cls_stage1: 0.1227  loss_box_reg_stage1: 0.6141  loss_cls_stage2: 0.1386  loss_box_reg_stage2: 0.6546  loss_mask: 0.1556  loss_rpn_cls: 0.0214  loss_rpn_loc: 0.06912    time: 0.7899  last_time: 0.7984  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:51:01 d2.utils.events]: \u001b[0m eta: 2:06:37  iter: 2339  total_loss: 2.343  loss_cls_stage0: 0.08837  loss_box_reg_stage0: 0.4595  loss_cls_stage1: 0.1026  loss_box_reg_stage1: 0.6254  loss_cls_stage2: 0.1106  loss_box_reg_stage2: 0.6908  loss_mask: 0.1827  loss_rpn_cls: 0.02088  loss_rpn_loc: 0.06953    time: 0.7900  last_time: 0.8084  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:51:17 d2.utils.events]: \u001b[0m eta: 2:06:26  iter: 2359  total_loss: 2.142  loss_cls_stage0: 0.06555  loss_box_reg_stage0: 0.3776  loss_cls_stage1: 0.06736  loss_box_reg_stage1: 0.5764  loss_cls_stage2: 0.1093  loss_box_reg_stage2: 0.6345  loss_mask: 0.1693  loss_rpn_cls: 0.02057  loss_rpn_loc: 0.06611    time: 0.7901  last_time: 0.7947  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:51:33 d2.utils.events]: \u001b[0m eta: 2:06:15  iter: 2379  total_loss: 1.935  loss_cls_stage0: 0.05838  loss_box_reg_stage0: 0.3503  loss_cls_stage1: 0.05299  loss_box_reg_stage1: 0.5565  loss_cls_stage2: 0.07108  loss_box_reg_stage2: 0.6365  loss_mask: 0.1507  loss_rpn_cls: 0.01105  loss_rpn_loc: 0.04863    time: 0.7902  last_time: 0.7986  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:51:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:51:50 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:51:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:51:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:51:50 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:51:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:51:50 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:51:50 d2.utils.events]: \u001b[0m eta: 2:06:04  iter: 2399  total_loss: 2.337  loss_cls_stage0: 0.08317  loss_box_reg_stage0: 0.4113  loss_cls_stage1: 0.0843  loss_box_reg_stage1: 0.6376  loss_cls_stage2: 0.1163  loss_box_reg_stage2: 0.7036  loss_mask: 0.1831  loss_rpn_cls: 0.01632  loss_rpn_loc: 0.05576    time: 0.7905  last_time: 0.7953  data_time: 0.0024  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:52:06 d2.utils.events]: \u001b[0m eta: 2:05:52  iter: 2419  total_loss: 2.411  loss_cls_stage0: 0.08258  loss_box_reg_stage0: 0.4478  loss_cls_stage1: 0.1076  loss_box_reg_stage1: 0.6415  loss_cls_stage2: 0.1219  loss_box_reg_stage2: 0.7204  loss_mask: 0.1867  loss_rpn_cls: 0.01908  loss_rpn_loc: 0.08142    time: 0.7906  last_time: 0.7826  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:52:21 d2.utils.events]: \u001b[0m eta: 2:05:40  iter: 2439  total_loss: 2  loss_cls_stage0: 0.05841  loss_box_reg_stage0: 0.3697  loss_cls_stage1: 0.05613  loss_box_reg_stage1: 0.5983  loss_cls_stage2: 0.07819  loss_box_reg_stage2: 0.6709  loss_mask: 0.1505  loss_rpn_cls: 0.01136  loss_rpn_loc: 0.03268    time: 0.7906  last_time: 0.8138  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:52:37 d2.utils.events]: \u001b[0m eta: 2:05:32  iter: 2459  total_loss: 2.14  loss_cls_stage0: 0.05625  loss_box_reg_stage0: 0.4024  loss_cls_stage1: 0.07119  loss_box_reg_stage1: 0.6117  loss_cls_stage2: 0.07773  loss_box_reg_stage2: 0.6914  loss_mask: 0.1487  loss_rpn_cls: 0.0122  loss_rpn_loc: 0.07317    time: 0.7906  last_time: 0.7890  data_time: 0.0017  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:52:53 d2.utils.events]: \u001b[0m eta: 2:05:21  iter: 2479  total_loss: 1.964  loss_cls_stage0: 0.05385  loss_box_reg_stage0: 0.3867  loss_cls_stage1: 0.07406  loss_box_reg_stage1: 0.4857  loss_cls_stage2: 0.1137  loss_box_reg_stage2: 0.6025  loss_mask: 0.1489  loss_rpn_cls: 0.01155  loss_rpn_loc: 0.0461    time: 0.7907  last_time: 0.7911  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:53:10 d2.utils.events]: \u001b[0m eta: 2:05:12  iter: 2499  total_loss: 2.173  loss_cls_stage0: 0.05905  loss_box_reg_stage0: 0.394  loss_cls_stage1: 0.06776  loss_box_reg_stage1: 0.5848  loss_cls_stage2: 0.09625  loss_box_reg_stage2: 0.7335  loss_mask: 0.1637  loss_rpn_cls: 0.01434  loss_rpn_loc: 0.08137    time: 0.7908  last_time: 0.8313  data_time: 0.0017  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:53:25 d2.utils.events]: \u001b[0m eta: 2:05:00  iter: 2519  total_loss: 2.097  loss_cls_stage0: 0.07558  loss_box_reg_stage0: 0.4094  loss_cls_stage1: 0.06641  loss_box_reg_stage1: 0.5002  loss_cls_stage2: 0.08398  loss_box_reg_stage2: 0.6506  loss_mask: 0.1627  loss_rpn_cls: 0.01507  loss_rpn_loc: 0.06449    time: 0.7908  last_time: 0.7949  data_time: 0.0017  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:53:41 d2.utils.events]: \u001b[0m eta: 2:04:50  iter: 2539  total_loss: 2.163  loss_cls_stage0: 0.07915  loss_box_reg_stage0: 0.3951  loss_cls_stage1: 0.07426  loss_box_reg_stage1: 0.5372  loss_cls_stage2: 0.09282  loss_box_reg_stage2: 0.7097  loss_mask: 0.1595  loss_rpn_cls: 0.02174  loss_rpn_loc: 0.06724    time: 0.7909  last_time: 0.8012  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:53:57 d2.utils.events]: \u001b[0m eta: 2:04:37  iter: 2559  total_loss: 2.366  loss_cls_stage0: 0.08877  loss_box_reg_stage0: 0.3939  loss_cls_stage1: 0.0744  loss_box_reg_stage1: 0.5822  loss_cls_stage2: 0.09385  loss_box_reg_stage2: 0.6915  loss_mask: 0.162  loss_rpn_cls: 0.01987  loss_rpn_loc: 0.07501    time: 0.7909  last_time: 0.7940  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:54:13 d2.utils.events]: \u001b[0m eta: 2:04:23  iter: 2579  total_loss: 2.151  loss_cls_stage0: 0.08824  loss_box_reg_stage0: 0.3817  loss_cls_stage1: 0.07067  loss_box_reg_stage1: 0.5365  loss_cls_stage2: 0.09417  loss_box_reg_stage2: 0.7201  loss_mask: 0.1721  loss_rpn_cls: 0.02404  loss_rpn_loc: 0.07323    time: 0.7910  last_time: 0.8015  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:54:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:54:29 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:54:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:54:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:54:29 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:54:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:54:29 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:54:29 d2.utils.events]: \u001b[0m eta: 2:04:10  iter: 2599  total_loss: 2.107  loss_cls_stage0: 0.07572  loss_box_reg_stage0: 0.3866  loss_cls_stage1: 0.07347  loss_box_reg_stage1: 0.491  loss_cls_stage2: 0.08504  loss_box_reg_stage2: 0.609  loss_mask: 0.1505  loss_rpn_cls: 0.01488  loss_rpn_loc: 0.05081    time: 0.7911  last_time: 0.7884  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:54:45 d2.utils.events]: \u001b[0m eta: 2:04:00  iter: 2619  total_loss: 2.337  loss_cls_stage0: 0.1116  loss_box_reg_stage0: 0.4116  loss_cls_stage1: 0.06666  loss_box_reg_stage1: 0.5986  loss_cls_stage2: 0.07669  loss_box_reg_stage2: 0.7523  loss_mask: 0.1839  loss_rpn_cls: 0.01452  loss_rpn_loc: 0.05346    time: 0.7911  last_time: 0.8007  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:55:01 d2.utils.events]: \u001b[0m eta: 2:03:47  iter: 2639  total_loss: 2.438  loss_cls_stage0: 0.07245  loss_box_reg_stage0: 0.4003  loss_cls_stage1: 0.08405  loss_box_reg_stage1: 0.678  loss_cls_stage2: 0.09393  loss_box_reg_stage2: 0.7126  loss_mask: 0.1467  loss_rpn_cls: 0.01519  loss_rpn_loc: 0.05817    time: 0.7912  last_time: 0.8308  data_time: 0.0018  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:55:17 d2.utils.events]: \u001b[0m eta: 2:03:38  iter: 2659  total_loss: 2.233  loss_cls_stage0: 0.08802  loss_box_reg_stage0: 0.3978  loss_cls_stage1: 0.08097  loss_box_reg_stage1: 0.5568  loss_cls_stage2: 0.1012  loss_box_reg_stage2: 0.7618  loss_mask: 0.1669  loss_rpn_cls: 0.01366  loss_rpn_loc: 0.062    time: 0.7913  last_time: 0.7946  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:55:33 d2.utils.events]: \u001b[0m eta: 2:03:26  iter: 2679  total_loss: 2.436  loss_cls_stage0: 0.07107  loss_box_reg_stage0: 0.4227  loss_cls_stage1: 0.08613  loss_box_reg_stage1: 0.6322  loss_cls_stage2: 0.1145  loss_box_reg_stage2: 0.731  loss_mask: 0.1659  loss_rpn_cls: 0.01377  loss_rpn_loc: 0.05837    time: 0.7913  last_time: 0.8263  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:55:49 d2.utils.events]: \u001b[0m eta: 2:03:14  iter: 2699  total_loss: 2.194  loss_cls_stage0: 0.06173  loss_box_reg_stage0: 0.384  loss_cls_stage1: 0.0618  loss_box_reg_stage1: 0.5537  loss_cls_stage2: 0.09456  loss_box_reg_stage2: 0.7019  loss_mask: 0.1655  loss_rpn_cls: 0.0127  loss_rpn_loc: 0.05183    time: 0.7914  last_time: 0.8104  data_time: 0.0018  last_data_time: 0.0028   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:56:06 d2.utils.events]: \u001b[0m eta: 2:03:03  iter: 2719  total_loss: 2.175  loss_cls_stage0: 0.1005  loss_box_reg_stage0: 0.4038  loss_cls_stage1: 0.07455  loss_box_reg_stage1: 0.5924  loss_cls_stage2: 0.1139  loss_box_reg_stage2: 0.7498  loss_mask: 0.1594  loss_rpn_cls: 0.03442  loss_rpn_loc: 0.05844    time: 0.7915  last_time: 0.7981  data_time: 0.0016  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:56:22 d2.utils.events]: \u001b[0m eta: 2:02:50  iter: 2739  total_loss: 2.178  loss_cls_stage0: 0.07126  loss_box_reg_stage0: 0.3339  loss_cls_stage1: 0.06746  loss_box_reg_stage1: 0.5389  loss_cls_stage2: 0.08515  loss_box_reg_stage2: 0.6815  loss_mask: 0.1657  loss_rpn_cls: 0.01644  loss_rpn_loc: 0.06113    time: 0.7916  last_time: 0.7958  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:56:37 d2.utils.events]: \u001b[0m eta: 2:02:36  iter: 2759  total_loss: 2.062  loss_cls_stage0: 0.09161  loss_box_reg_stage0: 0.369  loss_cls_stage1: 0.1079  loss_box_reg_stage1: 0.5189  loss_cls_stage2: 0.09769  loss_box_reg_stage2: 0.5895  loss_mask: 0.1644  loss_rpn_cls: 0.01681  loss_rpn_loc: 0.07063    time: 0.7916  last_time: 0.7968  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:56:53 d2.utils.events]: \u001b[0m eta: 2:02:21  iter: 2779  total_loss: 2.159  loss_cls_stage0: 0.0746  loss_box_reg_stage0: 0.3833  loss_cls_stage1: 0.0841  loss_box_reg_stage1: 0.5061  loss_cls_stage2: 0.08875  loss_box_reg_stage2: 0.6182  loss_mask: 0.1686  loss_rpn_cls: 0.02232  loss_rpn_loc: 0.02913    time: 0.7916  last_time: 0.7668  data_time: 0.0016  last_data_time: 0.0011   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:57:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:57:09 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:57:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:57:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:57:09 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:57:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:57:09 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:57:09 d2.utils.events]: \u001b[0m eta: 2:02:07  iter: 2799  total_loss: 2.193  loss_cls_stage0: 0.1022  loss_box_reg_stage0: 0.3788  loss_cls_stage1: 0.08711  loss_box_reg_stage1: 0.501  loss_cls_stage2: 0.08933  loss_box_reg_stage2: 0.6653  loss_mask: 0.1512  loss_rpn_cls: 0.02171  loss_rpn_loc: 0.04959    time: 0.7916  last_time: 0.7969  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:57:25 d2.utils.events]: \u001b[0m eta: 2:01:55  iter: 2819  total_loss: 2.228  loss_cls_stage0: 0.07505  loss_box_reg_stage0: 0.4205  loss_cls_stage1: 0.07106  loss_box_reg_stage1: 0.6534  loss_cls_stage2: 0.1009  loss_box_reg_stage2: 0.6327  loss_mask: 0.1696  loss_rpn_cls: 0.01701  loss_rpn_loc: 0.08339    time: 0.7917  last_time: 0.7972  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:57:41 d2.utils.events]: \u001b[0m eta: 2:01:42  iter: 2839  total_loss: 1.973  loss_cls_stage0: 0.06537  loss_box_reg_stage0: 0.3488  loss_cls_stage1: 0.04976  loss_box_reg_stage1: 0.5349  loss_cls_stage2: 0.07486  loss_box_reg_stage2: 0.6189  loss_mask: 0.1582  loss_rpn_cls: 0.02523  loss_rpn_loc: 0.07179    time: 0.7918  last_time: 0.8066  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:57:57 d2.utils.events]: \u001b[0m eta: 2:01:28  iter: 2859  total_loss: 2.285  loss_cls_stage0: 0.08905  loss_box_reg_stage0: 0.4158  loss_cls_stage1: 0.07787  loss_box_reg_stage1: 0.6189  loss_cls_stage2: 0.09864  loss_box_reg_stage2: 0.6504  loss_mask: 0.1559  loss_rpn_cls: 0.01999  loss_rpn_loc: 0.06105    time: 0.7918  last_time: 0.8051  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:58:13 d2.utils.events]: \u001b[0m eta: 2:01:15  iter: 2879  total_loss: 2.083  loss_cls_stage0: 0.05606  loss_box_reg_stage0: 0.3862  loss_cls_stage1: 0.0722  loss_box_reg_stage1: 0.5105  loss_cls_stage2: 0.09236  loss_box_reg_stage2: 0.5751  loss_mask: 0.142  loss_rpn_cls: 0.01114  loss_rpn_loc: 0.04893    time: 0.7919  last_time: 0.7852  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:58:29 d2.utils.events]: \u001b[0m eta: 2:01:00  iter: 2899  total_loss: 1.942  loss_cls_stage0: 0.06956  loss_box_reg_stage0: 0.3714  loss_cls_stage1: 0.08226  loss_box_reg_stage1: 0.5  loss_cls_stage2: 0.08849  loss_box_reg_stage2: 0.5729  loss_mask: 0.1578  loss_rpn_cls: 0.01695  loss_rpn_loc: 0.05824    time: 0.7919  last_time: 0.7965  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:58:46 d2.utils.events]: \u001b[0m eta: 2:00:46  iter: 2919  total_loss: 2.404  loss_cls_stage0: 0.09144  loss_box_reg_stage0: 0.4321  loss_cls_stage1: 0.08014  loss_box_reg_stage1: 0.6078  loss_cls_stage2: 0.09457  loss_box_reg_stage2: 0.7634  loss_mask: 0.1884  loss_rpn_cls: 0.0144  loss_rpn_loc: 0.07385    time: 0.7921  last_time: 0.8316  data_time: 0.0017  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:59:02 d2.utils.events]: \u001b[0m eta: 2:00:32  iter: 2939  total_loss: 2.04  loss_cls_stage0: 0.09599  loss_box_reg_stage0: 0.3907  loss_cls_stage1: 0.08044  loss_box_reg_stage1: 0.5572  loss_cls_stage2: 0.08841  loss_box_reg_stage2: 0.6651  loss_mask: 0.1575  loss_rpn_cls: 0.01477  loss_rpn_loc: 0.05848    time: 0.7921  last_time: 0.8034  data_time: 0.0020  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:59:18 d2.utils.events]: \u001b[0m eta: 2:00:18  iter: 2959  total_loss: 2.179  loss_cls_stage0: 0.05186  loss_box_reg_stage0: 0.3543  loss_cls_stage1: 0.05375  loss_box_reg_stage1: 0.5669  loss_cls_stage2: 0.0715  loss_box_reg_stage2: 0.7642  loss_mask: 0.174  loss_rpn_cls: 0.01442  loss_rpn_loc: 0.06145    time: 0.7922  last_time: 0.7903  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 11:59:34 d2.utils.events]: \u001b[0m eta: 2:00:03  iter: 2979  total_loss: 2.147  loss_cls_stage0: 0.07308  loss_box_reg_stage0: 0.3705  loss_cls_stage1: 0.05645  loss_box_reg_stage1: 0.5859  loss_cls_stage2: 0.07061  loss_box_reg_stage2: 0.7006  loss_mask: 0.1509  loss_rpn_cls: 0.01405  loss_rpn_loc: 0.06181    time: 0.7923  last_time: 0.7931  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:59:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 11:59:50 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 11:59:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 11:59:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 11:59:50 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 11:59:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 11:59:50 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 11:59:50 d2.utils.events]: \u001b[0m eta: 1:59:50  iter: 2999  total_loss: 2.503  loss_cls_stage0: 0.07845  loss_box_reg_stage0: 0.4513  loss_cls_stage1: 0.0658  loss_box_reg_stage1: 0.681  loss_cls_stage2: 0.09019  loss_box_reg_stage2: 0.7807  loss_mask: 0.1715  loss_rpn_cls: 0.01511  loss_rpn_loc: 0.05569    time: 0.7924  last_time: 0.8190  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:00:06 d2.utils.events]: \u001b[0m eta: 1:59:35  iter: 3019  total_loss: 2.154  loss_cls_stage0: 0.06223  loss_box_reg_stage0: 0.3923  loss_cls_stage1: 0.05941  loss_box_reg_stage1: 0.5849  loss_cls_stage2: 0.08243  loss_box_reg_stage2: 0.7404  loss_mask: 0.1442  loss_rpn_cls: 0.01112  loss_rpn_loc: 0.06011    time: 0.7925  last_time: 0.8180  data_time: 0.0019  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:00:22 d2.utils.events]: \u001b[0m eta: 1:59:19  iter: 3039  total_loss: 2.389  loss_cls_stage0: 0.09968  loss_box_reg_stage0: 0.4131  loss_cls_stage1: 0.09602  loss_box_reg_stage1: 0.6228  loss_cls_stage2: 0.1099  loss_box_reg_stage2: 0.6673  loss_mask: 0.1752  loss_rpn_cls: 0.01434  loss_rpn_loc: 0.06082    time: 0.7926  last_time: 0.7955  data_time: 0.0017  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:00:39 d2.utils.events]: \u001b[0m eta: 1:59:04  iter: 3059  total_loss: 2.218  loss_cls_stage0: 0.08044  loss_box_reg_stage0: 0.4324  loss_cls_stage1: 0.08731  loss_box_reg_stage1: 0.591  loss_cls_stage2: 0.1304  loss_box_reg_stage2: 0.6404  loss_mask: 0.1744  loss_rpn_cls: 0.01786  loss_rpn_loc: 0.07528    time: 0.7927  last_time: 0.7982  data_time: 0.0018  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:00:55 d2.utils.events]: \u001b[0m eta: 1:58:50  iter: 3079  total_loss: 2.166  loss_cls_stage0: 0.06394  loss_box_reg_stage0: 0.3973  loss_cls_stage1: 0.08417  loss_box_reg_stage1: 0.5147  loss_cls_stage2: 0.08719  loss_box_reg_stage2: 0.6498  loss_mask: 0.1422  loss_rpn_cls: 0.02033  loss_rpn_loc: 0.05188    time: 0.7928  last_time: 0.8160  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:01:11 d2.utils.events]: \u001b[0m eta: 1:58:36  iter: 3099  total_loss: 2.623  loss_cls_stage0: 0.1103  loss_box_reg_stage0: 0.4615  loss_cls_stage1: 0.1101  loss_box_reg_stage1: 0.6491  loss_cls_stage2: 0.1363  loss_box_reg_stage2: 0.6902  loss_mask: 0.1504  loss_rpn_cls: 0.03153  loss_rpn_loc: 0.07881    time: 0.7930  last_time: 0.8779  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:01:28 d2.utils.events]: \u001b[0m eta: 1:58:24  iter: 3119  total_loss: 2.288  loss_cls_stage0: 0.07564  loss_box_reg_stage0: 0.3723  loss_cls_stage1: 0.0856  loss_box_reg_stage1: 0.5952  loss_cls_stage2: 0.1025  loss_box_reg_stage2: 0.6338  loss_mask: 0.1772  loss_rpn_cls: 0.02486  loss_rpn_loc: 0.05978    time: 0.7931  last_time: 0.8140  data_time: 0.0019  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:01:44 d2.utils.events]: \u001b[0m eta: 1:58:09  iter: 3139  total_loss: 2.117  loss_cls_stage0: 0.07833  loss_box_reg_stage0: 0.3744  loss_cls_stage1: 0.06195  loss_box_reg_stage1: 0.5326  loss_cls_stage2: 0.07543  loss_box_reg_stage2: 0.5982  loss_mask: 0.1732  loss_rpn_cls: 0.02156  loss_rpn_loc: 0.06793    time: 0.7933  last_time: 0.8194  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:02:00 d2.utils.events]: \u001b[0m eta: 1:57:56  iter: 3159  total_loss: 2.149  loss_cls_stage0: 0.06858  loss_box_reg_stage0: 0.3892  loss_cls_stage1: 0.08504  loss_box_reg_stage1: 0.591  loss_cls_stage2: 0.08025  loss_box_reg_stage2: 0.6467  loss_mask: 0.1673  loss_rpn_cls: 0.01244  loss_rpn_loc: 0.05861    time: 0.7935  last_time: 0.8133  data_time: 0.0020  last_data_time: 0.0036   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:02:18 d2.utils.events]: \u001b[0m eta: 1:57:43  iter: 3179  total_loss: 2.23  loss_cls_stage0: 0.06706  loss_box_reg_stage0: 0.3866  loss_cls_stage1: 0.05924  loss_box_reg_stage1: 0.5753  loss_cls_stage2: 0.07723  loss_box_reg_stage2: 0.701  loss_mask: 0.1481  loss_rpn_cls: 0.0154  loss_rpn_loc: 0.05996    time: 0.7939  last_time: 0.8216  data_time: 0.0032  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:02:35 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:02:35 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:02:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:02:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:02:35 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:02:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:02:35 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:02:35 d2.utils.events]: \u001b[0m eta: 1:57:30  iter: 3199  total_loss: 1.772  loss_cls_stage0: 0.06915  loss_box_reg_stage0: 0.3277  loss_cls_stage1: 0.05465  loss_box_reg_stage1: 0.4626  loss_cls_stage2: 0.06806  loss_box_reg_stage2: 0.5305  loss_mask: 0.1481  loss_rpn_cls: 0.01472  loss_rpn_loc: 0.04516    time: 0.7942  last_time: 0.8522  data_time: 0.0027  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:02:51 d2.utils.events]: \u001b[0m eta: 1:57:16  iter: 3219  total_loss: 2.185  loss_cls_stage0: 0.07271  loss_box_reg_stage0: 0.374  loss_cls_stage1: 0.07455  loss_box_reg_stage1: 0.5583  loss_cls_stage2: 0.08947  loss_box_reg_stage2: 0.6588  loss_mask: 0.1655  loss_rpn_cls: 0.01959  loss_rpn_loc: 0.07201    time: 0.7945  last_time: 0.8042  data_time: 0.0022  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:03:08 d2.utils.events]: \u001b[0m eta: 1:57:02  iter: 3239  total_loss: 1.968  loss_cls_stage0: 0.08886  loss_box_reg_stage0: 0.333  loss_cls_stage1: 0.05283  loss_box_reg_stage1: 0.5079  loss_cls_stage2: 0.07229  loss_box_reg_stage2: 0.6224  loss_mask: 0.1416  loss_rpn_cls: 0.01832  loss_rpn_loc: 0.0571    time: 0.7946  last_time: 0.8156  data_time: 0.0025  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:03:24 d2.utils.events]: \u001b[0m eta: 1:56:49  iter: 3259  total_loss: 2.099  loss_cls_stage0: 0.05757  loss_box_reg_stage0: 0.3749  loss_cls_stage1: 0.0695  loss_box_reg_stage1: 0.5423  loss_cls_stage2: 0.07117  loss_box_reg_stage2: 0.6218  loss_mask: 0.1788  loss_rpn_cls: 0.01505  loss_rpn_loc: 0.07655    time: 0.7948  last_time: 0.8182  data_time: 0.0017  last_data_time: 0.0011   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:03:40 d2.utils.events]: \u001b[0m eta: 1:56:37  iter: 3279  total_loss: 2.115  loss_cls_stage0: 0.08684  loss_box_reg_stage0: 0.3551  loss_cls_stage1: 0.0837  loss_box_reg_stage1: 0.5941  loss_cls_stage2: 0.09727  loss_box_reg_stage2: 0.6318  loss_mask: 0.1806  loss_rpn_cls: 0.01556  loss_rpn_loc: 0.05974    time: 0.7949  last_time: 0.8143  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:03:57 d2.utils.events]: \u001b[0m eta: 1:56:27  iter: 3299  total_loss: 2.304  loss_cls_stage0: 0.07743  loss_box_reg_stage0: 0.3774  loss_cls_stage1: 0.08365  loss_box_reg_stage1: 0.5508  loss_cls_stage2: 0.09749  loss_box_reg_stage2: 0.6234  loss_mask: 0.1711  loss_rpn_cls: 0.01586  loss_rpn_loc: 0.04398    time: 0.7950  last_time: 0.8232  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:04:13 d2.utils.events]: \u001b[0m eta: 1:56:13  iter: 3319  total_loss: 2.209  loss_cls_stage0: 0.07303  loss_box_reg_stage0: 0.3942  loss_cls_stage1: 0.0536  loss_box_reg_stage1: 0.5439  loss_cls_stage2: 0.07996  loss_box_reg_stage2: 0.7551  loss_mask: 0.1907  loss_rpn_cls: 0.01148  loss_rpn_loc: 0.07049    time: 0.7952  last_time: 0.8148  data_time: 0.0021  last_data_time: 0.0033   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:04:29 d2.utils.events]: \u001b[0m eta: 1:56:00  iter: 3339  total_loss: 2.025  loss_cls_stage0: 0.07036  loss_box_reg_stage0: 0.3493  loss_cls_stage1: 0.06507  loss_box_reg_stage1: 0.4872  loss_cls_stage2: 0.08196  loss_box_reg_stage2: 0.6511  loss_mask: 0.1294  loss_rpn_cls: 0.01768  loss_rpn_loc: 0.05293    time: 0.7953  last_time: 0.8174  data_time: 0.0018  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:04:46 d2.utils.events]: \u001b[0m eta: 1:55:47  iter: 3359  total_loss: 1.948  loss_cls_stage0: 0.04764  loss_box_reg_stage0: 0.3421  loss_cls_stage1: 0.06379  loss_box_reg_stage1: 0.5049  loss_cls_stage2: 0.06831  loss_box_reg_stage2: 0.5699  loss_mask: 0.1631  loss_rpn_cls: 0.01248  loss_rpn_loc: 0.05152    time: 0.7954  last_time: 0.8332  data_time: 0.0017  last_data_time: 0.0032   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:05:02 d2.utils.events]: \u001b[0m eta: 1:55:32  iter: 3379  total_loss: 2.006  loss_cls_stage0: 0.05365  loss_box_reg_stage0: 0.3812  loss_cls_stage1: 0.05395  loss_box_reg_stage1: 0.5054  loss_cls_stage2: 0.06364  loss_box_reg_stage2: 0.6505  loss_mask: 0.1477  loss_rpn_cls: 0.007913  loss_rpn_loc: 0.05106    time: 0.7955  last_time: 0.8202  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:05:18 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:05:18 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:05:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:05:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:05:19 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:05:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:05:19 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:05:19 d2.utils.events]: \u001b[0m eta: 1:55:17  iter: 3399  total_loss: 2.148  loss_cls_stage0: 0.05559  loss_box_reg_stage0: 0.3852  loss_cls_stage1: 0.03749  loss_box_reg_stage1: 0.6066  loss_cls_stage2: 0.05688  loss_box_reg_stage2: 0.7618  loss_mask: 0.1421  loss_rpn_cls: 0.008227  loss_rpn_loc: 0.05038    time: 0.7957  last_time: 0.8189  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:05:35 d2.utils.events]: \u001b[0m eta: 1:55:05  iter: 3419  total_loss: 2.23  loss_cls_stage0: 0.07101  loss_box_reg_stage0: 0.3958  loss_cls_stage1: 0.09302  loss_box_reg_stage1: 0.5751  loss_cls_stage2: 0.1018  loss_box_reg_stage2: 0.7348  loss_mask: 0.154  loss_rpn_cls: 0.01057  loss_rpn_loc: 0.04528    time: 0.7958  last_time: 0.8099  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:05:51 d2.utils.events]: \u001b[0m eta: 1:54:56  iter: 3439  total_loss: 2.121  loss_cls_stage0: 0.06183  loss_box_reg_stage0: 0.3547  loss_cls_stage1: 0.06511  loss_box_reg_stage1: 0.5737  loss_cls_stage2: 0.08515  loss_box_reg_stage2: 0.684  loss_mask: 0.1569  loss_rpn_cls: 0.01025  loss_rpn_loc: 0.05482    time: 0.7959  last_time: 0.8189  data_time: 0.0018  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:06:08 d2.utils.events]: \u001b[0m eta: 1:54:48  iter: 3459  total_loss: 2.267  loss_cls_stage0: 0.05871  loss_box_reg_stage0: 0.4083  loss_cls_stage1: 0.06112  loss_box_reg_stage1: 0.5971  loss_cls_stage2: 0.08649  loss_box_reg_stage2: 0.7216  loss_mask: 0.1713  loss_rpn_cls: 0.008266  loss_rpn_loc: 0.0486    time: 0.7961  last_time: 0.8556  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:06:24 d2.utils.events]: \u001b[0m eta: 1:54:35  iter: 3479  total_loss: 2.259  loss_cls_stage0: 0.058  loss_box_reg_stage0: 0.3834  loss_cls_stage1: 0.06414  loss_box_reg_stage1: 0.6065  loss_cls_stage2: 0.06176  loss_box_reg_stage2: 0.8378  loss_mask: 0.1705  loss_rpn_cls: 0.01274  loss_rpn_loc: 0.06106    time: 0.7962  last_time: 0.8320  data_time: 0.0015  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:06:41 d2.utils.events]: \u001b[0m eta: 1:54:25  iter: 3499  total_loss: 2.056  loss_cls_stage0: 0.07385  loss_box_reg_stage0: 0.3362  loss_cls_stage1: 0.0397  loss_box_reg_stage1: 0.5632  loss_cls_stage2: 0.0714  loss_box_reg_stage2: 0.6661  loss_mask: 0.1521  loss_rpn_cls: 0.01179  loss_rpn_loc: 0.05071    time: 0.7963  last_time: 0.8236  data_time: 0.0019  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:06:57 d2.utils.events]: \u001b[0m eta: 1:54:15  iter: 3519  total_loss: 1.967  loss_cls_stage0: 0.05666  loss_box_reg_stage0: 0.3544  loss_cls_stage1: 0.05688  loss_box_reg_stage1: 0.5107  loss_cls_stage2: 0.08506  loss_box_reg_stage2: 0.7253  loss_mask: 0.1601  loss_rpn_cls: 0.009281  loss_rpn_loc: 0.06815    time: 0.7965  last_time: 0.8128  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:07:13 d2.utils.events]: \u001b[0m eta: 1:54:04  iter: 3539  total_loss: 2.118  loss_cls_stage0: 0.05317  loss_box_reg_stage0: 0.3938  loss_cls_stage1: 0.03825  loss_box_reg_stage1: 0.5506  loss_cls_stage2: 0.08302  loss_box_reg_stage2: 0.7607  loss_mask: 0.1675  loss_rpn_cls: 0.01244  loss_rpn_loc: 0.06214    time: 0.7966  last_time: 0.8095  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:07:30 d2.utils.events]: \u001b[0m eta: 1:53:53  iter: 3559  total_loss: 2.236  loss_cls_stage0: 0.07335  loss_box_reg_stage0: 0.4124  loss_cls_stage1: 0.06818  loss_box_reg_stage1: 0.583  loss_cls_stage2: 0.1004  loss_box_reg_stage2: 0.7026  loss_mask: 0.1605  loss_rpn_cls: 0.01542  loss_rpn_loc: 0.04305    time: 0.7967  last_time: 0.8066  data_time: 0.0019  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:07:46 d2.utils.events]: \u001b[0m eta: 1:53:40  iter: 3579  total_loss: 1.823  loss_cls_stage0: 0.0571  loss_box_reg_stage0: 0.3253  loss_cls_stage1: 0.0594  loss_box_reg_stage1: 0.4494  loss_cls_stage2: 0.062  loss_box_reg_stage2: 0.5153  loss_mask: 0.1669  loss_rpn_cls: 0.01092  loss_rpn_loc: 0.05693    time: 0.7968  last_time: 0.8210  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:08:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:08:02 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:08:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:08:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:08:02 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:08:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:08:02 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:08:02 d2.utils.events]: \u001b[0m eta: 1:53:26  iter: 3599  total_loss: 1.945  loss_cls_stage0: 0.04981  loss_box_reg_stage0: 0.3474  loss_cls_stage1: 0.03826  loss_box_reg_stage1: 0.4867  loss_cls_stage2: 0.0538  loss_box_reg_stage2: 0.6163  loss_mask: 0.1581  loss_rpn_cls: 0.01538  loss_rpn_loc: 0.05503    time: 0.7969  last_time: 0.8435  data_time: 0.0020  last_data_time: 0.0035   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:08:19 d2.utils.events]: \u001b[0m eta: 1:53:15  iter: 3619  total_loss: 2.022  loss_cls_stage0: 0.05172  loss_box_reg_stage0: 0.3434  loss_cls_stage1: 0.06368  loss_box_reg_stage1: 0.5042  loss_cls_stage2: 0.08709  loss_box_reg_stage2: 0.7292  loss_mask: 0.1604  loss_rpn_cls: 0.01319  loss_rpn_loc: 0.06206    time: 0.7970  last_time: 0.8334  data_time: 0.0015  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:08:35 d2.utils.events]: \u001b[0m eta: 1:53:00  iter: 3639  total_loss: 1.791  loss_cls_stage0: 0.05382  loss_box_reg_stage0: 0.2762  loss_cls_stage1: 0.04473  loss_box_reg_stage1: 0.4578  loss_cls_stage2: 0.07291  loss_box_reg_stage2: 0.573  loss_mask: 0.151  loss_rpn_cls: 0.01248  loss_rpn_loc: 0.05929    time: 0.7971  last_time: 0.8212  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:08:52 d2.utils.events]: \u001b[0m eta: 1:52:52  iter: 3659  total_loss: 1.964  loss_cls_stage0: 0.06699  loss_box_reg_stage0: 0.3433  loss_cls_stage1: 0.06043  loss_box_reg_stage1: 0.5622  loss_cls_stage2: 0.06984  loss_box_reg_stage2: 0.6618  loss_mask: 0.139  loss_rpn_cls: 0.008916  loss_rpn_loc: 0.04796    time: 0.7973  last_time: 0.8151  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:09:08 d2.utils.events]: \u001b[0m eta: 1:52:39  iter: 3679  total_loss: 2.023  loss_cls_stage0: 0.07584  loss_box_reg_stage0: 0.3751  loss_cls_stage1: 0.04778  loss_box_reg_stage1: 0.5118  loss_cls_stage2: 0.0718  loss_box_reg_stage2: 0.6216  loss_mask: 0.1536  loss_rpn_cls: 0.01011  loss_rpn_loc: 0.06377    time: 0.7974  last_time: 0.8254  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:09:24 d2.utils.events]: \u001b[0m eta: 1:52:23  iter: 3699  total_loss: 2.141  loss_cls_stage0: 0.0656  loss_box_reg_stage0: 0.3614  loss_cls_stage1: 0.06801  loss_box_reg_stage1: 0.5346  loss_cls_stage2: 0.08549  loss_box_reg_stage2: 0.7328  loss_mask: 0.1441  loss_rpn_cls: 0.01636  loss_rpn_loc: 0.05533    time: 0.7974  last_time: 0.8236  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:09:40 d2.utils.events]: \u001b[0m eta: 1:52:12  iter: 3719  total_loss: 2.188  loss_cls_stage0: 0.07545  loss_box_reg_stage0: 0.3926  loss_cls_stage1: 0.07158  loss_box_reg_stage1: 0.6279  loss_cls_stage2: 0.09221  loss_box_reg_stage2: 0.6736  loss_mask: 0.1558  loss_rpn_cls: 0.01564  loss_rpn_loc: 0.05029    time: 0.7975  last_time: 0.8183  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:09:57 d2.utils.events]: \u001b[0m eta: 1:51:58  iter: 3739  total_loss: 2.142  loss_cls_stage0: 0.05922  loss_box_reg_stage0: 0.3966  loss_cls_stage1: 0.05548  loss_box_reg_stage1: 0.5534  loss_cls_stage2: 0.08198  loss_box_reg_stage2: 0.5991  loss_mask: 0.1583  loss_rpn_cls: 0.01338  loss_rpn_loc: 0.05919    time: 0.7977  last_time: 0.8293  data_time: 0.0019  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:10:13 d2.utils.events]: \u001b[0m eta: 1:51:47  iter: 3759  total_loss: 1.956  loss_cls_stage0: 0.04837  loss_box_reg_stage0: 0.3573  loss_cls_stage1: 0.03796  loss_box_reg_stage1: 0.5377  loss_cls_stage2: 0.06557  loss_box_reg_stage2: 0.6761  loss_mask: 0.1525  loss_rpn_cls: 0.009578  loss_rpn_loc: 0.05608    time: 0.7978  last_time: 0.8475  data_time: 0.0018  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:10:31 d2.utils.events]: \u001b[0m eta: 1:51:33  iter: 3779  total_loss: 1.996  loss_cls_stage0: 0.05202  loss_box_reg_stage0: 0.3843  loss_cls_stage1: 0.04643  loss_box_reg_stage1: 0.4911  loss_cls_stage2: 0.07922  loss_box_reg_stage2: 0.627  loss_mask: 0.146  loss_rpn_cls: 0.01527  loss_rpn_loc: 0.04497    time: 0.7981  last_time: 0.8834  data_time: 0.0031  last_data_time: 0.0029   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:10:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:10:48 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:10:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:10:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:10:48 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:10:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:10:48 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:10:48 d2.utils.events]: \u001b[0m eta: 1:51:21  iter: 3799  total_loss: 2  loss_cls_stage0: 0.05654  loss_box_reg_stage0: 0.3653  loss_cls_stage1: 0.05793  loss_box_reg_stage1: 0.5318  loss_cls_stage2: 0.0669  loss_box_reg_stage2: 0.6267  loss_mask: 0.1381  loss_rpn_cls: 0.01414  loss_rpn_loc: 0.0422    time: 0.7985  last_time: 0.8293  data_time: 0.0027  last_data_time: 0.0035   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:11:05 d2.utils.events]: \u001b[0m eta: 1:51:09  iter: 3819  total_loss: 1.999  loss_cls_stage0: 0.0567  loss_box_reg_stage0: 0.349  loss_cls_stage1: 0.05436  loss_box_reg_stage1: 0.5214  loss_cls_stage2: 0.0678  loss_box_reg_stage2: 0.7132  loss_mask: 0.1614  loss_rpn_cls: 0.01161  loss_rpn_loc: 0.05218    time: 0.7987  last_time: 0.8057  data_time: 0.0024  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:11:21 d2.utils.events]: \u001b[0m eta: 1:50:54  iter: 3839  total_loss: 1.657  loss_cls_stage0: 0.0492  loss_box_reg_stage0: 0.3194  loss_cls_stage1: 0.04071  loss_box_reg_stage1: 0.4274  loss_cls_stage2: 0.04308  loss_box_reg_stage2: 0.5928  loss_mask: 0.1647  loss_rpn_cls: 0.01201  loss_rpn_loc: 0.06645    time: 0.7987  last_time: 0.8278  data_time: 0.0017  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:11:37 d2.utils.events]: \u001b[0m eta: 1:50:39  iter: 3859  total_loss: 1.849  loss_cls_stage0: 0.05624  loss_box_reg_stage0: 0.3079  loss_cls_stage1: 0.06359  loss_box_reg_stage1: 0.4263  loss_cls_stage2: 0.0705  loss_box_reg_stage2: 0.5727  loss_mask: 0.1476  loss_rpn_cls: 0.01048  loss_rpn_loc: 0.04953    time: 0.7989  last_time: 0.8043  data_time: 0.0024  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:11:54 d2.utils.events]: \u001b[0m eta: 1:50:23  iter: 3879  total_loss: 2.11  loss_cls_stage0: 0.06835  loss_box_reg_stage0: 0.3866  loss_cls_stage1: 0.04392  loss_box_reg_stage1: 0.5685  loss_cls_stage2: 0.07945  loss_box_reg_stage2: 0.7374  loss_mask: 0.1794  loss_rpn_cls: 0.01166  loss_rpn_loc: 0.04426    time: 0.7989  last_time: 0.7985  data_time: 0.0019  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:12:10 d2.utils.events]: \u001b[0m eta: 1:50:07  iter: 3899  total_loss: 2.014  loss_cls_stage0: 0.04318  loss_box_reg_stage0: 0.355  loss_cls_stage1: 0.02545  loss_box_reg_stage1: 0.5129  loss_cls_stage2: 0.0508  loss_box_reg_stage2: 0.6826  loss_mask: 0.1451  loss_rpn_cls: 0.01026  loss_rpn_loc: 0.05374    time: 0.7990  last_time: 0.8120  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:12:26 d2.utils.events]: \u001b[0m eta: 1:49:51  iter: 3919  total_loss: 1.927  loss_cls_stage0: 0.06307  loss_box_reg_stage0: 0.3462  loss_cls_stage1: 0.04493  loss_box_reg_stage1: 0.5088  loss_cls_stage2: 0.05502  loss_box_reg_stage2: 0.6384  loss_mask: 0.1517  loss_rpn_cls: 0.01075  loss_rpn_loc: 0.06667    time: 0.7990  last_time: 0.8127  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:12:42 d2.utils.events]: \u001b[0m eta: 1:49:36  iter: 3939  total_loss: 1.846  loss_cls_stage0: 0.05509  loss_box_reg_stage0: 0.3245  loss_cls_stage1: 0.0307  loss_box_reg_stage1: 0.4946  loss_cls_stage2: 0.04304  loss_box_reg_stage2: 0.6489  loss_mask: 0.1392  loss_rpn_cls: 0.007076  loss_rpn_loc: 0.06313    time: 0.7991  last_time: 0.8163  data_time: 0.0020  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:12:58 d2.utils.events]: \u001b[0m eta: 1:49:19  iter: 3959  total_loss: 1.906  loss_cls_stage0: 0.05947  loss_box_reg_stage0: 0.3775  loss_cls_stage1: 0.06203  loss_box_reg_stage1: 0.5206  loss_cls_stage2: 0.07564  loss_box_reg_stage2: 0.7088  loss_mask: 0.1497  loss_rpn_cls: 0.008829  loss_rpn_loc: 0.05179    time: 0.7991  last_time: 0.8009  data_time: 0.0018  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:13:14 d2.utils.events]: \u001b[0m eta: 1:49:03  iter: 3979  total_loss: 1.833  loss_cls_stage0: 0.0632  loss_box_reg_stage0: 0.3209  loss_cls_stage1: 0.05766  loss_box_reg_stage1: 0.4875  loss_cls_stage2: 0.07589  loss_box_reg_stage2: 0.6451  loss_mask: 0.1559  loss_rpn_cls: 0.01334  loss_rpn_loc: 0.0457    time: 0.7991  last_time: 0.7909  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:13:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:13:30 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:13:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:13:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:13:30 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:13:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:13:30 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:13:30 d2.utils.events]: \u001b[0m eta: 1:48:46  iter: 3999  total_loss: 2.104  loss_cls_stage0: 0.0829  loss_box_reg_stage0: 0.3476  loss_cls_stage1: 0.05123  loss_box_reg_stage1: 0.5394  loss_cls_stage2: 0.09676  loss_box_reg_stage2: 0.6656  loss_mask: 0.1706  loss_rpn_cls: 0.0155  loss_rpn_loc: 0.0479    time: 0.7991  last_time: 0.8370  data_time: 0.0019  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:13:48 d2.utils.events]: \u001b[0m eta: 1:48:32  iter: 4019  total_loss: 2.093  loss_cls_stage0: 0.0607  loss_box_reg_stage0: 0.3782  loss_cls_stage1: 0.04706  loss_box_reg_stage1: 0.5719  loss_cls_stage2: 0.07279  loss_box_reg_stage2: 0.6539  loss_mask: 0.1637  loss_rpn_cls: 0.01131  loss_rpn_loc: 0.0503    time: 0.7996  last_time: 1.0970  data_time: 0.0073  last_data_time: 0.0251   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:14:07 d2.utils.events]: \u001b[0m eta: 1:48:22  iter: 4039  total_loss: 1.786  loss_cls_stage0: 0.05323  loss_box_reg_stage0: 0.3322  loss_cls_stage1: 0.06202  loss_box_reg_stage1: 0.4829  loss_cls_stage2: 0.0672  loss_box_reg_stage2: 0.6305  loss_mask: 0.138  loss_rpn_cls: 0.008699  loss_rpn_loc: 0.03856    time: 0.8002  last_time: 0.8903  data_time: 0.0050  last_data_time: 0.0033   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:14:25 d2.utils.events]: \u001b[0m eta: 1:48:10  iter: 4059  total_loss: 2.054  loss_cls_stage0: 0.05555  loss_box_reg_stage0: 0.3627  loss_cls_stage1: 0.05744  loss_box_reg_stage1: 0.548  loss_cls_stage2: 0.06901  loss_box_reg_stage2: 0.7154  loss_mask: 0.1623  loss_rpn_cls: 0.01025  loss_rpn_loc: 0.04356    time: 0.8007  last_time: 0.9028  data_time: 0.0051  last_data_time: 0.0037   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:14:44 d2.utils.events]: \u001b[0m eta: 1:47:57  iter: 4079  total_loss: 1.862  loss_cls_stage0: 0.05742  loss_box_reg_stage0: 0.3441  loss_cls_stage1: 0.05245  loss_box_reg_stage1: 0.4911  loss_cls_stage2: 0.05441  loss_box_reg_stage2: 0.5925  loss_mask: 0.1515  loss_rpn_cls: 0.01112  loss_rpn_loc: 0.05971    time: 0.8015  last_time: 0.9448  data_time: 0.0074  last_data_time: 0.0098   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:15:01 d2.utils.events]: \u001b[0m eta: 1:47:41  iter: 4099  total_loss: 1.947  loss_cls_stage0: 0.06034  loss_box_reg_stage0: 0.3431  loss_cls_stage1: 0.04542  loss_box_reg_stage1: 0.5567  loss_cls_stage2: 0.06339  loss_box_reg_stage2: 0.6823  loss_mask: 0.1525  loss_rpn_cls: 0.01372  loss_rpn_loc: 0.06899    time: 0.8017  last_time: 0.8090  data_time: 0.0039  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:15:17 d2.utils.events]: \u001b[0m eta: 1:47:24  iter: 4119  total_loss: 1.634  loss_cls_stage0: 0.0529  loss_box_reg_stage0: 0.3279  loss_cls_stage1: 0.04477  loss_box_reg_stage1: 0.3969  loss_cls_stage2: 0.05319  loss_box_reg_stage2: 0.5769  loss_mask: 0.1477  loss_rpn_cls: 0.009442  loss_rpn_loc: 0.04816    time: 0.8017  last_time: 0.7979  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:15:33 d2.utils.events]: \u001b[0m eta: 1:47:05  iter: 4139  total_loss: 2.053  loss_cls_stage0: 0.0755  loss_box_reg_stage0: 0.4113  loss_cls_stage1: 0.04834  loss_box_reg_stage1: 0.568  loss_cls_stage2: 0.0806  loss_box_reg_stage2: 0.5919  loss_mask: 0.1711  loss_rpn_cls: 0.01214  loss_rpn_loc: 0.06276    time: 0.8017  last_time: 0.7898  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:15:49 d2.utils.events]: \u001b[0m eta: 1:46:43  iter: 4159  total_loss: 1.985  loss_cls_stage0: 0.05252  loss_box_reg_stage0: 0.3749  loss_cls_stage1: 0.04686  loss_box_reg_stage1: 0.5234  loss_cls_stage2: 0.07087  loss_box_reg_stage2: 0.712  loss_mask: 0.1503  loss_rpn_cls: 0.007698  loss_rpn_loc: 0.05154    time: 0.8017  last_time: 0.7881  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:16:05 d2.utils.events]: \u001b[0m eta: 1:46:19  iter: 4179  total_loss: 1.777  loss_cls_stage0: 0.03598  loss_box_reg_stage0: 0.3092  loss_cls_stage1: 0.02693  loss_box_reg_stage1: 0.4399  loss_cls_stage2: 0.04355  loss_box_reg_stage2: 0.6568  loss_mask: 0.1352  loss_rpn_cls: 0.00844  loss_rpn_loc: 0.06125    time: 0.8017  last_time: 0.8003  data_time: 0.0019  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:16:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:16:21 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:16:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:16:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:16:21 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:16:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:16:21 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:16:21 d2.utils.events]: \u001b[0m eta: 1:45:58  iter: 4199  total_loss: 2.041  loss_cls_stage0: 0.04859  loss_box_reg_stage0: 0.378  loss_cls_stage1: 0.05046  loss_box_reg_stage1: 0.5692  loss_cls_stage2: 0.06876  loss_box_reg_stage2: 0.7215  loss_mask: 0.1593  loss_rpn_cls: 0.009304  loss_rpn_loc: 0.04908    time: 0.8016  last_time: 0.8048  data_time: 0.0017  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:16:37 d2.utils.events]: \u001b[0m eta: 1:45:37  iter: 4219  total_loss: 1.54  loss_cls_stage0: 0.04837  loss_box_reg_stage0: 0.2882  loss_cls_stage1: 0.03532  loss_box_reg_stage1: 0.3738  loss_cls_stage2: 0.06601  loss_box_reg_stage2: 0.5362  loss_mask: 0.1386  loss_rpn_cls: 0.01136  loss_rpn_loc: 0.04944    time: 0.8016  last_time: 0.7862  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:16:53 d2.utils.events]: \u001b[0m eta: 1:45:19  iter: 4239  total_loss: 2.034  loss_cls_stage0: 0.0563  loss_box_reg_stage0: 0.3425  loss_cls_stage1: 0.04956  loss_box_reg_stage1: 0.5361  loss_cls_stage2: 0.07456  loss_box_reg_stage2: 0.7089  loss_mask: 0.1435  loss_rpn_cls: 0.01192  loss_rpn_loc: 0.0467    time: 0.8016  last_time: 0.7960  data_time: 0.0017  last_data_time: 0.0037   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:17:09 d2.utils.events]: \u001b[0m eta: 1:45:01  iter: 4259  total_loss: 1.692  loss_cls_stage0: 0.04281  loss_box_reg_stage0: 0.3101  loss_cls_stage1: 0.02999  loss_box_reg_stage1: 0.4675  loss_cls_stage2: 0.05177  loss_box_reg_stage2: 0.6061  loss_mask: 0.1328  loss_rpn_cls: 0.01053  loss_rpn_loc: 0.04446    time: 0.8016  last_time: 0.8043  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:17:25 d2.utils.events]: \u001b[0m eta: 1:44:44  iter: 4279  total_loss: 2.228  loss_cls_stage0: 0.06896  loss_box_reg_stage0: 0.3724  loss_cls_stage1: 0.05723  loss_box_reg_stage1: 0.5957  loss_cls_stage2: 0.09347  loss_box_reg_stage2: 0.6591  loss_mask: 0.1434  loss_rpn_cls: 0.008809  loss_rpn_loc: 0.04971    time: 0.8016  last_time: 0.8035  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:17:41 d2.utils.events]: \u001b[0m eta: 1:44:24  iter: 4299  total_loss: 2.002  loss_cls_stage0: 0.06277  loss_box_reg_stage0: 0.3589  loss_cls_stage1: 0.0461  loss_box_reg_stage1: 0.5477  loss_cls_stage2: 0.06511  loss_box_reg_stage2: 0.6334  loss_mask: 0.1578  loss_rpn_cls: 0.007438  loss_rpn_loc: 0.05813    time: 0.8016  last_time: 0.7957  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:17:57 d2.utils.events]: \u001b[0m eta: 1:44:03  iter: 4319  total_loss: 1.932  loss_cls_stage0: 0.05692  loss_box_reg_stage0: 0.3299  loss_cls_stage1: 0.03538  loss_box_reg_stage1: 0.4855  loss_cls_stage2: 0.06096  loss_box_reg_stage2: 0.6584  loss_mask: 0.1474  loss_rpn_cls: 0.00668  loss_rpn_loc: 0.06136    time: 0.8016  last_time: 0.7960  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:18:13 d2.utils.events]: \u001b[0m eta: 1:43:44  iter: 4339  total_loss: 2.16  loss_cls_stage0: 0.04905  loss_box_reg_stage0: 0.3691  loss_cls_stage1: 0.06246  loss_box_reg_stage1: 0.5687  loss_cls_stage2: 0.07224  loss_box_reg_stage2: 0.7693  loss_mask: 0.1633  loss_rpn_cls: 0.01165  loss_rpn_loc: 0.05058    time: 0.8016  last_time: 0.8048  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:18:29 d2.utils.events]: \u001b[0m eta: 1:43:24  iter: 4359  total_loss: 1.992  loss_cls_stage0: 0.07964  loss_box_reg_stage0: 0.3538  loss_cls_stage1: 0.07194  loss_box_reg_stage1: 0.5405  loss_cls_stage2: 0.0933  loss_box_reg_stage2: 0.642  loss_mask: 0.1365  loss_rpn_cls: 0.02199  loss_rpn_loc: 0.05437    time: 0.8016  last_time: 0.8010  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:18:45 d2.utils.events]: \u001b[0m eta: 1:43:03  iter: 4379  total_loss: 1.919  loss_cls_stage0: 0.09551  loss_box_reg_stage0: 0.3752  loss_cls_stage1: 0.07938  loss_box_reg_stage1: 0.4596  loss_cls_stage2: 0.07943  loss_box_reg_stage2: 0.5277  loss_mask: 0.1529  loss_rpn_cls: 0.02461  loss_rpn_loc: 0.0477    time: 0.8015  last_time: 0.7910  data_time: 0.0019  last_data_time: 0.0032   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:19:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:19:01 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:19:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:19:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:19:01 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:19:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:19:01 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:19:01 d2.utils.events]: \u001b[0m eta: 1:42:41  iter: 4399  total_loss: 2.169  loss_cls_stage0: 0.08305  loss_box_reg_stage0: 0.3878  loss_cls_stage1: 0.09973  loss_box_reg_stage1: 0.5375  loss_cls_stage2: 0.0914  loss_box_reg_stage2: 0.6996  loss_mask: 0.1509  loss_rpn_cls: 0.0209  loss_rpn_loc: 0.05887    time: 0.8015  last_time: 0.7986  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:19:17 d2.utils.events]: \u001b[0m eta: 1:42:21  iter: 4419  total_loss: 1.939  loss_cls_stage0: 0.06859  loss_box_reg_stage0: 0.3428  loss_cls_stage1: 0.05609  loss_box_reg_stage1: 0.4911  loss_cls_stage2: 0.08137  loss_box_reg_stage2: 0.6494  loss_mask: 0.1474  loss_rpn_cls: 0.01667  loss_rpn_loc: 0.04006    time: 0.8015  last_time: 0.7897  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:19:33 d2.utils.events]: \u001b[0m eta: 1:42:00  iter: 4439  total_loss: 2.131  loss_cls_stage0: 0.04986  loss_box_reg_stage0: 0.3596  loss_cls_stage1: 0.06146  loss_box_reg_stage1: 0.5748  loss_cls_stage2: 0.09354  loss_box_reg_stage2: 0.7401  loss_mask: 0.1765  loss_rpn_cls: 0.01584  loss_rpn_loc: 0.07656    time: 0.8015  last_time: 0.7976  data_time: 0.0018  last_data_time: 0.0031   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:19:49 d2.utils.events]: \u001b[0m eta: 1:41:40  iter: 4459  total_loss: 1.957  loss_cls_stage0: 0.06712  loss_box_reg_stage0: 0.3406  loss_cls_stage1: 0.04685  loss_box_reg_stage1: 0.5076  loss_cls_stage2: 0.0722  loss_box_reg_stage2: 0.6566  loss_mask: 0.1586  loss_rpn_cls: 0.01283  loss_rpn_loc: 0.07043    time: 0.8015  last_time: 0.7939  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:20:05 d2.utils.events]: \u001b[0m eta: 1:41:19  iter: 4479  total_loss: 2.169  loss_cls_stage0: 0.1033  loss_box_reg_stage0: 0.3957  loss_cls_stage1: 0.09246  loss_box_reg_stage1: 0.547  loss_cls_stage2: 0.116  loss_box_reg_stage2: 0.6112  loss_mask: 0.1592  loss_rpn_cls: 0.02  loss_rpn_loc: 0.04907    time: 0.8015  last_time: 0.7998  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:20:21 d2.utils.events]: \u001b[0m eta: 1:40:59  iter: 4499  total_loss: 2.284  loss_cls_stage0: 0.09213  loss_box_reg_stage0: 0.3641  loss_cls_stage1: 0.07949  loss_box_reg_stage1: 0.6039  loss_cls_stage2: 0.101  loss_box_reg_stage2: 0.6999  loss_mask: 0.171  loss_rpn_cls: 0.0168  loss_rpn_loc: 0.07948    time: 0.8015  last_time: 0.8053  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:20:37 d2.utils.events]: \u001b[0m eta: 1:40:36  iter: 4519  total_loss: 1.924  loss_cls_stage0: 0.06114  loss_box_reg_stage0: 0.3307  loss_cls_stage1: 0.06638  loss_box_reg_stage1: 0.457  loss_cls_stage2: 0.07986  loss_box_reg_stage2: 0.6361  loss_mask: 0.1527  loss_rpn_cls: 0.01547  loss_rpn_loc: 0.05722    time: 0.8015  last_time: 0.7983  data_time: 0.0017  last_data_time: 0.0035   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:20:53 d2.utils.events]: \u001b[0m eta: 1:40:18  iter: 4539  total_loss: 2.263  loss_cls_stage0: 0.04686  loss_box_reg_stage0: 0.3934  loss_cls_stage1: 0.05305  loss_box_reg_stage1: 0.5896  loss_cls_stage2: 0.09091  loss_box_reg_stage2: 0.6552  loss_mask: 0.1833  loss_rpn_cls: 0.01089  loss_rpn_loc: 0.06035    time: 0.8014  last_time: 0.8143  data_time: 0.0020  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:21:09 d2.utils.events]: \u001b[0m eta: 1:39:57  iter: 4559  total_loss: 2.063  loss_cls_stage0: 0.08159  loss_box_reg_stage0: 0.3734  loss_cls_stage1: 0.07563  loss_box_reg_stage1: 0.5247  loss_cls_stage2: 0.08499  loss_box_reg_stage2: 0.6348  loss_mask: 0.1428  loss_rpn_cls: 0.02073  loss_rpn_loc: 0.05305    time: 0.8014  last_time: 0.7950  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:21:25 d2.utils.events]: \u001b[0m eta: 1:39:38  iter: 4579  total_loss: 1.951  loss_cls_stage0: 0.06071  loss_box_reg_stage0: 0.3556  loss_cls_stage1: 0.05005  loss_box_reg_stage1: 0.5105  loss_cls_stage2: 0.06595  loss_box_reg_stage2: 0.6131  loss_mask: 0.182  loss_rpn_cls: 0.0146  loss_rpn_loc: 0.06108    time: 0.8014  last_time: 0.7984  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:21:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:21:41 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:21:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:21:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:21:41 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:21:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:21:41 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:21:41 d2.utils.events]: \u001b[0m eta: 1:39:17  iter: 4599  total_loss: 1.772  loss_cls_stage0: 0.07737  loss_box_reg_stage0: 0.3169  loss_cls_stage1: 0.05448  loss_box_reg_stage1: 0.4459  loss_cls_stage2: 0.07422  loss_box_reg_stage2: 0.5451  loss_mask: 0.1675  loss_rpn_cls: 0.01149  loss_rpn_loc: 0.05965    time: 0.8014  last_time: 0.8023  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:21:57 d2.utils.events]: \u001b[0m eta: 1:38:59  iter: 4619  total_loss: 1.619  loss_cls_stage0: 0.05906  loss_box_reg_stage0: 0.2703  loss_cls_stage1: 0.04251  loss_box_reg_stage1: 0.4099  loss_cls_stage2: 0.03872  loss_box_reg_stage2: 0.5586  loss_mask: 0.1485  loss_rpn_cls: 0.01239  loss_rpn_loc: 0.05321    time: 0.8014  last_time: 0.7854  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:22:13 d2.utils.events]: \u001b[0m eta: 1:38:41  iter: 4639  total_loss: 1.924  loss_cls_stage0: 0.04719  loss_box_reg_stage0: 0.3477  loss_cls_stage1: 0.05021  loss_box_reg_stage1: 0.4824  loss_cls_stage2: 0.06521  loss_box_reg_stage2: 0.5669  loss_mask: 0.1432  loss_rpn_cls: 0.01466  loss_rpn_loc: 0.05411    time: 0.8014  last_time: 0.7930  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:22:29 d2.utils.events]: \u001b[0m eta: 1:38:21  iter: 4659  total_loss: 1.778  loss_cls_stage0: 0.03304  loss_box_reg_stage0: 0.2697  loss_cls_stage1: 0.03552  loss_box_reg_stage1: 0.4665  loss_cls_stage2: 0.04333  loss_box_reg_stage2: 0.5869  loss_mask: 0.1535  loss_rpn_cls: 0.008802  loss_rpn_loc: 0.04393    time: 0.8013  last_time: 0.8003  data_time: 0.0019  last_data_time: 0.0038   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:22:45 d2.utils.events]: \u001b[0m eta: 1:38:01  iter: 4679  total_loss: 1.939  loss_cls_stage0: 0.08002  loss_box_reg_stage0: 0.3301  loss_cls_stage1: 0.04858  loss_box_reg_stage1: 0.4864  loss_cls_stage2: 0.06196  loss_box_reg_stage2: 0.7061  loss_mask: 0.1612  loss_rpn_cls: 0.01004  loss_rpn_loc: 0.05976    time: 0.8013  last_time: 0.8023  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:23:00 d2.utils.events]: \u001b[0m eta: 1:37:44  iter: 4699  total_loss: 1.654  loss_cls_stage0: 0.04849  loss_box_reg_stage0: 0.3173  loss_cls_stage1: 0.03847  loss_box_reg_stage1: 0.4277  loss_cls_stage2: 0.04878  loss_box_reg_stage2: 0.6341  loss_mask: 0.1433  loss_rpn_cls: 0.01159  loss_rpn_loc: 0.04662    time: 0.8013  last_time: 0.7820  data_time: 0.0017  last_data_time: 0.0011   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:23:16 d2.utils.events]: \u001b[0m eta: 1:37:23  iter: 4719  total_loss: 1.785  loss_cls_stage0: 0.04978  loss_box_reg_stage0: 0.3604  loss_cls_stage1: 0.06426  loss_box_reg_stage1: 0.4503  loss_cls_stage2: 0.06984  loss_box_reg_stage2: 0.5874  loss_mask: 0.1877  loss_rpn_cls: 0.01129  loss_rpn_loc: 0.07115    time: 0.8013  last_time: 0.7915  data_time: 0.0019  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:23:32 d2.utils.events]: \u001b[0m eta: 1:37:03  iter: 4739  total_loss: 1.862  loss_cls_stage0: 0.04878  loss_box_reg_stage0: 0.3417  loss_cls_stage1: 0.04651  loss_box_reg_stage1: 0.484  loss_cls_stage2: 0.05166  loss_box_reg_stage2: 0.5383  loss_mask: 0.1489  loss_rpn_cls: 0.01316  loss_rpn_loc: 0.04928    time: 0.8013  last_time: 0.7928  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:23:49 d2.utils.events]: \u001b[0m eta: 1:36:45  iter: 4759  total_loss: 1.804  loss_cls_stage0: 0.04471  loss_box_reg_stage0: 0.3293  loss_cls_stage1: 0.03514  loss_box_reg_stage1: 0.4891  loss_cls_stage2: 0.0536  loss_box_reg_stage2: 0.5481  loss_mask: 0.1393  loss_rpn_cls: 0.009817  loss_rpn_loc: 0.05683    time: 0.8013  last_time: 0.8043  data_time: 0.0019  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:24:05 d2.utils.events]: \u001b[0m eta: 1:36:25  iter: 4779  total_loss: 1.824  loss_cls_stage0: 0.0455  loss_box_reg_stage0: 0.3637  loss_cls_stage1: 0.04129  loss_box_reg_stage1: 0.4623  loss_cls_stage2: 0.06636  loss_box_reg_stage2: 0.6509  loss_mask: 0.1414  loss_rpn_cls: 0.009333  loss_rpn_loc: 0.0463    time: 0.8013  last_time: 0.7968  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:24:21 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:24:21 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:24:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:24:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:24:21 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:24:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:24:21 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:24:21 d2.utils.events]: \u001b[0m eta: 1:36:07  iter: 4799  total_loss: 1.774  loss_cls_stage0: 0.03767  loss_box_reg_stage0: 0.3456  loss_cls_stage1: 0.03921  loss_box_reg_stage1: 0.4818  loss_cls_stage2: 0.05969  loss_box_reg_stage2: 0.6327  loss_mask: 0.1596  loss_rpn_cls: 0.009982  loss_rpn_loc: 0.0537    time: 0.8012  last_time: 0.7982  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:24:37 d2.utils.events]: \u001b[0m eta: 1:35:49  iter: 4819  total_loss: 1.76  loss_cls_stage0: 0.04532  loss_box_reg_stage0: 0.3219  loss_cls_stage1: 0.02506  loss_box_reg_stage1: 0.4915  loss_cls_stage2: 0.04533  loss_box_reg_stage2: 0.5844  loss_mask: 0.1301  loss_rpn_cls: 0.00825  loss_rpn_loc: 0.05295    time: 0.8012  last_time: 0.8062  data_time: 0.0018  last_data_time: 0.0036   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:24:53 d2.utils.events]: \u001b[0m eta: 1:35:32  iter: 4839  total_loss: 1.791  loss_cls_stage0: 0.0409  loss_box_reg_stage0: 0.3142  loss_cls_stage1: 0.03186  loss_box_reg_stage1: 0.4949  loss_cls_stage2: 0.0364  loss_box_reg_stage2: 0.6666  loss_mask: 0.1402  loss_rpn_cls: 0.008353  loss_rpn_loc: 0.0419    time: 0.8013  last_time: 0.7951  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:25:09 d2.utils.events]: \u001b[0m eta: 1:35:15  iter: 4859  total_loss: 1.989  loss_cls_stage0: 0.05288  loss_box_reg_stage0: 0.3553  loss_cls_stage1: 0.02707  loss_box_reg_stage1: 0.5394  loss_cls_stage2: 0.03909  loss_box_reg_stage2: 0.7093  loss_mask: 0.1401  loss_rpn_cls: 0.007724  loss_rpn_loc: 0.05115    time: 0.8013  last_time: 0.7914  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:25:25 d2.utils.events]: \u001b[0m eta: 1:34:58  iter: 4879  total_loss: 2.059  loss_cls_stage0: 0.05411  loss_box_reg_stage0: 0.3752  loss_cls_stage1: 0.04267  loss_box_reg_stage1: 0.5683  loss_cls_stage2: 0.08142  loss_box_reg_stage2: 0.6831  loss_mask: 0.1266  loss_rpn_cls: 0.0106  loss_rpn_loc: 0.04466    time: 0.8013  last_time: 0.8233  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:25:41 d2.utils.events]: \u001b[0m eta: 1:34:41  iter: 4899  total_loss: 1.771  loss_cls_stage0: 0.04076  loss_box_reg_stage0: 0.3164  loss_cls_stage1: 0.025  loss_box_reg_stage1: 0.4589  loss_cls_stage2: 0.0296  loss_box_reg_stage2: 0.5945  loss_mask: 0.1368  loss_rpn_cls: 0.01174  loss_rpn_loc: 0.04394    time: 0.8013  last_time: 0.8003  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:25:57 d2.utils.events]: \u001b[0m eta: 1:34:24  iter: 4919  total_loss: 1.731  loss_cls_stage0: 0.04793  loss_box_reg_stage0: 0.3566  loss_cls_stage1: 0.03992  loss_box_reg_stage1: 0.4359  loss_cls_stage2: 0.0417  loss_box_reg_stage2: 0.5601  loss_mask: 0.1497  loss_rpn_cls: 0.007728  loss_rpn_loc: 0.04368    time: 0.8013  last_time: 0.7990  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:26:13 d2.utils.events]: \u001b[0m eta: 1:34:07  iter: 4939  total_loss: 1.71  loss_cls_stage0: 0.04287  loss_box_reg_stage0: 0.3063  loss_cls_stage1: 0.03045  loss_box_reg_stage1: 0.4424  loss_cls_stage2: 0.04017  loss_box_reg_stage2: 0.5712  loss_mask: 0.1355  loss_rpn_cls: 0.008046  loss_rpn_loc: 0.05282    time: 0.8013  last_time: 0.7878  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:26:29 d2.utils.events]: \u001b[0m eta: 1:33:51  iter: 4959  total_loss: 1.84  loss_cls_stage0: 0.05913  loss_box_reg_stage0: 0.3458  loss_cls_stage1: 0.06545  loss_box_reg_stage1: 0.4824  loss_cls_stage2: 0.07402  loss_box_reg_stage2: 0.6557  loss_mask: 0.1375  loss_rpn_cls: 0.01424  loss_rpn_loc: 0.05123    time: 0.8013  last_time: 0.8244  data_time: 0.0017  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:26:45 d2.utils.events]: \u001b[0m eta: 1:33:34  iter: 4979  total_loss: 1.792  loss_cls_stage0: 0.04836  loss_box_reg_stage0: 0.352  loss_cls_stage1: 0.04407  loss_box_reg_stage1: 0.4387  loss_cls_stage2: 0.05156  loss_box_reg_stage2: 0.5698  loss_mask: 0.1606  loss_rpn_cls: 0.0153  loss_rpn_loc: 0.04643    time: 0.8013  last_time: 0.7986  data_time: 0.0015  last_data_time: 0.0028   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:27:02 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:27:02 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:27:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:27:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:27:02 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:27:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:27:02 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:27:02 d2.utils.events]: \u001b[0m eta: 1:33:19  iter: 4999  total_loss: 1.837  loss_cls_stage0: 0.05121  loss_box_reg_stage0: 0.331  loss_cls_stage1: 0.03673  loss_box_reg_stage1: 0.4922  loss_cls_stage2: 0.04715  loss_box_reg_stage2: 0.5825  loss_mask: 0.159  loss_rpn_cls: 0.008416  loss_rpn_loc: 0.05679    time: 0.8013  last_time: 0.7915  data_time: 0.0018  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:27:18 d2.utils.events]: \u001b[0m eta: 1:33:01  iter: 5019  total_loss: 1.925  loss_cls_stage0: 0.05694  loss_box_reg_stage0: 0.3285  loss_cls_stage1: 0.05671  loss_box_reg_stage1: 0.4698  loss_cls_stage2: 0.08102  loss_box_reg_stage2: 0.6425  loss_mask: 0.1794  loss_rpn_cls: 0.01627  loss_rpn_loc: 0.05854    time: 0.8013  last_time: 0.7938  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:27:34 d2.utils.events]: \u001b[0m eta: 1:32:43  iter: 5039  total_loss: 1.806  loss_cls_stage0: 0.06298  loss_box_reg_stage0: 0.3378  loss_cls_stage1: 0.03751  loss_box_reg_stage1: 0.49  loss_cls_stage2: 0.05084  loss_box_reg_stage2: 0.6357  loss_mask: 0.1409  loss_rpn_cls: 0.01175  loss_rpn_loc: 0.03722    time: 0.8013  last_time: 0.7964  data_time: 0.0016  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:27:50 d2.utils.events]: \u001b[0m eta: 1:32:26  iter: 5059  total_loss: 1.584  loss_cls_stage0: 0.04647  loss_box_reg_stage0: 0.3018  loss_cls_stage1: 0.03214  loss_box_reg_stage1: 0.4189  loss_cls_stage2: 0.03317  loss_box_reg_stage2: 0.5406  loss_mask: 0.1279  loss_rpn_cls: 0.006493  loss_rpn_loc: 0.04401    time: 0.8013  last_time: 0.7785  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:28:06 d2.utils.events]: \u001b[0m eta: 1:32:09  iter: 5079  total_loss: 1.67  loss_cls_stage0: 0.04565  loss_box_reg_stage0: 0.3104  loss_cls_stage1: 0.03791  loss_box_reg_stage1: 0.4246  loss_cls_stage2: 0.03915  loss_box_reg_stage2: 0.5747  loss_mask: 0.1426  loss_rpn_cls: 0.009321  loss_rpn_loc: 0.05784    time: 0.8013  last_time: 0.7967  data_time: 0.0017  last_data_time: 0.0028   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:28:22 d2.utils.events]: \u001b[0m eta: 1:31:52  iter: 5099  total_loss: 1.655  loss_cls_stage0: 0.04081  loss_box_reg_stage0: 0.3299  loss_cls_stage1: 0.03931  loss_box_reg_stage1: 0.4572  loss_cls_stage2: 0.05624  loss_box_reg_stage2: 0.611  loss_mask: 0.1387  loss_rpn_cls: 0.007641  loss_rpn_loc: 0.04205    time: 0.8012  last_time: 0.7930  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:28:38 d2.utils.events]: \u001b[0m eta: 1:31:35  iter: 5119  total_loss: 1.984  loss_cls_stage0: 0.06718  loss_box_reg_stage0: 0.3219  loss_cls_stage1: 0.0469  loss_box_reg_stage1: 0.4968  loss_cls_stage2: 0.05811  loss_box_reg_stage2: 0.6761  loss_mask: 0.1607  loss_rpn_cls: 0.01148  loss_rpn_loc: 0.04342    time: 0.8012  last_time: 0.8350  data_time: 0.0018  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:28:54 d2.utils.events]: \u001b[0m eta: 1:31:20  iter: 5139  total_loss: 2.07  loss_cls_stage0: 0.07284  loss_box_reg_stage0: 0.3752  loss_cls_stage1: 0.06372  loss_box_reg_stage1: 0.5591  loss_cls_stage2: 0.0904  loss_box_reg_stage2: 0.6779  loss_mask: 0.1546  loss_rpn_cls: 0.01032  loss_rpn_loc: 0.05029    time: 0.8012  last_time: 0.7886  data_time: 0.0019  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:29:10 d2.utils.events]: \u001b[0m eta: 1:31:03  iter: 5159  total_loss: 1.93  loss_cls_stage0: 0.06116  loss_box_reg_stage0: 0.3252  loss_cls_stage1: 0.05181  loss_box_reg_stage1: 0.5211  loss_cls_stage2: 0.0542  loss_box_reg_stage2: 0.6414  loss_mask: 0.1654  loss_rpn_cls: 0.01177  loss_rpn_loc: 0.05452    time: 0.8012  last_time: 0.8161  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:29:26 d2.utils.events]: \u001b[0m eta: 1:30:46  iter: 5179  total_loss: 1.686  loss_cls_stage0: 0.06089  loss_box_reg_stage0: 0.2868  loss_cls_stage1: 0.05163  loss_box_reg_stage1: 0.4312  loss_cls_stage2: 0.05927  loss_box_reg_stage2: 0.6301  loss_mask: 0.1313  loss_rpn_cls: 0.01366  loss_rpn_loc: 0.06269    time: 0.8012  last_time: 0.7691  data_time: 0.0017  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:29:42 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:29:42 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:29:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:29:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:29:42 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:29:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:29:42 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:29:42 d2.utils.events]: \u001b[0m eta: 1:30:30  iter: 5199  total_loss: 1.797  loss_cls_stage0: 0.05832  loss_box_reg_stage0: 0.3796  loss_cls_stage1: 0.04297  loss_box_reg_stage1: 0.5021  loss_cls_stage2: 0.04173  loss_box_reg_stage2: 0.6814  loss_mask: 0.1483  loss_rpn_cls: 0.008876  loss_rpn_loc: 0.04771    time: 0.8012  last_time: 0.8011  data_time: 0.0015  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:29:58 d2.utils.events]: \u001b[0m eta: 1:30:14  iter: 5219  total_loss: 1.78  loss_cls_stage0: 0.04625  loss_box_reg_stage0: 0.3338  loss_cls_stage1: 0.04126  loss_box_reg_stage1: 0.4771  loss_cls_stage2: 0.05672  loss_box_reg_stage2: 0.5768  loss_mask: 0.146  loss_rpn_cls: 0.006759  loss_rpn_loc: 0.03432    time: 0.8012  last_time: 0.8168  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:30:14 d2.utils.events]: \u001b[0m eta: 1:29:58  iter: 5239  total_loss: 1.961  loss_cls_stage0: 0.05626  loss_box_reg_stage0: 0.3274  loss_cls_stage1: 0.02841  loss_box_reg_stage1: 0.488  loss_cls_stage2: 0.03403  loss_box_reg_stage2: 0.6993  loss_mask: 0.1369  loss_rpn_cls: 0.01318  loss_rpn_loc: 0.04852    time: 0.8012  last_time: 0.7982  data_time: 0.0017  last_data_time: 0.0028   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:30:30 d2.utils.events]: \u001b[0m eta: 1:29:42  iter: 5259  total_loss: 1.775  loss_cls_stage0: 0.04307  loss_box_reg_stage0: 0.3475  loss_cls_stage1: 0.03068  loss_box_reg_stage1: 0.493  loss_cls_stage2: 0.0437  loss_box_reg_stage2: 0.664  loss_mask: 0.1336  loss_rpn_cls: 0.00463  loss_rpn_loc: 0.0413    time: 0.8011  last_time: 0.7838  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:30:46 d2.utils.events]: \u001b[0m eta: 1:29:25  iter: 5279  total_loss: 1.541  loss_cls_stage0: 0.0401  loss_box_reg_stage0: 0.3182  loss_cls_stage1: 0.02477  loss_box_reg_stage1: 0.4117  loss_cls_stage2: 0.03387  loss_box_reg_stage2: 0.5917  loss_mask: 0.1081  loss_rpn_cls: 0.00602  loss_rpn_loc: 0.03467    time: 0.8011  last_time: 0.7967  data_time: 0.0019  last_data_time: 0.0036   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:31:02 d2.utils.events]: \u001b[0m eta: 1:29:08  iter: 5299  total_loss: 1.599  loss_cls_stage0: 0.03862  loss_box_reg_stage0: 0.2988  loss_cls_stage1: 0.02967  loss_box_reg_stage1: 0.4155  loss_cls_stage2: 0.04302  loss_box_reg_stage2: 0.5754  loss_mask: 0.1475  loss_rpn_cls: 0.007018  loss_rpn_loc: 0.02909    time: 0.8011  last_time: 0.7805  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:31:18 d2.utils.events]: \u001b[0m eta: 1:28:52  iter: 5319  total_loss: 2.084  loss_cls_stage0: 0.05149  loss_box_reg_stage0: 0.3376  loss_cls_stage1: 0.02541  loss_box_reg_stage1: 0.5547  loss_cls_stage2: 0.05108  loss_box_reg_stage2: 0.6624  loss_mask: 0.1374  loss_rpn_cls: 0.01121  loss_rpn_loc: 0.04318    time: 0.8011  last_time: 0.7960  data_time: 0.0019  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:31:34 d2.utils.events]: \u001b[0m eta: 1:28:35  iter: 5339  total_loss: 1.609  loss_cls_stage0: 0.03546  loss_box_reg_stage0: 0.2591  loss_cls_stage1: 0.02424  loss_box_reg_stage1: 0.3751  loss_cls_stage2: 0.02956  loss_box_reg_stage2: 0.4878  loss_mask: 0.1418  loss_rpn_cls: 0.009639  loss_rpn_loc: 0.04311    time: 0.8011  last_time: 0.7998  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:31:50 d2.utils.events]: \u001b[0m eta: 1:28:20  iter: 5359  total_loss: 1.612  loss_cls_stage0: 0.04127  loss_box_reg_stage0: 0.282  loss_cls_stage1: 0.0413  loss_box_reg_stage1: 0.3881  loss_cls_stage2: 0.05923  loss_box_reg_stage2: 0.5653  loss_mask: 0.1568  loss_rpn_cls: 0.009937  loss_rpn_loc: 0.04408    time: 0.8010  last_time: 0.7957  data_time: 0.0019  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:32:06 d2.utils.events]: \u001b[0m eta: 1:28:04  iter: 5379  total_loss: 1.753  loss_cls_stage0: 0.04836  loss_box_reg_stage0: 0.2978  loss_cls_stage1: 0.04769  loss_box_reg_stage1: 0.4332  loss_cls_stage2: 0.07061  loss_box_reg_stage2: 0.6642  loss_mask: 0.1395  loss_rpn_cls: 0.008111  loss_rpn_loc: 0.05174    time: 0.8010  last_time: 0.8075  data_time: 0.0019  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:32:22 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:32:22 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:32:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:32:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:32:22 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:32:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:32:22 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:32:22 d2.utils.events]: \u001b[0m eta: 1:27:49  iter: 5399  total_loss: 1.717  loss_cls_stage0: 0.04541  loss_box_reg_stage0: 0.2853  loss_cls_stage1: 0.0298  loss_box_reg_stage1: 0.4224  loss_cls_stage2: 0.04964  loss_box_reg_stage2: 0.6132  loss_mask: 0.1336  loss_rpn_cls: 0.00793  loss_rpn_loc: 0.044    time: 0.8010  last_time: 0.8025  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:32:38 d2.utils.events]: \u001b[0m eta: 1:27:32  iter: 5419  total_loss: 1.704  loss_cls_stage0: 0.03973  loss_box_reg_stage0: 0.2901  loss_cls_stage1: 0.03366  loss_box_reg_stage1: 0.4332  loss_cls_stage2: 0.03579  loss_box_reg_stage2: 0.5845  loss_mask: 0.1337  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.06207    time: 0.8010  last_time: 0.7965  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:32:54 d2.utils.events]: \u001b[0m eta: 1:27:16  iter: 5439  total_loss: 1.688  loss_cls_stage0: 0.05143  loss_box_reg_stage0: 0.312  loss_cls_stage1: 0.04022  loss_box_reg_stage1: 0.4281  loss_cls_stage2: 0.0669  loss_box_reg_stage2: 0.5319  loss_mask: 0.1455  loss_rpn_cls: 0.01168  loss_rpn_loc: 0.049    time: 0.8010  last_time: 0.7937  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:33:10 d2.utils.events]: \u001b[0m eta: 1:27:00  iter: 5459  total_loss: 1.759  loss_cls_stage0: 0.05226  loss_box_reg_stage0: 0.3193  loss_cls_stage1: 0.03429  loss_box_reg_stage1: 0.452  loss_cls_stage2: 0.04655  loss_box_reg_stage2: 0.6251  loss_mask: 0.1495  loss_rpn_cls: 0.007008  loss_rpn_loc: 0.05201    time: 0.8010  last_time: 0.8000  data_time: 0.0016  last_data_time: 0.0011   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:33:26 d2.utils.events]: \u001b[0m eta: 1:26:44  iter: 5479  total_loss: 1.549  loss_cls_stage0: 0.03661  loss_box_reg_stage0: 0.2887  loss_cls_stage1: 0.0232  loss_box_reg_stage1: 0.428  loss_cls_stage2: 0.03868  loss_box_reg_stage2: 0.5803  loss_mask: 0.1346  loss_rpn_cls: 0.008528  loss_rpn_loc: 0.0565    time: 0.8010  last_time: 0.7852  data_time: 0.0018  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:33:42 d2.utils.events]: \u001b[0m eta: 1:26:28  iter: 5499  total_loss: 1.693  loss_cls_stage0: 0.04647  loss_box_reg_stage0: 0.3089  loss_cls_stage1: 0.03477  loss_box_reg_stage1: 0.4434  loss_cls_stage2: 0.03713  loss_box_reg_stage2: 0.5661  loss_mask: 0.1598  loss_rpn_cls: 0.007033  loss_rpn_loc: 0.06789    time: 0.8010  last_time: 0.7913  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:33:58 d2.utils.events]: \u001b[0m eta: 1:26:12  iter: 5519  total_loss: 1.571  loss_cls_stage0: 0.03525  loss_box_reg_stage0: 0.2931  loss_cls_stage1: 0.0331  loss_box_reg_stage1: 0.395  loss_cls_stage2: 0.03069  loss_box_reg_stage2: 0.4951  loss_mask: 0.1314  loss_rpn_cls: 0.009484  loss_rpn_loc: 0.04709    time: 0.8010  last_time: 0.8006  data_time: 0.0015  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:34:14 d2.utils.events]: \u001b[0m eta: 1:25:56  iter: 5539  total_loss: 1.605  loss_cls_stage0: 0.03293  loss_box_reg_stage0: 0.2869  loss_cls_stage1: 0.03937  loss_box_reg_stage1: 0.3868  loss_cls_stage2: 0.05111  loss_box_reg_stage2: 0.5248  loss_mask: 0.1411  loss_rpn_cls: 0.01122  loss_rpn_loc: 0.04069    time: 0.8010  last_time: 0.7747  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:34:30 d2.utils.events]: \u001b[0m eta: 1:25:40  iter: 5559  total_loss: 1.786  loss_cls_stage0: 0.04107  loss_box_reg_stage0: 0.32  loss_cls_stage1: 0.0288  loss_box_reg_stage1: 0.5057  loss_cls_stage2: 0.0351  loss_box_reg_stage2: 0.5931  loss_mask: 0.1531  loss_rpn_cls: 0.004988  loss_rpn_loc: 0.05084    time: 0.8010  last_time: 0.8035  data_time: 0.0017  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:34:45 d2.utils.events]: \u001b[0m eta: 1:25:23  iter: 5579  total_loss: 1.729  loss_cls_stage0: 0.02925  loss_box_reg_stage0: 0.2866  loss_cls_stage1: 0.02765  loss_box_reg_stage1: 0.4972  loss_cls_stage2: 0.04337  loss_box_reg_stage2: 0.6157  loss_mask: 0.1319  loss_rpn_cls: 0.005258  loss_rpn_loc: 0.03855    time: 0.8010  last_time: 0.7959  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:35:01 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:35:01 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:35:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:35:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:35:02 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:35:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:35:02 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:35:02 d2.utils.events]: \u001b[0m eta: 1:25:08  iter: 5599  total_loss: 1.822  loss_cls_stage0: 0.04041  loss_box_reg_stage0: 0.3339  loss_cls_stage1: 0.0224  loss_box_reg_stage1: 0.4598  loss_cls_stage2: 0.03833  loss_box_reg_stage2: 0.6245  loss_mask: 0.1516  loss_rpn_cls: 0.006803  loss_rpn_loc: 0.04226    time: 0.8010  last_time: 0.8003  data_time: 0.0019  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:35:17 d2.utils.events]: \u001b[0m eta: 1:24:52  iter: 5619  total_loss: 1.491  loss_cls_stage0: 0.02879  loss_box_reg_stage0: 0.2656  loss_cls_stage1: 0.02299  loss_box_reg_stage1: 0.3871  loss_cls_stage2: 0.02606  loss_box_reg_stage2: 0.542  loss_mask: 0.1539  loss_rpn_cls: 0.008655  loss_rpn_loc: 0.06118    time: 0.8009  last_time: 0.7879  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:35:33 d2.utils.events]: \u001b[0m eta: 1:24:35  iter: 5639  total_loss: 1.579  loss_cls_stage0: 0.02664  loss_box_reg_stage0: 0.2853  loss_cls_stage1: 0.02845  loss_box_reg_stage1: 0.4201  loss_cls_stage2: 0.02187  loss_box_reg_stage2: 0.5728  loss_mask: 0.1356  loss_rpn_cls: 0.005418  loss_rpn_loc: 0.03535    time: 0.8009  last_time: 0.7929  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:35:49 d2.utils.events]: \u001b[0m eta: 1:24:19  iter: 5659  total_loss: 1.465  loss_cls_stage0: 0.04221  loss_box_reg_stage0: 0.2601  loss_cls_stage1: 0.01912  loss_box_reg_stage1: 0.3591  loss_cls_stage2: 0.03963  loss_box_reg_stage2: 0.5072  loss_mask: 0.1196  loss_rpn_cls: 0.006468  loss_rpn_loc: 0.05005    time: 0.8009  last_time: 0.7931  data_time: 0.0019  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:36:05 d2.utils.events]: \u001b[0m eta: 1:24:03  iter: 5679  total_loss: 1.341  loss_cls_stage0: 0.02457  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.02517  loss_box_reg_stage1: 0.3621  loss_cls_stage2: 0.02758  loss_box_reg_stage2: 0.4941  loss_mask: 0.1386  loss_rpn_cls: 0.006175  loss_rpn_loc: 0.04982    time: 0.8009  last_time: 0.7839  data_time: 0.0016  last_data_time: 0.0011   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:36:21 d2.utils.events]: \u001b[0m eta: 1:23:47  iter: 5699  total_loss: 1.67  loss_cls_stage0: 0.03303  loss_box_reg_stage0: 0.3216  loss_cls_stage1: 0.03807  loss_box_reg_stage1: 0.4736  loss_cls_stage2: 0.05104  loss_box_reg_stage2: 0.5452  loss_mask: 0.1376  loss_rpn_cls: 0.01021  loss_rpn_loc: 0.04216    time: 0.8009  last_time: 0.8082  data_time: 0.0019  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:36:37 d2.utils.events]: \u001b[0m eta: 1:23:32  iter: 5719  total_loss: 1.589  loss_cls_stage0: 0.05118  loss_box_reg_stage0: 0.3114  loss_cls_stage1: 0.03786  loss_box_reg_stage1: 0.4093  loss_cls_stage2: 0.04785  loss_box_reg_stage2: 0.5122  loss_mask: 0.1617  loss_rpn_cls: 0.006237  loss_rpn_loc: 0.0488    time: 0.8009  last_time: 0.8103  data_time: 0.0018  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:36:53 d2.utils.events]: \u001b[0m eta: 1:23:16  iter: 5739  total_loss: 1.708  loss_cls_stage0: 0.03846  loss_box_reg_stage0: 0.2983  loss_cls_stage1: 0.02393  loss_box_reg_stage1: 0.4764  loss_cls_stage2: 0.04601  loss_box_reg_stage2: 0.572  loss_mask: 0.1309  loss_rpn_cls: 0.009343  loss_rpn_loc: 0.05047    time: 0.8009  last_time: 0.7847  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:37:09 d2.utils.events]: \u001b[0m eta: 1:22:59  iter: 5759  total_loss: 1.675  loss_cls_stage0: 0.04986  loss_box_reg_stage0: 0.2956  loss_cls_stage1: 0.0298  loss_box_reg_stage1: 0.4595  loss_cls_stage2: 0.03583  loss_box_reg_stage2: 0.6236  loss_mask: 0.1455  loss_rpn_cls: 0.01184  loss_rpn_loc: 0.04475    time: 0.8009  last_time: 0.8036  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:37:25 d2.utils.events]: \u001b[0m eta: 1:22:43  iter: 5779  total_loss: 1.814  loss_cls_stage0: 0.03798  loss_box_reg_stage0: 0.3254  loss_cls_stage1: 0.02513  loss_box_reg_stage1: 0.4794  loss_cls_stage2: 0.04449  loss_box_reg_stage2: 0.6877  loss_mask: 0.1441  loss_rpn_cls: 0.005974  loss_rpn_loc: 0.05456    time: 0.8008  last_time: 0.7930  data_time: 0.0016  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:37:41 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:37:41 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:37:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:37:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:37:41 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:37:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:37:41 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:37:41 d2.utils.events]: \u001b[0m eta: 1:22:27  iter: 5799  total_loss: 1.337  loss_cls_stage0: 0.03353  loss_box_reg_stage0: 0.274  loss_cls_stage1: 0.01826  loss_box_reg_stage1: 0.3155  loss_cls_stage2: 0.02868  loss_box_reg_stage2: 0.5249  loss_mask: 0.1153  loss_rpn_cls: 0.008672  loss_rpn_loc: 0.06385    time: 0.8008  last_time: 0.7873  data_time: 0.0015  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:37:57 d2.utils.events]: \u001b[0m eta: 1:22:10  iter: 5819  total_loss: 1.879  loss_cls_stage0: 0.03961  loss_box_reg_stage0: 0.3339  loss_cls_stage1: 0.0344  loss_box_reg_stage1: 0.4707  loss_cls_stage2: 0.05364  loss_box_reg_stage2: 0.5878  loss_mask: 0.1407  loss_rpn_cls: 0.006267  loss_rpn_loc: 0.04968    time: 0.8008  last_time: 0.7971  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:38:13 d2.utils.events]: \u001b[0m eta: 1:21:53  iter: 5839  total_loss: 1.538  loss_cls_stage0: 0.0493  loss_box_reg_stage0: 0.2931  loss_cls_stage1: 0.03943  loss_box_reg_stage1: 0.3932  loss_cls_stage2: 0.05411  loss_box_reg_stage2: 0.4527  loss_mask: 0.1347  loss_rpn_cls: 0.005622  loss_rpn_loc: 0.03457    time: 0.8008  last_time: 0.8047  data_time: 0.0017  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:38:29 d2.utils.events]: \u001b[0m eta: 1:21:35  iter: 5859  total_loss: 1.293  loss_cls_stage0: 0.04406  loss_box_reg_stage0: 0.2527  loss_cls_stage1: 0.02958  loss_box_reg_stage1: 0.3445  loss_cls_stage2: 0.02648  loss_box_reg_stage2: 0.43  loss_mask: 0.1562  loss_rpn_cls: 0.01104  loss_rpn_loc: 0.06166    time: 0.8007  last_time: 0.7940  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:38:45 d2.utils.events]: \u001b[0m eta: 1:21:19  iter: 5879  total_loss: 1.616  loss_cls_stage0: 0.03321  loss_box_reg_stage0: 0.301  loss_cls_stage1: 0.02618  loss_box_reg_stage1: 0.4525  loss_cls_stage2: 0.03646  loss_box_reg_stage2: 0.563  loss_mask: 0.1469  loss_rpn_cls: 0.006292  loss_rpn_loc: 0.05011    time: 0.8007  last_time: 0.8007  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:39:01 d2.utils.events]: \u001b[0m eta: 1:21:02  iter: 5899  total_loss: 1.62  loss_cls_stage0: 0.03688  loss_box_reg_stage0: 0.2859  loss_cls_stage1: 0.029  loss_box_reg_stage1: 0.4493  loss_cls_stage2: 0.04165  loss_box_reg_stage2: 0.5815  loss_mask: 0.148  loss_rpn_cls: 0.01042  loss_rpn_loc: 0.03995    time: 0.8007  last_time: 0.7860  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:39:17 d2.utils.events]: \u001b[0m eta: 1:20:46  iter: 5919  total_loss: 1.495  loss_cls_stage0: 0.02761  loss_box_reg_stage0: 0.2806  loss_cls_stage1: 0.02797  loss_box_reg_stage1: 0.4404  loss_cls_stage2: 0.0472  loss_box_reg_stage2: 0.5497  loss_mask: 0.1257  loss_rpn_cls: 0.007763  loss_rpn_loc: 0.04205    time: 0.8007  last_time: 0.7940  data_time: 0.0014  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:39:32 d2.utils.events]: \u001b[0m eta: 1:20:29  iter: 5939  total_loss: 1.572  loss_cls_stage0: 0.04798  loss_box_reg_stage0: 0.2647  loss_cls_stage1: 0.02718  loss_box_reg_stage1: 0.4056  loss_cls_stage2: 0.04095  loss_box_reg_stage2: 0.564  loss_mask: 0.1245  loss_rpn_cls: 0.009359  loss_rpn_loc: 0.05069    time: 0.8007  last_time: 0.7819  data_time: 0.0015  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:39:48 d2.utils.events]: \u001b[0m eta: 1:20:13  iter: 5959  total_loss: 1.69  loss_cls_stage0: 0.04185  loss_box_reg_stage0: 0.3081  loss_cls_stage1: 0.02274  loss_box_reg_stage1: 0.4608  loss_cls_stage2: 0.0538  loss_box_reg_stage2: 0.5845  loss_mask: 0.1373  loss_rpn_cls: 0.008471  loss_rpn_loc: 0.03735    time: 0.8007  last_time: 0.7917  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:40:04 d2.utils.events]: \u001b[0m eta: 1:19:56  iter: 5979  total_loss: 1.508  loss_cls_stage0: 0.02819  loss_box_reg_stage0: 0.2851  loss_cls_stage1: 0.0163  loss_box_reg_stage1: 0.3566  loss_cls_stage2: 0.02864  loss_box_reg_stage2: 0.5281  loss_mask: 0.1387  loss_rpn_cls: 0.009833  loss_rpn_loc: 0.04337    time: 0.8006  last_time: 0.7982  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:40:20 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:40:20 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:40:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:40:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:40:20 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:40:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:40:20 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:40:20 d2.utils.events]: \u001b[0m eta: 1:19:39  iter: 5999  total_loss: 1.511  loss_cls_stage0: 0.03235  loss_box_reg_stage0: 0.2866  loss_cls_stage1: 0.03964  loss_box_reg_stage1: 0.3288  loss_cls_stage2: 0.04703  loss_box_reg_stage2: 0.5039  loss_mask: 0.1362  loss_rpn_cls: 0.006287  loss_rpn_loc: 0.03915    time: 0.8006  last_time: 0.7973  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:40:36 d2.utils.events]: \u001b[0m eta: 1:19:24  iter: 6019  total_loss: 1.957  loss_cls_stage0: 0.03666  loss_box_reg_stage0: 0.313  loss_cls_stage1: 0.0426  loss_box_reg_stage1: 0.5127  loss_cls_stage2: 0.04994  loss_box_reg_stage2: 0.6275  loss_mask: 0.1383  loss_rpn_cls: 0.012  loss_rpn_loc: 0.0507    time: 0.8006  last_time: 0.7851  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:40:52 d2.utils.events]: \u001b[0m eta: 1:19:07  iter: 6039  total_loss: 1.335  loss_cls_stage0: 0.04255  loss_box_reg_stage0: 0.247  loss_cls_stage1: 0.01718  loss_box_reg_stage1: 0.3098  loss_cls_stage2: 0.02974  loss_box_reg_stage2: 0.4599  loss_mask: 0.1322  loss_rpn_cls: 0.01343  loss_rpn_loc: 0.05703    time: 0.8006  last_time: 0.7801  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:41:08 d2.utils.events]: \u001b[0m eta: 1:18:49  iter: 6059  total_loss: 1.698  loss_cls_stage0: 0.06761  loss_box_reg_stage0: 0.3165  loss_cls_stage1: 0.02885  loss_box_reg_stage1: 0.4786  loss_cls_stage2: 0.02592  loss_box_reg_stage2: 0.5861  loss_mask: 0.1367  loss_rpn_cls: 0.01213  loss_rpn_loc: 0.0317    time: 0.8005  last_time: 0.7896  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:41:24 d2.utils.events]: \u001b[0m eta: 1:18:32  iter: 6079  total_loss: 1.54  loss_cls_stage0: 0.02719  loss_box_reg_stage0: 0.258  loss_cls_stage1: 0.02115  loss_box_reg_stage1: 0.417  loss_cls_stage2: 0.02281  loss_box_reg_stage2: 0.6021  loss_mask: 0.1273  loss_rpn_cls: 0.00825  loss_rpn_loc: 0.03748    time: 0.8005  last_time: 0.7925  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:41:40 d2.utils.events]: \u001b[0m eta: 1:18:16  iter: 6099  total_loss: 1.603  loss_cls_stage0: 0.0375  loss_box_reg_stage0: 0.3101  loss_cls_stage1: 0.01707  loss_box_reg_stage1: 0.4697  loss_cls_stage2: 0.03988  loss_box_reg_stage2: 0.5859  loss_mask: 0.1432  loss_rpn_cls: 0.007772  loss_rpn_loc: 0.05357    time: 0.8005  last_time: 0.7920  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:41:55 d2.utils.events]: \u001b[0m eta: 1:18:00  iter: 6119  total_loss: 1.51  loss_cls_stage0: 0.04147  loss_box_reg_stage0: 0.295  loss_cls_stage1: 0.03273  loss_box_reg_stage1: 0.3883  loss_cls_stage2: 0.02989  loss_box_reg_stage2: 0.5158  loss_mask: 0.1267  loss_rpn_cls: 0.007929  loss_rpn_loc: 0.04198    time: 0.8005  last_time: 0.8069  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:42:11 d2.utils.events]: \u001b[0m eta: 1:17:43  iter: 6139  total_loss: 1.618  loss_cls_stage0: 0.03347  loss_box_reg_stage0: 0.2978  loss_cls_stage1: 0.02955  loss_box_reg_stage1: 0.4419  loss_cls_stage2: 0.02994  loss_box_reg_stage2: 0.5559  loss_mask: 0.1348  loss_rpn_cls: 0.004716  loss_rpn_loc: 0.0473    time: 0.8004  last_time: 0.7928  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:42:27 d2.utils.events]: \u001b[0m eta: 1:17:27  iter: 6159  total_loss: 1.795  loss_cls_stage0: 0.04499  loss_box_reg_stage0: 0.3301  loss_cls_stage1: 0.02832  loss_box_reg_stage1: 0.4719  loss_cls_stage2: 0.03702  loss_box_reg_stage2: 0.6159  loss_mask: 0.1426  loss_rpn_cls: 0.006195  loss_rpn_loc: 0.05003    time: 0.8004  last_time: 0.7989  data_time: 0.0017  last_data_time: 0.0027   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:42:43 d2.utils.events]: \u001b[0m eta: 1:17:10  iter: 6179  total_loss: 1.827  loss_cls_stage0: 0.0237  loss_box_reg_stage0: 0.3436  loss_cls_stage1: 0.02947  loss_box_reg_stage1: 0.5026  loss_cls_stage2: 0.0315  loss_box_reg_stage2: 0.6175  loss_mask: 0.1582  loss_rpn_cls: 0.01062  loss_rpn_loc: 0.04812    time: 0.8004  last_time: 0.7885  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:42:59 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:42:59 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:42:59 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:42:59 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:42:59 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:42:59 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:42:59 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:42:59 d2.utils.events]: \u001b[0m eta: 1:16:54  iter: 6199  total_loss: 1.875  loss_cls_stage0: 0.04808  loss_box_reg_stage0: 0.2977  loss_cls_stage1: 0.03515  loss_box_reg_stage1: 0.4968  loss_cls_stage2: 0.0326  loss_box_reg_stage2: 0.7389  loss_mask: 0.1494  loss_rpn_cls: 0.007341  loss_rpn_loc: 0.04098    time: 0.8004  last_time: 0.7973  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:43:15 d2.utils.events]: \u001b[0m eta: 1:16:37  iter: 6219  total_loss: 1.497  loss_cls_stage0: 0.02178  loss_box_reg_stage0: 0.2437  loss_cls_stage1: 0.02411  loss_box_reg_stage1: 0.3647  loss_cls_stage2: 0.01882  loss_box_reg_stage2: 0.606  loss_mask: 0.1342  loss_rpn_cls: 0.006393  loss_rpn_loc: 0.03921    time: 0.8004  last_time: 0.7826  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:43:31 d2.utils.events]: \u001b[0m eta: 1:16:21  iter: 6239  total_loss: 1.69  loss_cls_stage0: 0.03075  loss_box_reg_stage0: 0.2733  loss_cls_stage1: 0.0289  loss_box_reg_stage1: 0.4634  loss_cls_stage2: 0.02272  loss_box_reg_stage2: 0.6081  loss_mask: 0.1154  loss_rpn_cls: 0.006944  loss_rpn_loc: 0.03533    time: 0.8003  last_time: 0.7967  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:43:47 d2.utils.events]: \u001b[0m eta: 1:16:03  iter: 6259  total_loss: 1.669  loss_cls_stage0: 0.03732  loss_box_reg_stage0: 0.2826  loss_cls_stage1: 0.02998  loss_box_reg_stage1: 0.4412  loss_cls_stage2: 0.0454  loss_box_reg_stage2: 0.5876  loss_mask: 0.1327  loss_rpn_cls: 0.01009  loss_rpn_loc: 0.0429    time: 0.8003  last_time: 0.7835  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:44:02 d2.utils.events]: \u001b[0m eta: 1:15:46  iter: 6279  total_loss: 1.557  loss_cls_stage0: 0.03434  loss_box_reg_stage0: 0.2844  loss_cls_stage1: 0.01975  loss_box_reg_stage1: 0.4175  loss_cls_stage2: 0.04145  loss_box_reg_stage2: 0.5773  loss_mask: 0.1481  loss_rpn_cls: 0.008185  loss_rpn_loc: 0.05567    time: 0.8003  last_time: 0.7930  data_time: 0.0015  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:44:18 d2.utils.events]: \u001b[0m eta: 1:15:31  iter: 6299  total_loss: 1.671  loss_cls_stage0: 0.03816  loss_box_reg_stage0: 0.291  loss_cls_stage1: 0.02952  loss_box_reg_stage1: 0.4394  loss_cls_stage2: 0.04163  loss_box_reg_stage2: 0.6076  loss_mask: 0.1256  loss_rpn_cls: 0.006234  loss_rpn_loc: 0.03182    time: 0.8003  last_time: 0.8155  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:44:34 d2.utils.events]: \u001b[0m eta: 1:15:14  iter: 6319  total_loss: 1.822  loss_cls_stage0: 0.04256  loss_box_reg_stage0: 0.3256  loss_cls_stage1: 0.03437  loss_box_reg_stage1: 0.4766  loss_cls_stage2: 0.04791  loss_box_reg_stage2: 0.6605  loss_mask: 0.1398  loss_rpn_cls: 0.007826  loss_rpn_loc: 0.04121    time: 0.8002  last_time: 0.7851  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:44:50 d2.utils.events]: \u001b[0m eta: 1:14:59  iter: 6339  total_loss: 1.392  loss_cls_stage0: 0.04514  loss_box_reg_stage0: 0.2726  loss_cls_stage1: 0.01744  loss_box_reg_stage1: 0.3964  loss_cls_stage2: 0.03039  loss_box_reg_stage2: 0.5002  loss_mask: 0.1283  loss_rpn_cls: 0.006108  loss_rpn_loc: 0.04281    time: 0.8002  last_time: 0.7955  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:45:06 d2.utils.events]: \u001b[0m eta: 1:14:42  iter: 6359  total_loss: 1.7  loss_cls_stage0: 0.03548  loss_box_reg_stage0: 0.2782  loss_cls_stage1: 0.02685  loss_box_reg_stage1: 0.4646  loss_cls_stage2: 0.03216  loss_box_reg_stage2: 0.5801  loss_mask: 0.1431  loss_rpn_cls: 0.006628  loss_rpn_loc: 0.05149    time: 0.8002  last_time: 0.7847  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:45:22 d2.utils.events]: \u001b[0m eta: 1:14:25  iter: 6379  total_loss: 1.338  loss_cls_stage0: 0.02277  loss_box_reg_stage0: 0.2599  loss_cls_stage1: 0.02338  loss_box_reg_stage1: 0.3522  loss_cls_stage2: 0.04  loss_box_reg_stage2: 0.4877  loss_mask: 0.1263  loss_rpn_cls: 0.005209  loss_rpn_loc: 0.03984    time: 0.8002  last_time: 0.7957  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:45:38 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:45:38 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:45:38 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:45:38 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:45:38 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:45:38 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:45:38 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:45:38 d2.utils.events]: \u001b[0m eta: 1:14:08  iter: 6399  total_loss: 1.547  loss_cls_stage0: 0.03351  loss_box_reg_stage0: 0.2846  loss_cls_stage1: 0.01512  loss_box_reg_stage1: 0.4252  loss_cls_stage2: 0.02029  loss_box_reg_stage2: 0.5738  loss_mask: 0.1256  loss_rpn_cls: 0.005777  loss_rpn_loc: 0.03077    time: 0.8002  last_time: 0.7939  data_time: 0.0015  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:45:54 d2.utils.events]: \u001b[0m eta: 1:13:51  iter: 6419  total_loss: 1.32  loss_cls_stage0: 0.03283  loss_box_reg_stage0: 0.2634  loss_cls_stage1: 0.01767  loss_box_reg_stage1: 0.369  loss_cls_stage2: 0.02593  loss_box_reg_stage2: 0.479  loss_mask: 0.1309  loss_rpn_cls: 0.007225  loss_rpn_loc: 0.02928    time: 0.8001  last_time: 0.7898  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:46:10 d2.utils.events]: \u001b[0m eta: 1:13:35  iter: 6439  total_loss: 1.487  loss_cls_stage0: 0.02891  loss_box_reg_stage0: 0.2502  loss_cls_stage1: 0.01957  loss_box_reg_stage1: 0.3825  loss_cls_stage2: 0.02928  loss_box_reg_stage2: 0.5188  loss_mask: 0.1352  loss_rpn_cls: 0.007138  loss_rpn_loc: 0.03633    time: 0.8001  last_time: 0.7948  data_time: 0.0015  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:46:26 d2.utils.events]: \u001b[0m eta: 1:13:19  iter: 6459  total_loss: 1.438  loss_cls_stage0: 0.02393  loss_box_reg_stage0: 0.269  loss_cls_stage1: 0.02331  loss_box_reg_stage1: 0.3693  loss_cls_stage2: 0.02729  loss_box_reg_stage2: 0.5366  loss_mask: 0.1263  loss_rpn_cls: 0.006912  loss_rpn_loc: 0.03665    time: 0.8001  last_time: 0.7950  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:46:41 d2.utils.events]: \u001b[0m eta: 1:13:03  iter: 6479  total_loss: 1.879  loss_cls_stage0: 0.04298  loss_box_reg_stage0: 0.3423  loss_cls_stage1: 0.04302  loss_box_reg_stage1: 0.454  loss_cls_stage2: 0.05662  loss_box_reg_stage2: 0.6441  loss_mask: 0.1623  loss_rpn_cls: 0.005686  loss_rpn_loc: 0.04582    time: 0.8001  last_time: 0.7850  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:46:57 d2.utils.events]: \u001b[0m eta: 1:12:47  iter: 6499  total_loss: 1.621  loss_cls_stage0: 0.03083  loss_box_reg_stage0: 0.3227  loss_cls_stage1: 0.02394  loss_box_reg_stage1: 0.4366  loss_cls_stage2: 0.03206  loss_box_reg_stage2: 0.5951  loss_mask: 0.14  loss_rpn_cls: 0.004865  loss_rpn_loc: 0.04658    time: 0.8001  last_time: 0.7945  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:47:13 d2.utils.events]: \u001b[0m eta: 1:12:31  iter: 6519  total_loss: 1.327  loss_cls_stage0: 0.02611  loss_box_reg_stage0: 0.2564  loss_cls_stage1: 0.01837  loss_box_reg_stage1: 0.3469  loss_cls_stage2: 0.02939  loss_box_reg_stage2: 0.4386  loss_mask: 0.1461  loss_rpn_cls: 0.005144  loss_rpn_loc: 0.03622    time: 0.8000  last_time: 0.8271  data_time: 0.0017  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:47:29 d2.utils.events]: \u001b[0m eta: 1:12:15  iter: 6539  total_loss: 1.737  loss_cls_stage0: 0.03954  loss_box_reg_stage0: 0.3191  loss_cls_stage1: 0.03224  loss_box_reg_stage1: 0.4406  loss_cls_stage2: 0.03868  loss_box_reg_stage2: 0.6248  loss_mask: 0.1359  loss_rpn_cls: 0.004602  loss_rpn_loc: 0.06318    time: 0.8000  last_time: 0.7997  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:47:45 d2.utils.events]: \u001b[0m eta: 1:11:59  iter: 6559  total_loss: 1.579  loss_cls_stage0: 0.02597  loss_box_reg_stage0: 0.2829  loss_cls_stage1: 0.02232  loss_box_reg_stage1: 0.3918  loss_cls_stage2: 0.03879  loss_box_reg_stage2: 0.5726  loss_mask: 0.1415  loss_rpn_cls: 0.006123  loss_rpn_loc: 0.04684    time: 0.8000  last_time: 0.7863  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:48:01 d2.utils.events]: \u001b[0m eta: 1:11:43  iter: 6579  total_loss: 1.459  loss_cls_stage0: 0.02923  loss_box_reg_stage0: 0.284  loss_cls_stage1: 0.02142  loss_box_reg_stage1: 0.3774  loss_cls_stage2: 0.03153  loss_box_reg_stage2: 0.5732  loss_mask: 0.1423  loss_rpn_cls: 0.005573  loss_rpn_loc: 0.04065    time: 0.8000  last_time: 0.7878  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:48:17 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:48:17 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:48:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:48:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:48:17 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:48:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:48:17 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:48:17 d2.utils.events]: \u001b[0m eta: 1:11:27  iter: 6599  total_loss: 1.455  loss_cls_stage0: 0.03993  loss_box_reg_stage0: 0.2741  loss_cls_stage1: 0.01282  loss_box_reg_stage1: 0.4264  loss_cls_stage2: 0.02683  loss_box_reg_stage2: 0.5631  loss_mask: 0.1375  loss_rpn_cls: 0.00492  loss_rpn_loc: 0.04154    time: 0.8000  last_time: 0.7975  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:48:33 d2.utils.events]: \u001b[0m eta: 1:11:11  iter: 6619  total_loss: 1.453  loss_cls_stage0: 0.02808  loss_box_reg_stage0: 0.2605  loss_cls_stage1: 0.01194  loss_box_reg_stage1: 0.3863  loss_cls_stage2: 0.02426  loss_box_reg_stage2: 0.5289  loss_mask: 0.1388  loss_rpn_cls: 0.006246  loss_rpn_loc: 0.04028    time: 0.8000  last_time: 0.7889  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:48:49 d2.utils.events]: \u001b[0m eta: 1:10:55  iter: 6639  total_loss: 1.262  loss_cls_stage0: 0.02145  loss_box_reg_stage0: 0.2558  loss_cls_stage1: 0.01244  loss_box_reg_stage1: 0.331  loss_cls_stage2: 0.01775  loss_box_reg_stage2: 0.4804  loss_mask: 0.1177  loss_rpn_cls: 0.002629  loss_rpn_loc: 0.03149    time: 0.8000  last_time: 0.8234  data_time: 0.0018  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:49:05 d2.utils.events]: \u001b[0m eta: 1:10:40  iter: 6659  total_loss: 1.581  loss_cls_stage0: 0.02885  loss_box_reg_stage0: 0.2911  loss_cls_stage1: 0.02012  loss_box_reg_stage1: 0.4076  loss_cls_stage2: 0.0201  loss_box_reg_stage2: 0.5427  loss_mask: 0.124  loss_rpn_cls: 0.002918  loss_rpn_loc: 0.04189    time: 0.8000  last_time: 0.7931  data_time: 0.0019  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:49:21 d2.utils.events]: \u001b[0m eta: 1:10:24  iter: 6679  total_loss: 1.467  loss_cls_stage0: 0.03896  loss_box_reg_stage0: 0.2749  loss_cls_stage1: 0.01113  loss_box_reg_stage1: 0.3907  loss_cls_stage2: 0.01738  loss_box_reg_stage2: 0.5278  loss_mask: 0.1256  loss_rpn_cls: 0.00453  loss_rpn_loc: 0.04193    time: 0.8000  last_time: 0.7864  data_time: 0.0017  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:49:37 d2.utils.events]: \u001b[0m eta: 1:10:08  iter: 6699  total_loss: 1.61  loss_cls_stage0: 0.03341  loss_box_reg_stage0: 0.2907  loss_cls_stage1: 0.02052  loss_box_reg_stage1: 0.4247  loss_cls_stage2: 0.03189  loss_box_reg_stage2: 0.5997  loss_mask: 0.1282  loss_rpn_cls: 0.004813  loss_rpn_loc: 0.03791    time: 0.8000  last_time: 0.7916  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:49:53 d2.utils.events]: \u001b[0m eta: 1:09:51  iter: 6719  total_loss: 1.361  loss_cls_stage0: 0.02977  loss_box_reg_stage0: 0.2454  loss_cls_stage1: 0.03219  loss_box_reg_stage1: 0.3667  loss_cls_stage2: 0.03129  loss_box_reg_stage2: 0.4232  loss_mask: 0.1438  loss_rpn_cls: 0.004926  loss_rpn_loc: 0.03085    time: 0.7999  last_time: 0.7851  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:50:09 d2.utils.events]: \u001b[0m eta: 1:09:36  iter: 6739  total_loss: 1.486  loss_cls_stage0: 0.03745  loss_box_reg_stage0: 0.2668  loss_cls_stage1: 0.01525  loss_box_reg_stage1: 0.4089  loss_cls_stage2: 0.0252  loss_box_reg_stage2: 0.5601  loss_mask: 0.1295  loss_rpn_cls: 0.006515  loss_rpn_loc: 0.04542    time: 0.7999  last_time: 0.7822  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:50:25 d2.utils.events]: \u001b[0m eta: 1:09:20  iter: 6759  total_loss: 1.39  loss_cls_stage0: 0.03143  loss_box_reg_stage0: 0.2465  loss_cls_stage1: 0.01489  loss_box_reg_stage1: 0.3496  loss_cls_stage2: 0.02477  loss_box_reg_stage2: 0.4912  loss_mask: 0.1561  loss_rpn_cls: 0.005595  loss_rpn_loc: 0.04632    time: 0.7999  last_time: 0.7797  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:50:41 d2.utils.events]: \u001b[0m eta: 1:09:04  iter: 6779  total_loss: 1.468  loss_cls_stage0: 0.03346  loss_box_reg_stage0: 0.2864  loss_cls_stage1: 0.0298  loss_box_reg_stage1: 0.379  loss_cls_stage2: 0.02676  loss_box_reg_stage2: 0.5216  loss_mask: 0.1256  loss_rpn_cls: 0.005412  loss_rpn_loc: 0.04065    time: 0.7999  last_time: 0.7928  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:50:56 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:50:56 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:50:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:50:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:50:56 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:50:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:50:56 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:50:56 d2.utils.events]: \u001b[0m eta: 1:08:48  iter: 6799  total_loss: 1.681  loss_cls_stage0: 0.03048  loss_box_reg_stage0: 0.3132  loss_cls_stage1: 0.02516  loss_box_reg_stage1: 0.4351  loss_cls_stage2: 0.03745  loss_box_reg_stage2: 0.5829  loss_mask: 0.1456  loss_rpn_cls: 0.006439  loss_rpn_loc: 0.03288    time: 0.7999  last_time: 0.7890  data_time: 0.0015  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:51:12 d2.utils.events]: \u001b[0m eta: 1:08:32  iter: 6819  total_loss: 1.503  loss_cls_stage0: 0.039  loss_box_reg_stage0: 0.3153  loss_cls_stage1: 0.0283  loss_box_reg_stage1: 0.3752  loss_cls_stage2: 0.0293  loss_box_reg_stage2: 0.5413  loss_mask: 0.1365  loss_rpn_cls: 0.004694  loss_rpn_loc: 0.03354    time: 0.7999  last_time: 0.7897  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:51:28 d2.utils.events]: \u001b[0m eta: 1:08:17  iter: 6839  total_loss: 1.612  loss_cls_stage0: 0.02485  loss_box_reg_stage0: 0.2679  loss_cls_stage1: 0.02272  loss_box_reg_stage1: 0.4296  loss_cls_stage2: 0.03483  loss_box_reg_stage2: 0.5573  loss_mask: 0.1355  loss_rpn_cls: 0.006104  loss_rpn_loc: 0.04742    time: 0.7999  last_time: 0.7900  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:51:44 d2.utils.events]: \u001b[0m eta: 1:08:00  iter: 6859  total_loss: 1.674  loss_cls_stage0: 0.04543  loss_box_reg_stage0: 0.3105  loss_cls_stage1: 0.02473  loss_box_reg_stage1: 0.4247  loss_cls_stage2: 0.0445  loss_box_reg_stage2: 0.5386  loss_mask: 0.1422  loss_rpn_cls: 0.01082  loss_rpn_loc: 0.0509    time: 0.7998  last_time: 0.7938  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:52:00 d2.utils.events]: \u001b[0m eta: 1:07:44  iter: 6879  total_loss: 1.688  loss_cls_stage0: 0.04578  loss_box_reg_stage0: 0.2759  loss_cls_stage1: 0.03114  loss_box_reg_stage1: 0.4491  loss_cls_stage2: 0.03433  loss_box_reg_stage2: 0.5699  loss_mask: 0.1276  loss_rpn_cls: 0.006283  loss_rpn_loc: 0.03992    time: 0.7998  last_time: 0.7891  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:52:16 d2.utils.events]: \u001b[0m eta: 1:07:28  iter: 6899  total_loss: 1.387  loss_cls_stage0: 0.0319  loss_box_reg_stage0: 0.2497  loss_cls_stage1: 0.02836  loss_box_reg_stage1: 0.3481  loss_cls_stage2: 0.02175  loss_box_reg_stage2: 0.4471  loss_mask: 0.135  loss_rpn_cls: 0.005207  loss_rpn_loc: 0.04229    time: 0.7998  last_time: 0.8134  data_time: 0.0015  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:52:32 d2.utils.events]: \u001b[0m eta: 1:07:12  iter: 6919  total_loss: 1.405  loss_cls_stage0: 0.02033  loss_box_reg_stage0: 0.2902  loss_cls_stage1: 0.0139  loss_box_reg_stage1: 0.3416  loss_cls_stage2: 0.02624  loss_box_reg_stage2: 0.5081  loss_mask: 0.1459  loss_rpn_cls: 0.007828  loss_rpn_loc: 0.03338    time: 0.7998  last_time: 0.8142  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:52:48 d2.utils.events]: \u001b[0m eta: 1:06:57  iter: 6939  total_loss: 1.455  loss_cls_stage0: 0.02523  loss_box_reg_stage0: 0.2545  loss_cls_stage1: 0.01854  loss_box_reg_stage1: 0.3994  loss_cls_stage2: 0.03065  loss_box_reg_stage2: 0.5669  loss_mask: 0.1261  loss_rpn_cls: 0.004916  loss_rpn_loc: 0.04013    time: 0.7998  last_time: 0.7882  data_time: 0.0017  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:53:04 d2.utils.events]: \u001b[0m eta: 1:06:41  iter: 6959  total_loss: 1.623  loss_cls_stage0: 0.0318  loss_box_reg_stage0: 0.2996  loss_cls_stage1: 0.0264  loss_box_reg_stage1: 0.4129  loss_cls_stage2: 0.03912  loss_box_reg_stage2: 0.5586  loss_mask: 0.1391  loss_rpn_cls: 0.004675  loss_rpn_loc: 0.04211    time: 0.7998  last_time: 0.7973  data_time: 0.0017  last_data_time: 0.0034   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:53:20 d2.utils.events]: \u001b[0m eta: 1:06:25  iter: 6979  total_loss: 1.399  loss_cls_stage0: 0.02496  loss_box_reg_stage0: 0.2614  loss_cls_stage1: 0.01568  loss_box_reg_stage1: 0.3541  loss_cls_stage2: 0.02481  loss_box_reg_stage2: 0.5269  loss_mask: 0.1114  loss_rpn_cls: 0.005427  loss_rpn_loc: 0.03224    time: 0.7998  last_time: 0.8147  data_time: 0.0017  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:53:36 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:53:36 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:53:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:53:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:53:36 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:53:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:53:36 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:53:36 d2.utils.events]: \u001b[0m eta: 1:06:09  iter: 6999  total_loss: 1.438  loss_cls_stage0: 0.02896  loss_box_reg_stage0: 0.2443  loss_cls_stage1: 0.01387  loss_box_reg_stage1: 0.391  loss_cls_stage2: 0.02511  loss_box_reg_stage2: 0.5527  loss_mask: 0.1259  loss_rpn_cls: 0.004126  loss_rpn_loc: 0.04032    time: 0.7998  last_time: 0.7970  data_time: 0.0014  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:53:52 d2.utils.events]: \u001b[0m eta: 1:05:54  iter: 7019  total_loss: 1.585  loss_cls_stage0: 0.04124  loss_box_reg_stage0: 0.2885  loss_cls_stage1: 0.01753  loss_box_reg_stage1: 0.4003  loss_cls_stage2: 0.03177  loss_box_reg_stage2: 0.5523  loss_mask: 0.1251  loss_rpn_cls: 0.006107  loss_rpn_loc: 0.03161    time: 0.7998  last_time: 0.8047  data_time: 0.0019  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:54:08 d2.utils.events]: \u001b[0m eta: 1:05:38  iter: 7039  total_loss: 1.198  loss_cls_stage0: 0.01982  loss_box_reg_stage0: 0.2413  loss_cls_stage1: 0.01134  loss_box_reg_stage1: 0.3183  loss_cls_stage2: 0.01381  loss_box_reg_stage2: 0.4134  loss_mask: 0.137  loss_rpn_cls: 0.004556  loss_rpn_loc: 0.0334    time: 0.7998  last_time: 0.8153  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:54:24 d2.utils.events]: \u001b[0m eta: 1:05:23  iter: 7059  total_loss: 1.323  loss_cls_stage0: 0.02547  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.01679  loss_box_reg_stage1: 0.3199  loss_cls_stage2: 0.01612  loss_box_reg_stage2: 0.4368  loss_mask: 0.1285  loss_rpn_cls: 0.003556  loss_rpn_loc: 0.04139    time: 0.7998  last_time: 0.7965  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:54:40 d2.utils.events]: \u001b[0m eta: 1:05:07  iter: 7079  total_loss: 1.262  loss_cls_stage0: 0.02677  loss_box_reg_stage0: 0.2308  loss_cls_stage1: 0.01405  loss_box_reg_stage1: 0.3319  loss_cls_stage2: 0.01765  loss_box_reg_stage2: 0.4753  loss_mask: 0.1138  loss_rpn_cls: 0.004259  loss_rpn_loc: 0.03555    time: 0.7997  last_time: 0.7824  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:54:56 d2.utils.events]: \u001b[0m eta: 1:04:52  iter: 7099  total_loss: 1.499  loss_cls_stage0: 0.02635  loss_box_reg_stage0: 0.2576  loss_cls_stage1: 0.01722  loss_box_reg_stage1: 0.404  loss_cls_stage2: 0.02015  loss_box_reg_stage2: 0.5289  loss_mask: 0.1289  loss_rpn_cls: 0.006009  loss_rpn_loc: 0.04537    time: 0.7997  last_time: 0.7985  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:55:12 d2.utils.events]: \u001b[0m eta: 1:04:36  iter: 7119  total_loss: 1.308  loss_cls_stage0: 0.02944  loss_box_reg_stage0: 0.2383  loss_cls_stage1: 0.02112  loss_box_reg_stage1: 0.3459  loss_cls_stage2: 0.0213  loss_box_reg_stage2: 0.4765  loss_mask: 0.115  loss_rpn_cls: 0.006375  loss_rpn_loc: 0.03647    time: 0.7997  last_time: 0.8070  data_time: 0.0018  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:55:27 d2.utils.events]: \u001b[0m eta: 1:04:21  iter: 7139  total_loss: 1.611  loss_cls_stage0: 0.0457  loss_box_reg_stage0: 0.2896  loss_cls_stage1: 0.02785  loss_box_reg_stage1: 0.4001  loss_cls_stage2: 0.02998  loss_box_reg_stage2: 0.5459  loss_mask: 0.127  loss_rpn_cls: 0.006253  loss_rpn_loc: 0.04269    time: 0.7997  last_time: 0.8173  data_time: 0.0018  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:55:43 d2.utils.events]: \u001b[0m eta: 1:04:05  iter: 7159  total_loss: 1.482  loss_cls_stage0: 0.02534  loss_box_reg_stage0: 0.2618  loss_cls_stage1: 0.0219  loss_box_reg_stage1: 0.3878  loss_cls_stage2: 0.03868  loss_box_reg_stage2: 0.5136  loss_mask: 0.1398  loss_rpn_cls: 0.00627  loss_rpn_loc: 0.05709    time: 0.7997  last_time: 0.8066  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:55:59 d2.utils.events]: \u001b[0m eta: 1:03:49  iter: 7179  total_loss: 1.623  loss_cls_stage0: 0.03288  loss_box_reg_stage0: 0.2862  loss_cls_stage1: 0.02347  loss_box_reg_stage1: 0.4164  loss_cls_stage2: 0.02815  loss_box_reg_stage2: 0.5141  loss_mask: 0.142  loss_rpn_cls: 0.006213  loss_rpn_loc: 0.04098    time: 0.7997  last_time: 0.7996  data_time: 0.0016  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:56:15 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:56:15 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:56:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:56:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:56:15 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:56:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:56:16 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:56:16 d2.utils.events]: \u001b[0m eta: 1:03:34  iter: 7199  total_loss: 1.672  loss_cls_stage0: 0.03405  loss_box_reg_stage0: 0.303  loss_cls_stage1: 0.01793  loss_box_reg_stage1: 0.4125  loss_cls_stage2: 0.02627  loss_box_reg_stage2: 0.5949  loss_mask: 0.1405  loss_rpn_cls: 0.005581  loss_rpn_loc: 0.03845    time: 0.7997  last_time: 0.7875  data_time: 0.0018  last_data_time: 0.0027   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:56:32 d2.utils.events]: \u001b[0m eta: 1:03:19  iter: 7219  total_loss: 1.613  loss_cls_stage0: 0.03399  loss_box_reg_stage0: 0.2833  loss_cls_stage1: 0.03169  loss_box_reg_stage1: 0.418  loss_cls_stage2: 0.03854  loss_box_reg_stage2: 0.6137  loss_mask: 0.1399  loss_rpn_cls: 0.004022  loss_rpn_loc: 0.04993    time: 0.7997  last_time: 0.8042  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:56:48 d2.utils.events]: \u001b[0m eta: 1:03:03  iter: 7239  total_loss: 1.625  loss_cls_stage0: 0.03838  loss_box_reg_stage0: 0.2595  loss_cls_stage1: 0.02412  loss_box_reg_stage1: 0.4351  loss_cls_stage2: 0.02265  loss_box_reg_stage2: 0.5565  loss_mask: 0.1459  loss_rpn_cls: 0.008079  loss_rpn_loc: 0.04735    time: 0.7997  last_time: 0.8039  data_time: 0.0020  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:57:03 d2.utils.events]: \u001b[0m eta: 1:02:48  iter: 7259  total_loss: 1.618  loss_cls_stage0: 0.02659  loss_box_reg_stage0: 0.2771  loss_cls_stage1: 0.01821  loss_box_reg_stage1: 0.4175  loss_cls_stage2: 0.04065  loss_box_reg_stage2: 0.6135  loss_mask: 0.1273  loss_rpn_cls: 0.008124  loss_rpn_loc: 0.03544    time: 0.7997  last_time: 0.8039  data_time: 0.0015  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:57:19 d2.utils.events]: \u001b[0m eta: 1:02:32  iter: 7279  total_loss: 1.29  loss_cls_stage0: 0.02946  loss_box_reg_stage0: 0.259  loss_cls_stage1: 0.0209  loss_box_reg_stage1: 0.3519  loss_cls_stage2: 0.02454  loss_box_reg_stage2: 0.4964  loss_mask: 0.1213  loss_rpn_cls: 0.005381  loss_rpn_loc: 0.04282    time: 0.7997  last_time: 0.7877  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:57:35 d2.utils.events]: \u001b[0m eta: 1:02:16  iter: 7299  total_loss: 1.33  loss_cls_stage0: 0.02539  loss_box_reg_stage0: 0.2445  loss_cls_stage1: 0.01646  loss_box_reg_stage1: 0.3521  loss_cls_stage2: 0.02114  loss_box_reg_stage2: 0.4683  loss_mask: 0.1177  loss_rpn_cls: 0.004915  loss_rpn_loc: 0.03605    time: 0.7997  last_time: 0.7949  data_time: 0.0019  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:57:51 d2.utils.events]: \u001b[0m eta: 1:02:00  iter: 7319  total_loss: 1.54  loss_cls_stage0: 0.03425  loss_box_reg_stage0: 0.271  loss_cls_stage1: 0.01966  loss_box_reg_stage1: 0.3993  loss_cls_stage2: 0.03161  loss_box_reg_stage2: 0.5761  loss_mask: 0.1489  loss_rpn_cls: 0.004646  loss_rpn_loc: 0.04457    time: 0.7997  last_time: 0.7897  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:58:07 d2.utils.events]: \u001b[0m eta: 1:01:44  iter: 7339  total_loss: 1.471  loss_cls_stage0: 0.02893  loss_box_reg_stage0: 0.2556  loss_cls_stage1: 0.01953  loss_box_reg_stage1: 0.355  loss_cls_stage2: 0.03918  loss_box_reg_stage2: 0.5667  loss_mask: 0.123  loss_rpn_cls: 0.006684  loss_rpn_loc: 0.04024    time: 0.7997  last_time: 0.8004  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:58:23 d2.utils.events]: \u001b[0m eta: 1:01:28  iter: 7359  total_loss: 1.397  loss_cls_stage0: 0.02511  loss_box_reg_stage0: 0.2513  loss_cls_stage1: 0.0195  loss_box_reg_stage1: 0.367  loss_cls_stage2: 0.02277  loss_box_reg_stage2: 0.4868  loss_mask: 0.1334  loss_rpn_cls: 0.007324  loss_rpn_loc: 0.04368    time: 0.7997  last_time: 0.7967  data_time: 0.0015  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:58:39 d2.utils.events]: \u001b[0m eta: 1:01:13  iter: 7379  total_loss: 1.445  loss_cls_stage0: 0.03006  loss_box_reg_stage0: 0.2593  loss_cls_stage1: 0.01937  loss_box_reg_stage1: 0.3706  loss_cls_stage2: 0.02917  loss_box_reg_stage2: 0.5252  loss_mask: 0.1082  loss_rpn_cls: 0.008447  loss_rpn_loc: 0.03272    time: 0.7996  last_time: 0.7976  data_time: 0.0018  last_data_time: 0.0030   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:58:55 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 12:58:55 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 12:58:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 12:58:55 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 12:58:55 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 12:58:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 12:58:55 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 12:58:55 d2.utils.events]: \u001b[0m eta: 1:00:58  iter: 7399  total_loss: 1.382  loss_cls_stage0: 0.03926  loss_box_reg_stage0: 0.2825  loss_cls_stage1: 0.01655  loss_box_reg_stage1: 0.378  loss_cls_stage2: 0.03007  loss_box_reg_stage2: 0.5087  loss_mask: 0.1131  loss_rpn_cls: 0.003331  loss_rpn_loc: 0.03075    time: 0.7996  last_time: 0.8075  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:59:11 d2.utils.events]: \u001b[0m eta: 1:00:43  iter: 7419  total_loss: 1.553  loss_cls_stage0: 0.03081  loss_box_reg_stage0: 0.2854  loss_cls_stage1: 0.03172  loss_box_reg_stage1: 0.4065  loss_cls_stage2: 0.03433  loss_box_reg_stage2: 0.519  loss_mask: 0.1326  loss_rpn_cls: 0.004098  loss_rpn_loc: 0.0298    time: 0.7996  last_time: 0.7846  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:59:27 d2.utils.events]: \u001b[0m eta: 1:00:27  iter: 7439  total_loss: 1.404  loss_cls_stage0: 0.02103  loss_box_reg_stage0: 0.2438  loss_cls_stage1: 0.01358  loss_box_reg_stage1: 0.3752  loss_cls_stage2: 0.02524  loss_box_reg_stage2: 0.4946  loss_mask: 0.1305  loss_rpn_cls: 0.007329  loss_rpn_loc: 0.04578    time: 0.7996  last_time: 0.7976  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:59:43 d2.utils.events]: \u001b[0m eta: 1:00:12  iter: 7459  total_loss: 1.391  loss_cls_stage0: 0.03313  loss_box_reg_stage0: 0.2492  loss_cls_stage1: 0.01993  loss_box_reg_stage1: 0.3863  loss_cls_stage2: 0.02597  loss_box_reg_stage2: 0.4681  loss_mask: 0.1163  loss_rpn_cls: 0.004026  loss_rpn_loc: 0.04297    time: 0.7996  last_time: 0.8046  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 12:59:59 d2.utils.events]: \u001b[0m eta: 0:59:56  iter: 7479  total_loss: 1.281  loss_cls_stage0: 0.02591  loss_box_reg_stage0: 0.2385  loss_cls_stage1: 0.01206  loss_box_reg_stage1: 0.3436  loss_cls_stage2: 0.01723  loss_box_reg_stage2: 0.4609  loss_mask: 0.1447  loss_rpn_cls: 0.005247  loss_rpn_loc: 0.03361    time: 0.7996  last_time: 0.7919  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:00:15 d2.utils.events]: \u001b[0m eta: 0:59:41  iter: 7499  total_loss: 1.286  loss_cls_stage0: 0.01869  loss_box_reg_stage0: 0.2367  loss_cls_stage1: 0.01612  loss_box_reg_stage1: 0.3214  loss_cls_stage2: 0.02372  loss_box_reg_stage2: 0.4662  loss_mask: 0.1266  loss_rpn_cls: 0.004336  loss_rpn_loc: 0.0429    time: 0.7996  last_time: 0.7989  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:00:31 d2.utils.events]: \u001b[0m eta: 0:59:24  iter: 7519  total_loss: 1.556  loss_cls_stage0: 0.03993  loss_box_reg_stage0: 0.258  loss_cls_stage1: 0.0224  loss_box_reg_stage1: 0.3854  loss_cls_stage2: 0.03596  loss_box_reg_stage2: 0.5545  loss_mask: 0.1382  loss_rpn_cls: 0.004814  loss_rpn_loc: 0.05681    time: 0.7996  last_time: 0.7918  data_time: 0.0016  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:00:47 d2.utils.events]: \u001b[0m eta: 0:59:08  iter: 7539  total_loss: 1.377  loss_cls_stage0: 0.04365  loss_box_reg_stage0: 0.2459  loss_cls_stage1: 0.02693  loss_box_reg_stage1: 0.3371  loss_cls_stage2: 0.03658  loss_box_reg_stage2: 0.51  loss_mask: 0.1283  loss_rpn_cls: 0.007731  loss_rpn_loc: 0.04382    time: 0.7996  last_time: 0.8060  data_time: 0.0015  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:01:03 d2.utils.events]: \u001b[0m eta: 0:58:52  iter: 7559  total_loss: 1.432  loss_cls_stage0: 0.03389  loss_box_reg_stage0: 0.2702  loss_cls_stage1: 0.02163  loss_box_reg_stage1: 0.3573  loss_cls_stage2: 0.02845  loss_box_reg_stage2: 0.5235  loss_mask: 0.1239  loss_rpn_cls: 0.005206  loss_rpn_loc: 0.04091    time: 0.7996  last_time: 0.8003  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:01:18 d2.utils.events]: \u001b[0m eta: 0:58:36  iter: 7579  total_loss: 1.542  loss_cls_stage0: 0.03403  loss_box_reg_stage0: 0.3025  loss_cls_stage1: 0.02249  loss_box_reg_stage1: 0.4039  loss_cls_stage2: 0.03633  loss_box_reg_stage2: 0.5533  loss_mask: 0.1175  loss_rpn_cls: 0.00494  loss_rpn_loc: 0.04125    time: 0.7995  last_time: 0.8167  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:01:34 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:01:34 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:01:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:01:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:01:34 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:01:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:01:34 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:01:34 d2.utils.events]: \u001b[0m eta: 0:58:20  iter: 7599  total_loss: 1.323  loss_cls_stage0: 0.03024  loss_box_reg_stage0: 0.2478  loss_cls_stage1: 0.01364  loss_box_reg_stage1: 0.3319  loss_cls_stage2: 0.022  loss_box_reg_stage2: 0.4885  loss_mask: 0.1283  loss_rpn_cls: 0.003774  loss_rpn_loc: 0.03415    time: 0.7995  last_time: 0.7949  data_time: 0.0017  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:01:50 d2.utils.events]: \u001b[0m eta: 0:58:05  iter: 7619  total_loss: 1.3  loss_cls_stage0: 0.02978  loss_box_reg_stage0: 0.2431  loss_cls_stage1: 0.0134  loss_box_reg_stage1: 0.3677  loss_cls_stage2: 0.03027  loss_box_reg_stage2: 0.4675  loss_mask: 0.1243  loss_rpn_cls: 0.003624  loss_rpn_loc: 0.02595    time: 0.7995  last_time: 0.8032  data_time: 0.0020  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:02:06 d2.utils.events]: \u001b[0m eta: 0:57:49  iter: 7639  total_loss: 1.428  loss_cls_stage0: 0.02116  loss_box_reg_stage0: 0.283  loss_cls_stage1: 0.02164  loss_box_reg_stage1: 0.3605  loss_cls_stage2: 0.02067  loss_box_reg_stage2: 0.5423  loss_mask: 0.1442  loss_rpn_cls: 0.003485  loss_rpn_loc: 0.03559    time: 0.7995  last_time: 0.7957  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:02:22 d2.utils.events]: \u001b[0m eta: 0:57:32  iter: 7659  total_loss: 1.452  loss_cls_stage0: 0.0256  loss_box_reg_stage0: 0.2705  loss_cls_stage1: 0.01746  loss_box_reg_stage1: 0.3821  loss_cls_stage2: 0.02718  loss_box_reg_stage2: 0.5362  loss_mask: 0.1439  loss_rpn_cls: 0.002957  loss_rpn_loc: 0.0384    time: 0.7995  last_time: 0.8085  data_time: 0.0016  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:02:38 d2.utils.events]: \u001b[0m eta: 0:57:16  iter: 7679  total_loss: 1.424  loss_cls_stage0: 0.03511  loss_box_reg_stage0: 0.252  loss_cls_stage1: 0.01881  loss_box_reg_stage1: 0.4099  loss_cls_stage2: 0.02449  loss_box_reg_stage2: 0.5136  loss_mask: 0.1225  loss_rpn_cls: 0.007453  loss_rpn_loc: 0.04084    time: 0.7995  last_time: 0.7955  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:02:54 d2.utils.events]: \u001b[0m eta: 0:57:00  iter: 7699  total_loss: 1.509  loss_cls_stage0: 0.02457  loss_box_reg_stage0: 0.2882  loss_cls_stage1: 0.01361  loss_box_reg_stage1: 0.3746  loss_cls_stage2: 0.03132  loss_box_reg_stage2: 0.5636  loss_mask: 0.1313  loss_rpn_cls: 0.004893  loss_rpn_loc: 0.03333    time: 0.7995  last_time: 0.7953  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:03:10 d2.utils.events]: \u001b[0m eta: 0:56:44  iter: 7719  total_loss: 1.376  loss_cls_stage0: 0.02348  loss_box_reg_stage0: 0.2598  loss_cls_stage1: 0.01618  loss_box_reg_stage1: 0.357  loss_cls_stage2: 0.01713  loss_box_reg_stage2: 0.5134  loss_mask: 0.1184  loss_rpn_cls: 0.004994  loss_rpn_loc: 0.03364    time: 0.7995  last_time: 0.7971  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:03:26 d2.utils.events]: \u001b[0m eta: 0:56:28  iter: 7739  total_loss: 1.249  loss_cls_stage0: 0.02919  loss_box_reg_stage0: 0.238  loss_cls_stage1: 0.01629  loss_box_reg_stage1: 0.3504  loss_cls_stage2: 0.01595  loss_box_reg_stage2: 0.4545  loss_mask: 0.1269  loss_rpn_cls: 0.004738  loss_rpn_loc: 0.0456    time: 0.7995  last_time: 0.7863  data_time: 0.0016  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:03:42 d2.utils.events]: \u001b[0m eta: 0:56:12  iter: 7759  total_loss: 1.55  loss_cls_stage0: 0.03549  loss_box_reg_stage0: 0.2586  loss_cls_stage1: 0.01924  loss_box_reg_stage1: 0.3861  loss_cls_stage2: 0.02009  loss_box_reg_stage2: 0.5809  loss_mask: 0.1324  loss_rpn_cls: 0.003547  loss_rpn_loc: 0.03811    time: 0.7994  last_time: 0.7969  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:03:58 d2.utils.events]: \u001b[0m eta: 0:55:57  iter: 7779  total_loss: 1.39  loss_cls_stage0: 0.03319  loss_box_reg_stage0: 0.227  loss_cls_stage1: 0.01848  loss_box_reg_stage1: 0.327  loss_cls_stage2: 0.01527  loss_box_reg_stage2: 0.5319  loss_mask: 0.1407  loss_rpn_cls: 0.004809  loss_rpn_loc: 0.036    time: 0.7994  last_time: 0.8064  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:04:14 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:04:14 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:04:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:04:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:04:14 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:04:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:04:14 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:04:14 d2.utils.events]: \u001b[0m eta: 0:55:41  iter: 7799  total_loss: 1.308  loss_cls_stage0: 0.03033  loss_box_reg_stage0: 0.2336  loss_cls_stage1: 0.01393  loss_box_reg_stage1: 0.3306  loss_cls_stage2: 0.02085  loss_box_reg_stage2: 0.4928  loss_mask: 0.1253  loss_rpn_cls: 0.006719  loss_rpn_loc: 0.03253    time: 0.7994  last_time: 0.7931  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:04:30 d2.utils.events]: \u001b[0m eta: 0:55:25  iter: 7819  total_loss: 1.041  loss_cls_stage0: 0.0209  loss_box_reg_stage0: 0.2049  loss_cls_stage1: 0.00662  loss_box_reg_stage1: 0.2559  loss_cls_stage2: 0.01411  loss_box_reg_stage2: 0.3702  loss_mask: 0.1188  loss_rpn_cls: 0.003738  loss_rpn_loc: 0.03145    time: 0.7994  last_time: 0.8026  data_time: 0.0017  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:04:46 d2.utils.events]: \u001b[0m eta: 0:55:09  iter: 7839  total_loss: 1.507  loss_cls_stage0: 0.02315  loss_box_reg_stage0: 0.2647  loss_cls_stage1: 0.01248  loss_box_reg_stage1: 0.4109  loss_cls_stage2: 0.02422  loss_box_reg_stage2: 0.4849  loss_mask: 0.1344  loss_rpn_cls: 0.003654  loss_rpn_loc: 0.04557    time: 0.7994  last_time: 0.7929  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:05:02 d2.utils.events]: \u001b[0m eta: 0:54:53  iter: 7859  total_loss: 1.321  loss_cls_stage0: 0.02923  loss_box_reg_stage0: 0.2377  loss_cls_stage1: 0.01445  loss_box_reg_stage1: 0.3378  loss_cls_stage2: 0.03187  loss_box_reg_stage2: 0.464  loss_mask: 0.1231  loss_rpn_cls: 0.005561  loss_rpn_loc: 0.0437    time: 0.7994  last_time: 0.7952  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:05:17 d2.utils.events]: \u001b[0m eta: 0:54:37  iter: 7879  total_loss: 1.134  loss_cls_stage0: 0.03683  loss_box_reg_stage0: 0.2124  loss_cls_stage1: 0.01782  loss_box_reg_stage1: 0.3077  loss_cls_stage2: 0.01891  loss_box_reg_stage2: 0.4131  loss_mask: 0.124  loss_rpn_cls: 0.004207  loss_rpn_loc: 0.02987    time: 0.7994  last_time: 0.8049  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:05:33 d2.utils.events]: \u001b[0m eta: 0:54:22  iter: 7899  total_loss: 1.446  loss_cls_stage0: 0.02609  loss_box_reg_stage0: 0.2557  loss_cls_stage1: 0.0215  loss_box_reg_stage1: 0.3711  loss_cls_stage2: 0.02928  loss_box_reg_stage2: 0.5669  loss_mask: 0.1351  loss_rpn_cls: 0.005072  loss_rpn_loc: 0.03481    time: 0.7994  last_time: 0.7879  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:05:49 d2.utils.events]: \u001b[0m eta: 0:54:06  iter: 7919  total_loss: 1.697  loss_cls_stage0: 0.03601  loss_box_reg_stage0: 0.2842  loss_cls_stage1: 0.02451  loss_box_reg_stage1: 0.4358  loss_cls_stage2: 0.05443  loss_box_reg_stage2: 0.6089  loss_mask: 0.1424  loss_rpn_cls: 0.003607  loss_rpn_loc: 0.03431    time: 0.7994  last_time: 0.7947  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:06:05 d2.utils.events]: \u001b[0m eta: 0:53:50  iter: 7939  total_loss: 1.321  loss_cls_stage0: 0.03001  loss_box_reg_stage0: 0.2631  loss_cls_stage1: 0.01568  loss_box_reg_stage1: 0.3633  loss_cls_stage2: 0.01487  loss_box_reg_stage2: 0.5249  loss_mask: 0.1175  loss_rpn_cls: 0.003893  loss_rpn_loc: 0.03283    time: 0.7994  last_time: 0.7836  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:06:21 d2.utils.events]: \u001b[0m eta: 0:53:34  iter: 7959  total_loss: 1.279  loss_cls_stage0: 0.03076  loss_box_reg_stage0: 0.2114  loss_cls_stage1: 0.009849  loss_box_reg_stage1: 0.3397  loss_cls_stage2: 0.02271  loss_box_reg_stage2: 0.493  loss_mask: 0.1115  loss_rpn_cls: 0.004442  loss_rpn_loc: 0.0392    time: 0.7994  last_time: 0.7965  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:06:37 d2.utils.events]: \u001b[0m eta: 0:53:18  iter: 7979  total_loss: 1.405  loss_cls_stage0: 0.02401  loss_box_reg_stage0: 0.2571  loss_cls_stage1: 0.01437  loss_box_reg_stage1: 0.3888  loss_cls_stage2: 0.03021  loss_box_reg_stage2: 0.5187  loss_mask: 0.1332  loss_rpn_cls: 0.005308  loss_rpn_loc: 0.04001    time: 0.7994  last_time: 0.7918  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:06:53 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:06:53 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:06:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:06:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:06:53 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:06:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:06:53 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:06:53 d2.utils.events]: \u001b[0m eta: 0:53:02  iter: 7999  total_loss: 1.562  loss_cls_stage0: 0.0275  loss_box_reg_stage0: 0.3047  loss_cls_stage1: 0.01915  loss_box_reg_stage1: 0.3938  loss_cls_stage2: 0.02894  loss_box_reg_stage2: 0.5258  loss_mask: 0.1232  loss_rpn_cls: 0.005873  loss_rpn_loc: 0.03849    time: 0.7994  last_time: 0.7992  data_time: 0.0018  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:07:09 d2.utils.events]: \u001b[0m eta: 0:52:46  iter: 8019  total_loss: 1.382  loss_cls_stage0: 0.02374  loss_box_reg_stage0: 0.2281  loss_cls_stage1: 0.01566  loss_box_reg_stage1: 0.3635  loss_cls_stage2: 0.02195  loss_box_reg_stage2: 0.5056  loss_mask: 0.1263  loss_rpn_cls: 0.006104  loss_rpn_loc: 0.03108    time: 0.7993  last_time: 0.7941  data_time: 0.0018  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:07:25 d2.utils.events]: \u001b[0m eta: 0:52:30  iter: 8039  total_loss: 1.41  loss_cls_stage0: 0.02433  loss_box_reg_stage0: 0.2432  loss_cls_stage1: 0.01966  loss_box_reg_stage1: 0.3354  loss_cls_stage2: 0.02059  loss_box_reg_stage2: 0.5164  loss_mask: 0.1261  loss_rpn_cls: 0.005683  loss_rpn_loc: 0.05636    time: 0.7993  last_time: 0.8109  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:07:41 d2.utils.events]: \u001b[0m eta: 0:52:14  iter: 8059  total_loss: 1.514  loss_cls_stage0: 0.03011  loss_box_reg_stage0: 0.2624  loss_cls_stage1: 0.01408  loss_box_reg_stage1: 0.3941  loss_cls_stage2: 0.01656  loss_box_reg_stage2: 0.5525  loss_mask: 0.1423  loss_rpn_cls: 0.004324  loss_rpn_loc: 0.04344    time: 0.7993  last_time: 0.7997  data_time: 0.0018  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:07:57 d2.utils.events]: \u001b[0m eta: 0:51:58  iter: 8079  total_loss: 1.061  loss_cls_stage0: 0.01969  loss_box_reg_stage0: 0.1969  loss_cls_stage1: 0.01164  loss_box_reg_stage1: 0.2701  loss_cls_stage2: 0.01539  loss_box_reg_stage2: 0.3941  loss_mask: 0.1082  loss_rpn_cls: 0.002549  loss_rpn_loc: 0.02345    time: 0.7993  last_time: 0.7969  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:08:13 d2.utils.events]: \u001b[0m eta: 0:51:42  iter: 8099  total_loss: 1.24  loss_cls_stage0: 0.01902  loss_box_reg_stage0: 0.25  loss_cls_stage1: 0.01364  loss_box_reg_stage1: 0.3429  loss_cls_stage2: 0.01973  loss_box_reg_stage2: 0.4333  loss_mask: 0.1171  loss_rpn_cls: 0.007065  loss_rpn_loc: 0.02983    time: 0.7993  last_time: 0.8196  data_time: 0.0016  last_data_time: 0.0032   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:08:29 d2.utils.events]: \u001b[0m eta: 0:51:26  iter: 8119  total_loss: 1.64  loss_cls_stage0: 0.03626  loss_box_reg_stage0: 0.3011  loss_cls_stage1: 0.02151  loss_box_reg_stage1: 0.433  loss_cls_stage2: 0.03017  loss_box_reg_stage2: 0.5806  loss_mask: 0.141  loss_rpn_cls: 0.006207  loss_rpn_loc: 0.04154    time: 0.7993  last_time: 0.7898  data_time: 0.0019  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:08:45 d2.utils.events]: \u001b[0m eta: 0:51:10  iter: 8139  total_loss: 1.191  loss_cls_stage0: 0.03109  loss_box_reg_stage0: 0.2348  loss_cls_stage1: 0.01269  loss_box_reg_stage1: 0.308  loss_cls_stage2: 0.01705  loss_box_reg_stage2: 0.4165  loss_mask: 0.1145  loss_rpn_cls: 0.005862  loss_rpn_loc: 0.0366    time: 0.7993  last_time: 0.8073  data_time: 0.0018  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:09:01 d2.utils.events]: \u001b[0m eta: 0:50:54  iter: 8159  total_loss: 1.333  loss_cls_stage0: 0.02814  loss_box_reg_stage0: 0.2579  loss_cls_stage1: 0.01435  loss_box_reg_stage1: 0.3509  loss_cls_stage2: 0.02876  loss_box_reg_stage2: 0.4887  loss_mask: 0.1176  loss_rpn_cls: 0.003303  loss_rpn_loc: 0.03478    time: 0.7993  last_time: 0.7902  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:09:16 d2.utils.events]: \u001b[0m eta: 0:50:38  iter: 8179  total_loss: 1.393  loss_cls_stage0: 0.02489  loss_box_reg_stage0: 0.256  loss_cls_stage1: 0.01052  loss_box_reg_stage1: 0.3551  loss_cls_stage2: 0.02744  loss_box_reg_stage2: 0.4715  loss_mask: 0.121  loss_rpn_cls: 0.004177  loss_rpn_loc: 0.04684    time: 0.7993  last_time: 0.8172  data_time: 0.0018  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:09:32 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:09:32 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:09:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:09:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:09:32 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:09:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:09:32 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:09:32 d2.utils.events]: \u001b[0m eta: 0:50:21  iter: 8199  total_loss: 1.468  loss_cls_stage0: 0.03022  loss_box_reg_stage0: 0.2721  loss_cls_stage1: 0.01748  loss_box_reg_stage1: 0.3599  loss_cls_stage2: 0.02337  loss_box_reg_stage2: 0.5055  loss_mask: 0.138  loss_rpn_cls: 0.004492  loss_rpn_loc: 0.02895    time: 0.7992  last_time: 0.7925  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:09:48 d2.utils.events]: \u001b[0m eta: 0:50:05  iter: 8219  total_loss: 1.175  loss_cls_stage0: 0.02235  loss_box_reg_stage0: 0.2217  loss_cls_stage1: 0.02059  loss_box_reg_stage1: 0.2523  loss_cls_stage2: 0.01593  loss_box_reg_stage2: 0.4036  loss_mask: 0.1073  loss_rpn_cls: 0.006121  loss_rpn_loc: 0.02947    time: 0.7992  last_time: 0.7839  data_time: 0.0016  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:10:04 d2.utils.events]: \u001b[0m eta: 0:49:49  iter: 8239  total_loss: 1.249  loss_cls_stage0: 0.02111  loss_box_reg_stage0: 0.2282  loss_cls_stage1: 0.01603  loss_box_reg_stage1: 0.3121  loss_cls_stage2: 0.02006  loss_box_reg_stage2: 0.4183  loss_mask: 0.1126  loss_rpn_cls: 0.003339  loss_rpn_loc: 0.0448    time: 0.7992  last_time: 0.8026  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:10:20 d2.utils.events]: \u001b[0m eta: 0:49:34  iter: 8259  total_loss: 1.111  loss_cls_stage0: 0.02146  loss_box_reg_stage0: 0.2346  loss_cls_stage1: 0.01099  loss_box_reg_stage1: 0.3053  loss_cls_stage2: 0.01969  loss_box_reg_stage2: 0.3831  loss_mask: 0.1049  loss_rpn_cls: 0.00552  loss_rpn_loc: 0.04092    time: 0.7992  last_time: 0.7883  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:10:36 d2.utils.events]: \u001b[0m eta: 0:49:18  iter: 8279  total_loss: 1.42  loss_cls_stage0: 0.02488  loss_box_reg_stage0: 0.2643  loss_cls_stage1: 0.01595  loss_box_reg_stage1: 0.3686  loss_cls_stage2: 0.02077  loss_box_reg_stage2: 0.537  loss_mask: 0.1303  loss_rpn_cls: 0.003282  loss_rpn_loc: 0.04188    time: 0.7992  last_time: 0.8089  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:10:52 d2.utils.events]: \u001b[0m eta: 0:49:02  iter: 8299  total_loss: 1.555  loss_cls_stage0: 0.03485  loss_box_reg_stage0: 0.2668  loss_cls_stage1: 0.01454  loss_box_reg_stage1: 0.4376  loss_cls_stage2: 0.02619  loss_box_reg_stage2: 0.5811  loss_mask: 0.1353  loss_rpn_cls: 0.004371  loss_rpn_loc: 0.03672    time: 0.7992  last_time: 0.7939  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:11:08 d2.utils.events]: \u001b[0m eta: 0:48:46  iter: 8319  total_loss: 1.611  loss_cls_stage0: 0.03331  loss_box_reg_stage0: 0.273  loss_cls_stage1: 0.02776  loss_box_reg_stage1: 0.3974  loss_cls_stage2: 0.03454  loss_box_reg_stage2: 0.5791  loss_mask: 0.1416  loss_rpn_cls: 0.006694  loss_rpn_loc: 0.03697    time: 0.7992  last_time: 0.7995  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:11:24 d2.utils.events]: \u001b[0m eta: 0:48:30  iter: 8339  total_loss: 1.281  loss_cls_stage0: 0.02898  loss_box_reg_stage0: 0.2448  loss_cls_stage1: 0.01385  loss_box_reg_stage1: 0.3241  loss_cls_stage2: 0.01461  loss_box_reg_stage2: 0.4543  loss_mask: 0.1199  loss_rpn_cls: 0.004012  loss_rpn_loc: 0.03357    time: 0.7992  last_time: 0.8060  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:11:40 d2.utils.events]: \u001b[0m eta: 0:48:14  iter: 8359  total_loss: 1.216  loss_cls_stage0: 0.0312  loss_box_reg_stage0: 0.2217  loss_cls_stage1: 0.009591  loss_box_reg_stage1: 0.3045  loss_cls_stage2: 0.01243  loss_box_reg_stage2: 0.4377  loss_mask: 0.1254  loss_rpn_cls: 0.003331  loss_rpn_loc: 0.02968    time: 0.7992  last_time: 0.8053  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:11:56 d2.utils.events]: \u001b[0m eta: 0:47:58  iter: 8379  total_loss: 1.382  loss_cls_stage0: 0.02998  loss_box_reg_stage0: 0.2479  loss_cls_stage1: 0.01028  loss_box_reg_stage1: 0.3383  loss_cls_stage2: 0.01974  loss_box_reg_stage2: 0.4786  loss_mask: 0.1398  loss_rpn_cls: 0.005021  loss_rpn_loc: 0.03018    time: 0.7992  last_time: 0.8059  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:12:12 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:12:12 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:12:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:12:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:12:12 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:12:12 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:12:12 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:12:12 d2.utils.events]: \u001b[0m eta: 0:47:42  iter: 8399  total_loss: 1.372  loss_cls_stage0: 0.02692  loss_box_reg_stage0: 0.25  loss_cls_stage1: 0.0249  loss_box_reg_stage1: 0.3786  loss_cls_stage2: 0.02632  loss_box_reg_stage2: 0.4821  loss_mask: 0.1124  loss_rpn_cls: 0.004082  loss_rpn_loc: 0.03757    time: 0.7992  last_time: 0.7890  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:12:28 d2.utils.events]: \u001b[0m eta: 0:47:26  iter: 8419  total_loss: 1.288  loss_cls_stage0: 0.02678  loss_box_reg_stage0: 0.2331  loss_cls_stage1: 0.01934  loss_box_reg_stage1: 0.3444  loss_cls_stage2: 0.01625  loss_box_reg_stage2: 0.4376  loss_mask: 0.1226  loss_rpn_cls: 0.003681  loss_rpn_loc: 0.03084    time: 0.7992  last_time: 0.7916  data_time: 0.0018  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:12:44 d2.utils.events]: \u001b[0m eta: 0:47:10  iter: 8439  total_loss: 1.296  loss_cls_stage0: 0.03288  loss_box_reg_stage0: 0.2459  loss_cls_stage1: 0.01445  loss_box_reg_stage1: 0.3491  loss_cls_stage2: 0.02  loss_box_reg_stage2: 0.4657  loss_mask: 0.1223  loss_rpn_cls: 0.00386  loss_rpn_loc: 0.03417    time: 0.7992  last_time: 0.7882  data_time: 0.0022  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:13:00 d2.utils.events]: \u001b[0m eta: 0:46:54  iter: 8459  total_loss: 1.092  loss_cls_stage0: 0.0291  loss_box_reg_stage0: 0.2209  loss_cls_stage1: 0.009737  loss_box_reg_stage1: 0.287  loss_cls_stage2: 0.01313  loss_box_reg_stage2: 0.404  loss_mask: 0.1108  loss_rpn_cls: 0.00483  loss_rpn_loc: 0.01922    time: 0.7992  last_time: 0.7851  data_time: 0.0015  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:13:16 d2.utils.events]: \u001b[0m eta: 0:46:38  iter: 8479  total_loss: 1.161  loss_cls_stage0: 0.0135  loss_box_reg_stage0: 0.2114  loss_cls_stage1: 0.01284  loss_box_reg_stage1: 0.3074  loss_cls_stage2: 0.02075  loss_box_reg_stage2: 0.4599  loss_mask: 0.1069  loss_rpn_cls: 0.003551  loss_rpn_loc: 0.03374    time: 0.7991  last_time: 0.7935  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:13:31 d2.utils.events]: \u001b[0m eta: 0:46:22  iter: 8499  total_loss: 1.239  loss_cls_stage0: 0.01648  loss_box_reg_stage0: 0.2065  loss_cls_stage1: 0.01262  loss_box_reg_stage1: 0.3062  loss_cls_stage2: 0.01017  loss_box_reg_stage2: 0.471  loss_mask: 0.1219  loss_rpn_cls: 0.004743  loss_rpn_loc: 0.0408    time: 0.7991  last_time: 0.7912  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:13:47 d2.utils.events]: \u001b[0m eta: 0:46:06  iter: 8519  total_loss: 1.51  loss_cls_stage0: 0.03416  loss_box_reg_stage0: 0.2395  loss_cls_stage1: 0.01257  loss_box_reg_stage1: 0.3821  loss_cls_stage2: 0.03518  loss_box_reg_stage2: 0.5181  loss_mask: 0.1234  loss_rpn_cls: 0.004782  loss_rpn_loc: 0.04237    time: 0.7991  last_time: 0.7830  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:14:03 d2.utils.events]: \u001b[0m eta: 0:45:51  iter: 8539  total_loss: 1.444  loss_cls_stage0: 0.02781  loss_box_reg_stage0: 0.248  loss_cls_stage1: 0.007202  loss_box_reg_stage1: 0.3778  loss_cls_stage2: 0.02392  loss_box_reg_stage2: 0.5622  loss_mask: 0.136  loss_rpn_cls: 0.004887  loss_rpn_loc: 0.04416    time: 0.7991  last_time: 0.8081  data_time: 0.0018  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:14:19 d2.utils.events]: \u001b[0m eta: 0:45:35  iter: 8559  total_loss: 1.311  loss_cls_stage0: 0.03038  loss_box_reg_stage0: 0.225  loss_cls_stage1: 0.0141  loss_box_reg_stage1: 0.3268  loss_cls_stage2: 0.01075  loss_box_reg_stage2: 0.5082  loss_mask: 0.1343  loss_rpn_cls: 0.004052  loss_rpn_loc: 0.04327    time: 0.7991  last_time: 0.7859  data_time: 0.0017  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:14:35 d2.utils.events]: \u001b[0m eta: 0:45:19  iter: 8579  total_loss: 1.389  loss_cls_stage0: 0.01658  loss_box_reg_stage0: 0.2457  loss_cls_stage1: 0.007537  loss_box_reg_stage1: 0.3639  loss_cls_stage2: 0.01502  loss_box_reg_stage2: 0.4879  loss_mask: 0.1191  loss_rpn_cls: 0.002757  loss_rpn_loc: 0.03963    time: 0.7991  last_time: 0.7944  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:14:51 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:14:51 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:14:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:14:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:14:51 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:14:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:14:51 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:14:51 d2.utils.events]: \u001b[0m eta: 0:45:03  iter: 8599  total_loss: 1.399  loss_cls_stage0: 0.02402  loss_box_reg_stage0: 0.2679  loss_cls_stage1: 0.01454  loss_box_reg_stage1: 0.3604  loss_cls_stage2: 0.03105  loss_box_reg_stage2: 0.5031  loss_mask: 0.1352  loss_rpn_cls: 0.004653  loss_rpn_loc: 0.04348    time: 0.7991  last_time: 0.7878  data_time: 0.0019  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:15:07 d2.utils.events]: \u001b[0m eta: 0:44:47  iter: 8619  total_loss: 1.364  loss_cls_stage0: 0.02424  loss_box_reg_stage0: 0.2225  loss_cls_stage1: 0.0157  loss_box_reg_stage1: 0.3423  loss_cls_stage2: 0.01373  loss_box_reg_stage2: 0.4926  loss_mask: 0.1226  loss_rpn_cls: 0.005628  loss_rpn_loc: 0.03861    time: 0.7991  last_time: 0.7845  data_time: 0.0018  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:15:23 d2.utils.events]: \u001b[0m eta: 0:44:31  iter: 8639  total_loss: 1.296  loss_cls_stage0: 0.02089  loss_box_reg_stage0: 0.2347  loss_cls_stage1: 0.01928  loss_box_reg_stage1: 0.3423  loss_cls_stage2: 0.0272  loss_box_reg_stage2: 0.4363  loss_mask: 0.1233  loss_rpn_cls: 0.004744  loss_rpn_loc: 0.03403    time: 0.7991  last_time: 0.8011  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:15:39 d2.utils.events]: \u001b[0m eta: 0:44:15  iter: 8659  total_loss: 1.311  loss_cls_stage0: 0.01493  loss_box_reg_stage0: 0.2401  loss_cls_stage1: 0.0119  loss_box_reg_stage1: 0.3347  loss_cls_stage2: 0.02935  loss_box_reg_stage2: 0.4741  loss_mask: 0.1128  loss_rpn_cls: 0.004357  loss_rpn_loc: 0.03596    time: 0.7991  last_time: 0.8120  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:15:55 d2.utils.events]: \u001b[0m eta: 0:44:00  iter: 8679  total_loss: 1.401  loss_cls_stage0: 0.03433  loss_box_reg_stage0: 0.2667  loss_cls_stage1: 0.01634  loss_box_reg_stage1: 0.3885  loss_cls_stage2: 0.02266  loss_box_reg_stage2: 0.5196  loss_mask: 0.1253  loss_rpn_cls: 0.004287  loss_rpn_loc: 0.03719    time: 0.7991  last_time: 0.8033  data_time: 0.0021  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:16:11 d2.utils.events]: \u001b[0m eta: 0:43:44  iter: 8699  total_loss: 1.353  loss_cls_stage0: 0.01683  loss_box_reg_stage0: 0.2148  loss_cls_stage1: 0.01205  loss_box_reg_stage1: 0.3546  loss_cls_stage2: 0.02395  loss_box_reg_stage2: 0.5114  loss_mask: 0.1231  loss_rpn_cls: 0.004003  loss_rpn_loc: 0.03882    time: 0.7991  last_time: 0.7878  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:16:27 d2.utils.events]: \u001b[0m eta: 0:43:28  iter: 8719  total_loss: 1.526  loss_cls_stage0: 0.02132  loss_box_reg_stage0: 0.2591  loss_cls_stage1: 0.02995  loss_box_reg_stage1: 0.3654  loss_cls_stage2: 0.02949  loss_box_reg_stage2: 0.5313  loss_mask: 0.1261  loss_rpn_cls: 0.002736  loss_rpn_loc: 0.04186    time: 0.7991  last_time: 0.8047  data_time: 0.0017  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:16:43 d2.utils.events]: \u001b[0m eta: 0:43:13  iter: 8739  total_loss: 1.446  loss_cls_stage0: 0.02635  loss_box_reg_stage0: 0.2557  loss_cls_stage1: 0.02089  loss_box_reg_stage1: 0.4041  loss_cls_stage2: 0.01419  loss_box_reg_stage2: 0.5542  loss_mask: 0.1093  loss_rpn_cls: 0.003365  loss_rpn_loc: 0.0337    time: 0.7991  last_time: 0.8078  data_time: 0.0017  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:16:59 d2.utils.events]: \u001b[0m eta: 0:42:57  iter: 8759  total_loss: 1.356  loss_cls_stage0: 0.02776  loss_box_reg_stage0: 0.253  loss_cls_stage1: 0.01069  loss_box_reg_stage1: 0.3161  loss_cls_stage2: 0.02254  loss_box_reg_stage2: 0.4669  loss_mask: 0.1308  loss_rpn_cls: 0.003767  loss_rpn_loc: 0.04278    time: 0.7991  last_time: 0.8067  data_time: 0.0017  last_data_time: 0.0031   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:17:15 d2.utils.events]: \u001b[0m eta: 0:42:41  iter: 8779  total_loss: 1.427  loss_cls_stage0: 0.0244  loss_box_reg_stage0: 0.273  loss_cls_stage1: 0.01264  loss_box_reg_stage1: 0.378  loss_cls_stage2: 0.02397  loss_box_reg_stage2: 0.4851  loss_mask: 0.1232  loss_rpn_cls: 0.002343  loss_rpn_loc: 0.03639    time: 0.7991  last_time: 0.7945  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:17:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:17:31 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:17:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:17:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:17:31 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:17:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:17:31 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:17:31 d2.utils.events]: \u001b[0m eta: 0:42:25  iter: 8799  total_loss: 1.238  loss_cls_stage0: 0.02064  loss_box_reg_stage0: 0.2468  loss_cls_stage1: 0.01584  loss_box_reg_stage1: 0.3534  loss_cls_stage2: 0.02314  loss_box_reg_stage2: 0.4617  loss_mask: 0.1169  loss_rpn_cls: 0.002376  loss_rpn_loc: 0.03766    time: 0.7991  last_time: 0.7940  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:17:47 d2.utils.events]: \u001b[0m eta: 0:42:09  iter: 8819  total_loss: 1.448  loss_cls_stage0: 0.01548  loss_box_reg_stage0: 0.2445  loss_cls_stage1: 0.01944  loss_box_reg_stage1: 0.331  loss_cls_stage2: 0.01744  loss_box_reg_stage2: 0.5314  loss_mask: 0.1443  loss_rpn_cls: 0.00471  loss_rpn_loc: 0.034    time: 0.7991  last_time: 0.7999  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:18:03 d2.utils.events]: \u001b[0m eta: 0:41:53  iter: 8839  total_loss: 1.337  loss_cls_stage0: 0.02737  loss_box_reg_stage0: 0.246  loss_cls_stage1: 0.01217  loss_box_reg_stage1: 0.3672  loss_cls_stage2: 0.02914  loss_box_reg_stage2: 0.5042  loss_mask: 0.1245  loss_rpn_cls: 0.006946  loss_rpn_loc: 0.03215    time: 0.7991  last_time: 0.7873  data_time: 0.0017  last_data_time: 0.0029   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:18:19 d2.utils.events]: \u001b[0m eta: 0:41:37  iter: 8859  total_loss: 1.188  loss_cls_stage0: 0.02322  loss_box_reg_stage0: 0.2223  loss_cls_stage1: 0.01312  loss_box_reg_stage1: 0.2877  loss_cls_stage2: 0.01724  loss_box_reg_stage2: 0.411  loss_mask: 0.1042  loss_rpn_cls: 0.005259  loss_rpn_loc: 0.0405    time: 0.7991  last_time: 0.7879  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:18:35 d2.utils.events]: \u001b[0m eta: 0:41:22  iter: 8879  total_loss: 1.338  loss_cls_stage0: 0.02012  loss_box_reg_stage0: 0.2426  loss_cls_stage1: 0.01324  loss_box_reg_stage1: 0.3124  loss_cls_stage2: 0.03209  loss_box_reg_stage2: 0.508  loss_mask: 0.1165  loss_rpn_cls: 0.003919  loss_rpn_loc: 0.03714    time: 0.7991  last_time: 0.7994  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:18:51 d2.utils.events]: \u001b[0m eta: 0:41:06  iter: 8899  total_loss: 1.103  loss_cls_stage0: 0.02215  loss_box_reg_stage0: 0.241  loss_cls_stage1: 0.01438  loss_box_reg_stage1: 0.2731  loss_cls_stage2: 0.02364  loss_box_reg_stage2: 0.3372  loss_mask: 0.1225  loss_rpn_cls: 0.002555  loss_rpn_loc: 0.036    time: 0.7991  last_time: 0.8110  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:19:07 d2.utils.events]: \u001b[0m eta: 0:40:50  iter: 8919  total_loss: 1.356  loss_cls_stage0: 0.01788  loss_box_reg_stage0: 0.248  loss_cls_stage1: 0.02161  loss_box_reg_stage1: 0.3571  loss_cls_stage2: 0.01983  loss_box_reg_stage2: 0.39  loss_mask: 0.1226  loss_rpn_cls: 0.003145  loss_rpn_loc: 0.0386    time: 0.7991  last_time: 0.8024  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:19:23 d2.utils.events]: \u001b[0m eta: 0:40:34  iter: 8939  total_loss: 1.112  loss_cls_stage0: 0.02014  loss_box_reg_stage0: 0.2216  loss_cls_stage1: 0.01091  loss_box_reg_stage1: 0.2771  loss_cls_stage2: 0.0148  loss_box_reg_stage2: 0.4085  loss_mask: 0.1125  loss_rpn_cls: 0.002314  loss_rpn_loc: 0.02755    time: 0.7991  last_time: 0.7876  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:19:39 d2.utils.events]: \u001b[0m eta: 0:40:18  iter: 8959  total_loss: 1.12  loss_cls_stage0: 0.01633  loss_box_reg_stage0: 0.1992  loss_cls_stage1: 0.005017  loss_box_reg_stage1: 0.3007  loss_cls_stage2: 0.01029  loss_box_reg_stage2: 0.4134  loss_mask: 0.1147  loss_rpn_cls: 0.004659  loss_rpn_loc: 0.03645    time: 0.7991  last_time: 0.8043  data_time: 0.0016  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:19:55 d2.utils.events]: \u001b[0m eta: 0:40:02  iter: 8979  total_loss: 1.068  loss_cls_stage0: 0.01425  loss_box_reg_stage0: 0.224  loss_cls_stage1: 0.01399  loss_box_reg_stage1: 0.2721  loss_cls_stage2: 0.01081  loss_box_reg_stage2: 0.3923  loss_mask: 0.1069  loss_rpn_cls: 0.004499  loss_rpn_loc: 0.04528    time: 0.7990  last_time: 0.8146  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:20:11 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:20:11 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:20:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:20:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:20:11 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:20:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:20:11 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:20:11 d2.utils.events]: \u001b[0m eta: 0:39:46  iter: 8999  total_loss: 1.37  loss_cls_stage0: 0.02798  loss_box_reg_stage0: 0.2383  loss_cls_stage1: 0.01805  loss_box_reg_stage1: 0.3567  loss_cls_stage2: 0.01959  loss_box_reg_stage2: 0.5369  loss_mask: 0.1264  loss_rpn_cls: 0.004746  loss_rpn_loc: 0.05012    time: 0.7990  last_time: 0.7982  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:20:27 d2.utils.events]: \u001b[0m eta: 0:39:30  iter: 9019  total_loss: 1.448  loss_cls_stage0: 0.0238  loss_box_reg_stage0: 0.2585  loss_cls_stage1: 0.009936  loss_box_reg_stage1: 0.3585  loss_cls_stage2: 0.03518  loss_box_reg_stage2: 0.4972  loss_mask: 0.1316  loss_rpn_cls: 0.002633  loss_rpn_loc: 0.04345    time: 0.7990  last_time: 0.8092  data_time: 0.0018  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:20:43 d2.utils.events]: \u001b[0m eta: 0:39:14  iter: 9039  total_loss: 1.58  loss_cls_stage0: 0.03603  loss_box_reg_stage0: 0.2498  loss_cls_stage1: 0.03008  loss_box_reg_stage1: 0.3807  loss_cls_stage2: 0.04386  loss_box_reg_stage2: 0.5545  loss_mask: 0.1291  loss_rpn_cls: 0.0151  loss_rpn_loc: 0.04171    time: 0.7990  last_time: 0.8065  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:20:59 d2.utils.events]: \u001b[0m eta: 0:38:58  iter: 9059  total_loss: 1.695  loss_cls_stage0: 0.04018  loss_box_reg_stage0: 0.3042  loss_cls_stage1: 0.02615  loss_box_reg_stage1: 0.4287  loss_cls_stage2: 0.03611  loss_box_reg_stage2: 0.5892  loss_mask: 0.1368  loss_rpn_cls: 0.00718  loss_rpn_loc: 0.03943    time: 0.7990  last_time: 0.7876  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:21:15 d2.utils.events]: \u001b[0m eta: 0:38:42  iter: 9079  total_loss: 1.416  loss_cls_stage0: 0.02935  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.016  loss_box_reg_stage1: 0.357  loss_cls_stage2: 0.016  loss_box_reg_stage2: 0.4775  loss_mask: 0.1283  loss_rpn_cls: 0.008503  loss_rpn_loc: 0.03474    time: 0.7990  last_time: 0.7929  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:21:30 d2.utils.events]: \u001b[0m eta: 0:38:27  iter: 9099  total_loss: 1.51  loss_cls_stage0: 0.02397  loss_box_reg_stage0: 0.2522  loss_cls_stage1: 0.01788  loss_box_reg_stage1: 0.3834  loss_cls_stage2: 0.03555  loss_box_reg_stage2: 0.4281  loss_mask: 0.13  loss_rpn_cls: 0.00831  loss_rpn_loc: 0.0375    time: 0.7990  last_time: 0.8008  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:21:46 d2.utils.events]: \u001b[0m eta: 0:38:11  iter: 9119  total_loss: 1.288  loss_cls_stage0: 0.02971  loss_box_reg_stage0: 0.2443  loss_cls_stage1: 0.01328  loss_box_reg_stage1: 0.3251  loss_cls_stage2: 0.02363  loss_box_reg_stage2: 0.4647  loss_mask: 0.1104  loss_rpn_cls: 0.005161  loss_rpn_loc: 0.03607    time: 0.7990  last_time: 0.7939  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:22:02 d2.utils.events]: \u001b[0m eta: 0:37:55  iter: 9139  total_loss: 1.431  loss_cls_stage0: 0.02999  loss_box_reg_stage0: 0.2697  loss_cls_stage1: 0.01535  loss_box_reg_stage1: 0.3788  loss_cls_stage2: 0.02192  loss_box_reg_stage2: 0.4824  loss_mask: 0.1354  loss_rpn_cls: 0.006069  loss_rpn_loc: 0.03115    time: 0.7990  last_time: 0.7949  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:22:18 d2.utils.events]: \u001b[0m eta: 0:37:39  iter: 9159  total_loss: 1.245  loss_cls_stage0: 0.02327  loss_box_reg_stage0: 0.2437  loss_cls_stage1: 0.01359  loss_box_reg_stage1: 0.3189  loss_cls_stage2: 0.01742  loss_box_reg_stage2: 0.4424  loss_mask: 0.1101  loss_rpn_cls: 0.003859  loss_rpn_loc: 0.03248    time: 0.7990  last_time: 0.7894  data_time: 0.0017  last_data_time: 0.0030   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:22:34 d2.utils.events]: \u001b[0m eta: 0:37:23  iter: 9179  total_loss: 1.185  loss_cls_stage0: 0.02764  loss_box_reg_stage0: 0.2461  loss_cls_stage1: 0.01081  loss_box_reg_stage1: 0.3074  loss_cls_stage2: 0.01623  loss_box_reg_stage2: 0.4321  loss_mask: 0.1191  loss_rpn_cls: 0.003127  loss_rpn_loc: 0.02972    time: 0.7990  last_time: 0.8180  data_time: 0.0017  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:22:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:22:50 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:22:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:22:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:22:50 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:22:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:22:50 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:22:50 d2.utils.events]: \u001b[0m eta: 0:37:08  iter: 9199  total_loss: 1.391  loss_cls_stage0: 0.02754  loss_box_reg_stage0: 0.2434  loss_cls_stage1: 0.01267  loss_box_reg_stage1: 0.3599  loss_cls_stage2: 0.01891  loss_box_reg_stage2: 0.4842  loss_mask: 0.123  loss_rpn_cls: 0.004298  loss_rpn_loc: 0.03741    time: 0.7990  last_time: 0.7835  data_time: 0.0018  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:23:06 d2.utils.events]: \u001b[0m eta: 0:36:52  iter: 9219  total_loss: 1.228  loss_cls_stage0: 0.0166  loss_box_reg_stage0: 0.2204  loss_cls_stage1: 0.01695  loss_box_reg_stage1: 0.3025  loss_cls_stage2: 0.01668  loss_box_reg_stage2: 0.4616  loss_mask: 0.1263  loss_rpn_cls: 0.003123  loss_rpn_loc: 0.02831    time: 0.7990  last_time: 0.8090  data_time: 0.0023  last_data_time: 0.0032   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:23:23 d2.utils.events]: \u001b[0m eta: 0:36:37  iter: 9239  total_loss: 1.512  loss_cls_stage0: 0.01984  loss_box_reg_stage0: 0.2629  loss_cls_stage1: 0.01841  loss_box_reg_stage1: 0.3857  loss_cls_stage2: 0.03299  loss_box_reg_stage2: 0.5151  loss_mask: 0.1321  loss_rpn_cls: 0.004537  loss_rpn_loc: 0.03442    time: 0.7990  last_time: 0.8280  data_time: 0.0021  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:23:39 d2.utils.events]: \u001b[0m eta: 0:36:22  iter: 9259  total_loss: 1.26  loss_cls_stage0: 0.02402  loss_box_reg_stage0: 0.2118  loss_cls_stage1: 0.008871  loss_box_reg_stage1: 0.3116  loss_cls_stage2: 0.01813  loss_box_reg_stage2: 0.4932  loss_mask: 0.1283  loss_rpn_cls: 0.007614  loss_rpn_loc: 0.03527    time: 0.7991  last_time: 0.7915  data_time: 0.0022  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:23:55 d2.utils.events]: \u001b[0m eta: 0:36:06  iter: 9279  total_loss: 1.122  loss_cls_stage0: 0.02712  loss_box_reg_stage0: 0.2046  loss_cls_stage1: 0.01319  loss_box_reg_stage1: 0.2875  loss_cls_stage2: 0.01667  loss_box_reg_stage2: 0.4129  loss_mask: 0.1171  loss_rpn_cls: 0.007326  loss_rpn_loc: 0.02696    time: 0.7991  last_time: 0.8062  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:24:11 d2.utils.events]: \u001b[0m eta: 0:35:50  iter: 9299  total_loss: 1.087  loss_cls_stage0: 0.02237  loss_box_reg_stage0: 0.2143  loss_cls_stage1: 0.01536  loss_box_reg_stage1: 0.2907  loss_cls_stage2: 0.02229  loss_box_reg_stage2: 0.4101  loss_mask: 0.1146  loss_rpn_cls: 0.006322  loss_rpn_loc: 0.03164    time: 0.7991  last_time: 0.8068  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:24:27 d2.utils.events]: \u001b[0m eta: 0:35:34  iter: 9319  total_loss: 1.19  loss_cls_stage0: 0.02629  loss_box_reg_stage0: 0.2325  loss_cls_stage1: 0.01069  loss_box_reg_stage1: 0.2823  loss_cls_stage2: 0.01748  loss_box_reg_stage2: 0.4754  loss_mask: 0.1327  loss_rpn_cls: 0.004337  loss_rpn_loc: 0.03439    time: 0.7991  last_time: 0.7955  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:24:43 d2.utils.events]: \u001b[0m eta: 0:35:18  iter: 9339  total_loss: 1.225  loss_cls_stage0: 0.01637  loss_box_reg_stage0: 0.2405  loss_cls_stage1: 0.02002  loss_box_reg_stage1: 0.3246  loss_cls_stage2: 0.01006  loss_box_reg_stage2: 0.4173  loss_mask: 0.1296  loss_rpn_cls: 0.002927  loss_rpn_loc: 0.03269    time: 0.7991  last_time: 0.7969  data_time: 0.0017  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:24:59 d2.utils.events]: \u001b[0m eta: 0:35:02  iter: 9359  total_loss: 1.328  loss_cls_stage0: 0.03397  loss_box_reg_stage0: 0.2574  loss_cls_stage1: 0.008176  loss_box_reg_stage1: 0.3592  loss_cls_stage2: 0.02039  loss_box_reg_stage2: 0.5072  loss_mask: 0.1236  loss_rpn_cls: 0.004418  loss_rpn_loc: 0.02806    time: 0.7991  last_time: 0.8138  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:25:15 d2.utils.events]: \u001b[0m eta: 0:34:46  iter: 9379  total_loss: 1.196  loss_cls_stage0: 0.0285  loss_box_reg_stage0: 0.22  loss_cls_stage1: 0.005547  loss_box_reg_stage1: 0.3029  loss_cls_stage2: 0.01182  loss_box_reg_stage2: 0.4466  loss_mask: 0.1255  loss_rpn_cls: 0.004524  loss_rpn_loc: 0.04282    time: 0.7991  last_time: 0.8017  data_time: 0.0016  last_data_time: 0.0011   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:25:31 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:25:31 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:25:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:25:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:25:31 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:25:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:25:31 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:25:31 d2.utils.events]: \u001b[0m eta: 0:34:30  iter: 9399  total_loss: 1.304  loss_cls_stage0: 0.03643  loss_box_reg_stage0: 0.2232  loss_cls_stage1: 0.01569  loss_box_reg_stage1: 0.3413  loss_cls_stage2: 0.009382  loss_box_reg_stage2: 0.4611  loss_mask: 0.113  loss_rpn_cls: 0.003189  loss_rpn_loc: 0.0389    time: 0.7991  last_time: 0.7895  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:25:47 d2.utils.events]: \u001b[0m eta: 0:34:15  iter: 9419  total_loss: 1.442  loss_cls_stage0: 0.02467  loss_box_reg_stage0: 0.2408  loss_cls_stage1: 0.007991  loss_box_reg_stage1: 0.3517  loss_cls_stage2: 0.01947  loss_box_reg_stage2: 0.5022  loss_mask: 0.1149  loss_rpn_cls: 0.002479  loss_rpn_loc: 0.031    time: 0.7991  last_time: 0.7970  data_time: 0.0018  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:26:03 d2.utils.events]: \u001b[0m eta: 0:33:59  iter: 9439  total_loss: 1.147  loss_cls_stage0: 0.0187  loss_box_reg_stage0: 0.2104  loss_cls_stage1: 0.0121  loss_box_reg_stage1: 0.2799  loss_cls_stage2: 0.01248  loss_box_reg_stage2: 0.4113  loss_mask: 0.1062  loss_rpn_cls: 0.006001  loss_rpn_loc: 0.0412    time: 0.7990  last_time: 0.8113  data_time: 0.0020  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:26:19 d2.utils.events]: \u001b[0m eta: 0:33:43  iter: 9459  total_loss: 1.317  loss_cls_stage0: 0.02229  loss_box_reg_stage0: 0.2455  loss_cls_stage1: 0.01435  loss_box_reg_stage1: 0.3556  loss_cls_stage2: 0.01569  loss_box_reg_stage2: 0.4976  loss_mask: 0.1114  loss_rpn_cls: 0.002911  loss_rpn_loc: 0.03078    time: 0.7990  last_time: 0.7986  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:26:35 d2.utils.events]: \u001b[0m eta: 0:33:28  iter: 9479  total_loss: 1.4  loss_cls_stage0: 0.03283  loss_box_reg_stage0: 0.247  loss_cls_stage1: 0.02032  loss_box_reg_stage1: 0.3567  loss_cls_stage2: 0.02275  loss_box_reg_stage2: 0.5297  loss_mask: 0.136  loss_rpn_cls: 0.0049  loss_rpn_loc: 0.02712    time: 0.7990  last_time: 0.8001  data_time: 0.0021  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:26:51 d2.utils.events]: \u001b[0m eta: 0:33:12  iter: 9499  total_loss: 1.422  loss_cls_stage0: 0.02778  loss_box_reg_stage0: 0.2624  loss_cls_stage1: 0.01349  loss_box_reg_stage1: 0.3502  loss_cls_stage2: 0.01923  loss_box_reg_stage2: 0.548  loss_mask: 0.1239  loss_rpn_cls: 0.003684  loss_rpn_loc: 0.03329    time: 0.7990  last_time: 0.8063  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:27:07 d2.utils.events]: \u001b[0m eta: 0:32:57  iter: 9519  total_loss: 1.312  loss_cls_stage0: 0.03021  loss_box_reg_stage0: 0.2462  loss_cls_stage1: 0.01311  loss_box_reg_stage1: 0.3462  loss_cls_stage2: 0.02731  loss_box_reg_stage2: 0.4929  loss_mask: 0.1108  loss_rpn_cls: 0.004104  loss_rpn_loc: 0.03701    time: 0.7990  last_time: 0.8047  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:27:23 d2.utils.events]: \u001b[0m eta: 0:32:41  iter: 9539  total_loss: 1.109  loss_cls_stage0: 0.01958  loss_box_reg_stage0: 0.2048  loss_cls_stage1: 0.009182  loss_box_reg_stage1: 0.301  loss_cls_stage2: 0.006528  loss_box_reg_stage2: 0.3946  loss_mask: 0.1189  loss_rpn_cls: 0.003382  loss_rpn_loc: 0.03127    time: 0.7990  last_time: 0.8090  data_time: 0.0017  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:27:39 d2.utils.events]: \u001b[0m eta: 0:32:25  iter: 9559  total_loss: 1.293  loss_cls_stage0: 0.02286  loss_box_reg_stage0: 0.2186  loss_cls_stage1: 0.01936  loss_box_reg_stage1: 0.3155  loss_cls_stage2: 0.02558  loss_box_reg_stage2: 0.4081  loss_mask: 0.1283  loss_rpn_cls: 0.006908  loss_rpn_loc: 0.03764    time: 0.7990  last_time: 0.8015  data_time: 0.0015  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:27:55 d2.utils.events]: \u001b[0m eta: 0:32:08  iter: 9579  total_loss: 1.242  loss_cls_stage0: 0.02115  loss_box_reg_stage0: 0.2325  loss_cls_stage1: 0.01572  loss_box_reg_stage1: 0.32  loss_cls_stage2: 0.02564  loss_box_reg_stage2: 0.509  loss_mask: 0.1181  loss_rpn_cls: 0.003012  loss_rpn_loc: 0.0331    time: 0.7990  last_time: 0.8077  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:28:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:28:10 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:28:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:28:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:28:10 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:28:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:28:10 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:28:10 d2.utils.events]: \u001b[0m eta: 0:31:52  iter: 9599  total_loss: 1.369  loss_cls_stage0: 0.021  loss_box_reg_stage0: 0.2567  loss_cls_stage1: 0.01389  loss_box_reg_stage1: 0.3581  loss_cls_stage2: 0.01858  loss_box_reg_stage2: 0.5229  loss_mask: 0.1362  loss_rpn_cls: 0.004595  loss_rpn_loc: 0.03472    time: 0.7990  last_time: 0.7856  data_time: 0.0019  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:28:27 d2.utils.events]: \u001b[0m eta: 0:31:37  iter: 9619  total_loss: 1.229  loss_cls_stage0: 0.02028  loss_box_reg_stage0: 0.2222  loss_cls_stage1: 0.01571  loss_box_reg_stage1: 0.303  loss_cls_stage2: 0.01556  loss_box_reg_stage2: 0.4574  loss_mask: 0.1132  loss_rpn_cls: 0.003224  loss_rpn_loc: 0.04097    time: 0.7990  last_time: 0.7983  data_time: 0.0017  last_data_time: 0.0028   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:28:42 d2.utils.events]: \u001b[0m eta: 0:31:21  iter: 9639  total_loss: 1.317  loss_cls_stage0: 0.03417  loss_box_reg_stage0: 0.2498  loss_cls_stage1: 0.01808  loss_box_reg_stage1: 0.3649  loss_cls_stage2: 0.01563  loss_box_reg_stage2: 0.4702  loss_mask: 0.1295  loss_rpn_cls: 0.005018  loss_rpn_loc: 0.04125    time: 0.7990  last_time: 0.7899  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:28:58 d2.utils.events]: \u001b[0m eta: 0:31:05  iter: 9659  total_loss: 1.298  loss_cls_stage0: 0.03608  loss_box_reg_stage0: 0.2422  loss_cls_stage1: 0.01204  loss_box_reg_stage1: 0.3115  loss_cls_stage2: 0.02014  loss_box_reg_stage2: 0.4843  loss_mask: 0.1254  loss_rpn_cls: 0.003246  loss_rpn_loc: 0.0299    time: 0.7990  last_time: 0.8028  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:29:14 d2.utils.events]: \u001b[0m eta: 0:30:48  iter: 9679  total_loss: 1.351  loss_cls_stage0: 0.02397  loss_box_reg_stage0: 0.2187  loss_cls_stage1: 0.01011  loss_box_reg_stage1: 0.3618  loss_cls_stage2: 0.01637  loss_box_reg_stage2: 0.4916  loss_mask: 0.1324  loss_rpn_cls: 0.002928  loss_rpn_loc: 0.03655    time: 0.7990  last_time: 0.8010  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:29:30 d2.utils.events]: \u001b[0m eta: 0:30:32  iter: 9699  total_loss: 1.097  loss_cls_stage0: 0.0212  loss_box_reg_stage0: 0.2227  loss_cls_stage1: 0.01171  loss_box_reg_stage1: 0.2614  loss_cls_stage2: 0.01116  loss_box_reg_stage2: 0.3981  loss_mask: 0.1199  loss_rpn_cls: 0.002763  loss_rpn_loc: 0.02812    time: 0.7990  last_time: 0.7976  data_time: 0.0015  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:29:46 d2.utils.events]: \u001b[0m eta: 0:30:16  iter: 9719  total_loss: 1.11  loss_cls_stage0: 0.01692  loss_box_reg_stage0: 0.2387  loss_cls_stage1: 0.00871  loss_box_reg_stage1: 0.3076  loss_cls_stage2: 0.01698  loss_box_reg_stage2: 0.3866  loss_mask: 0.1181  loss_rpn_cls: 0.00609  loss_rpn_loc: 0.03474    time: 0.7990  last_time: 0.7901  data_time: 0.0020  last_data_time: 0.0029   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:30:02 d2.utils.events]: \u001b[0m eta: 0:30:00  iter: 9739  total_loss: 1.102  loss_cls_stage0: 0.02854  loss_box_reg_stage0: 0.2389  loss_cls_stage1: 0.004511  loss_box_reg_stage1: 0.2793  loss_cls_stage2: 0.008512  loss_box_reg_stage2: 0.3742  loss_mask: 0.1065  loss_rpn_cls: 0.002924  loss_rpn_loc: 0.0241    time: 0.7990  last_time: 0.8008  data_time: 0.0015  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:30:18 d2.utils.events]: \u001b[0m eta: 0:29:44  iter: 9759  total_loss: 1.2  loss_cls_stage0: 0.02457  loss_box_reg_stage0: 0.2234  loss_cls_stage1: 0.01055  loss_box_reg_stage1: 0.3089  loss_cls_stage2: 0.01365  loss_box_reg_stage2: 0.4884  loss_mask: 0.1194  loss_rpn_cls: 0.003415  loss_rpn_loc: 0.02924    time: 0.7990  last_time: 0.8018  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:30:34 d2.utils.events]: \u001b[0m eta: 0:29:29  iter: 9779  total_loss: 1.247  loss_cls_stage0: 0.02288  loss_box_reg_stage0: 0.2238  loss_cls_stage1: 0.01174  loss_box_reg_stage1: 0.3183  loss_cls_stage2: 0.01869  loss_box_reg_stage2: 0.4755  loss_mask: 0.1125  loss_rpn_cls: 0.003104  loss_rpn_loc: 0.03678    time: 0.7990  last_time: 0.8033  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:30:50 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:30:50 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:30:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:30:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:30:50 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:30:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:30:50 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:30:50 d2.utils.events]: \u001b[0m eta: 0:29:13  iter: 9799  total_loss: 1.21  loss_cls_stage0: 0.01957  loss_box_reg_stage0: 0.2062  loss_cls_stage1: 0.01177  loss_box_reg_stage1: 0.3138  loss_cls_stage2: 0.009468  loss_box_reg_stage2: 0.4767  loss_mask: 0.1197  loss_rpn_cls: 0.003426  loss_rpn_loc: 0.0275    time: 0.7990  last_time: 0.7899  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:31:06 d2.utils.events]: \u001b[0m eta: 0:28:57  iter: 9819  total_loss: 1.199  loss_cls_stage0: 0.01737  loss_box_reg_stage0: 0.222  loss_cls_stage1: 0.01978  loss_box_reg_stage1: 0.3367  loss_cls_stage2: 0.013  loss_box_reg_stage2: 0.4552  loss_mask: 0.1164  loss_rpn_cls: 0.003777  loss_rpn_loc: 0.0306    time: 0.7990  last_time: 0.8015  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:31:22 d2.utils.events]: \u001b[0m eta: 0:28:41  iter: 9839  total_loss: 1.281  loss_cls_stage0: 0.016  loss_box_reg_stage0: 0.2419  loss_cls_stage1: 0.01178  loss_box_reg_stage1: 0.3359  loss_cls_stage2: 0.02103  loss_box_reg_stage2: 0.4457  loss_mask: 0.1187  loss_rpn_cls: 0.004647  loss_rpn_loc: 0.02843    time: 0.7989  last_time: 0.7977  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:31:38 d2.utils.events]: \u001b[0m eta: 0:28:25  iter: 9859  total_loss: 1.227  loss_cls_stage0: 0.01409  loss_box_reg_stage0: 0.2246  loss_cls_stage1: 0.01008  loss_box_reg_stage1: 0.3135  loss_cls_stage2: 0.02059  loss_box_reg_stage2: 0.4439  loss_mask: 0.1162  loss_rpn_cls: 0.001617  loss_rpn_loc: 0.03068    time: 0.7989  last_time: 0.8004  data_time: 0.0027  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:31:54 d2.utils.events]: \u001b[0m eta: 0:28:09  iter: 9879  total_loss: 1.225  loss_cls_stage0: 0.0233  loss_box_reg_stage0: 0.2141  loss_cls_stage1: 0.01103  loss_box_reg_stage1: 0.3249  loss_cls_stage2: 0.01276  loss_box_reg_stage2: 0.4811  loss_mask: 0.1267  loss_rpn_cls: 0.002891  loss_rpn_loc: 0.03198    time: 0.7989  last_time: 0.7922  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:32:10 d2.utils.events]: \u001b[0m eta: 0:27:53  iter: 9899  total_loss: 1.122  loss_cls_stage0: 0.02279  loss_box_reg_stage0: 0.2127  loss_cls_stage1: 0.006589  loss_box_reg_stage1: 0.2631  loss_cls_stage2: 0.01385  loss_box_reg_stage2: 0.4591  loss_mask: 0.1102  loss_rpn_cls: 0.002833  loss_rpn_loc: 0.03756    time: 0.7989  last_time: 0.8041  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:32:26 d2.utils.events]: \u001b[0m eta: 0:27:37  iter: 9919  total_loss: 1.134  loss_cls_stage0: 0.01222  loss_box_reg_stage0: 0.1958  loss_cls_stage1: 0.004791  loss_box_reg_stage1: 0.2624  loss_cls_stage2: 0.01194  loss_box_reg_stage2: 0.387  loss_mask: 0.1236  loss_rpn_cls: 0.003377  loss_rpn_loc: 0.02954    time: 0.7989  last_time: 0.7950  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:32:41 d2.utils.events]: \u001b[0m eta: 0:27:21  iter: 9939  total_loss: 1.205  loss_cls_stage0: 0.03108  loss_box_reg_stage0: 0.2348  loss_cls_stage1: 0.01213  loss_box_reg_stage1: 0.2929  loss_cls_stage2: 0.01144  loss_box_reg_stage2: 0.4366  loss_mask: 0.1145  loss_rpn_cls: 0.002321  loss_rpn_loc: 0.02427    time: 0.7989  last_time: 0.7891  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:32:57 d2.utils.events]: \u001b[0m eta: 0:27:05  iter: 9959  total_loss: 1.502  loss_cls_stage0: 0.02453  loss_box_reg_stage0: 0.2499  loss_cls_stage1: 0.01338  loss_box_reg_stage1: 0.3959  loss_cls_stage2: 0.02031  loss_box_reg_stage2: 0.5629  loss_mask: 0.1297  loss_rpn_cls: 0.005023  loss_rpn_loc: 0.03245    time: 0.7989  last_time: 0.7884  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:33:13 d2.utils.events]: \u001b[0m eta: 0:26:49  iter: 9979  total_loss: 1.228  loss_cls_stage0: 0.03103  loss_box_reg_stage0: 0.2161  loss_cls_stage1: 0.01643  loss_box_reg_stage1: 0.3298  loss_cls_stage2: 0.01816  loss_box_reg_stage2: 0.4071  loss_mask: 0.1226  loss_rpn_cls: 0.002945  loss_rpn_loc: 0.03448    time: 0.7989  last_time: 0.8012  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:33:30 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:33:30 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:33:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:33:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:33:30 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:33:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:33:30 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:33:30 d2.utils.events]: \u001b[0m eta: 0:26:33  iter: 9999  total_loss: 1.116  loss_cls_stage0: 0.02573  loss_box_reg_stage0: 0.2019  loss_cls_stage1: 0.01096  loss_box_reg_stage1: 0.2616  loss_cls_stage2: 0.0134  loss_box_reg_stage2: 0.4321  loss_mask: 0.1366  loss_rpn_cls: 0.003874  loss_rpn_loc: 0.03525    time: 0.7989  last_time: 0.8002  data_time: 0.0015  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:33:46 d2.utils.events]: \u001b[0m eta: 0:26:17  iter: 10019  total_loss: 1.376  loss_cls_stage0: 0.01464  loss_box_reg_stage0: 0.2396  loss_cls_stage1: 0.01368  loss_box_reg_stage1: 0.3463  loss_cls_stage2: 0.02978  loss_box_reg_stage2: 0.5242  loss_mask: 0.1269  loss_rpn_cls: 0.002142  loss_rpn_loc: 0.04069    time: 0.7989  last_time: 0.7925  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:34:02 d2.utils.events]: \u001b[0m eta: 0:26:01  iter: 10039  total_loss: 1.15  loss_cls_stage0: 0.02732  loss_box_reg_stage0: 0.21  loss_cls_stage1: 0.009239  loss_box_reg_stage1: 0.251  loss_cls_stage2: 0.01279  loss_box_reg_stage2: 0.3986  loss_mask: 0.1111  loss_rpn_cls: 0.004276  loss_rpn_loc: 0.03936    time: 0.7989  last_time: 0.7991  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:34:18 d2.utils.events]: \u001b[0m eta: 0:25:45  iter: 10059  total_loss: 1.095  loss_cls_stage0: 0.02599  loss_box_reg_stage0: 0.1966  loss_cls_stage1: 0.006222  loss_box_reg_stage1: 0.2982  loss_cls_stage2: 0.01256  loss_box_reg_stage2: 0.4323  loss_mask: 0.1019  loss_rpn_cls: 0.003398  loss_rpn_loc: 0.02569    time: 0.7989  last_time: 0.7870  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:34:34 d2.utils.events]: \u001b[0m eta: 0:25:30  iter: 10079  total_loss: 1.345  loss_cls_stage0: 0.04305  loss_box_reg_stage0: 0.2352  loss_cls_stage1: 0.01082  loss_box_reg_stage1: 0.3543  loss_cls_stage2: 0.01315  loss_box_reg_stage2: 0.5008  loss_mask: 0.1105  loss_rpn_cls: 0.003874  loss_rpn_loc: 0.03416    time: 0.7989  last_time: 0.7807  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:34:50 d2.utils.events]: \u001b[0m eta: 0:25:13  iter: 10099  total_loss: 0.9733  loss_cls_stage0: 0.02475  loss_box_reg_stage0: 0.206  loss_cls_stage1: 0.008031  loss_box_reg_stage1: 0.2385  loss_cls_stage2: 0.01229  loss_box_reg_stage2: 0.3362  loss_mask: 0.103  loss_rpn_cls: 0.003441  loss_rpn_loc: 0.02619    time: 0.7989  last_time: 0.8001  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:35:06 d2.utils.events]: \u001b[0m eta: 0:24:57  iter: 10119  total_loss: 1.082  loss_cls_stage0: 0.01588  loss_box_reg_stage0: 0.212  loss_cls_stage1: 0.00657  loss_box_reg_stage1: 0.2645  loss_cls_stage2: 0.01436  loss_box_reg_stage2: 0.4134  loss_mask: 0.0964  loss_rpn_cls: 0.002753  loss_rpn_loc: 0.0285    time: 0.7989  last_time: 0.7926  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:35:22 d2.utils.events]: \u001b[0m eta: 0:24:41  iter: 10139  total_loss: 1.316  loss_cls_stage0: 0.03415  loss_box_reg_stage0: 0.2622  loss_cls_stage1: 0.01515  loss_box_reg_stage1: 0.369  loss_cls_stage2: 0.02747  loss_box_reg_stage2: 0.4647  loss_mask: 0.1291  loss_rpn_cls: 0.002629  loss_rpn_loc: 0.0362    time: 0.7989  last_time: 0.7961  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:35:38 d2.utils.events]: \u001b[0m eta: 0:24:25  iter: 10159  total_loss: 1.189  loss_cls_stage0: 0.01465  loss_box_reg_stage0: 0.231  loss_cls_stage1: 0.005924  loss_box_reg_stage1: 0.3296  loss_cls_stage2: 0.01644  loss_box_reg_stage2: 0.4468  loss_mask: 0.1104  loss_rpn_cls: 0.00345  loss_rpn_loc: 0.02409    time: 0.7989  last_time: 0.7919  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:35:54 d2.utils.events]: \u001b[0m eta: 0:24:10  iter: 10179  total_loss: 1.149  loss_cls_stage0: 0.02693  loss_box_reg_stage0: 0.2211  loss_cls_stage1: 0.01139  loss_box_reg_stage1: 0.2854  loss_cls_stage2: 0.02553  loss_box_reg_stage2: 0.3865  loss_mask: 0.1183  loss_rpn_cls: 0.002924  loss_rpn_loc: 0.03219    time: 0.7989  last_time: 0.7943  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:36:10 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:36:10 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:36:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:36:10 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:36:10 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:36:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:36:10 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:36:10 d2.utils.events]: \u001b[0m eta: 0:23:54  iter: 10199  total_loss: 1.184  loss_cls_stage0: 0.01624  loss_box_reg_stage0: 0.2057  loss_cls_stage1: 0.01572  loss_box_reg_stage1: 0.3231  loss_cls_stage2: 0.0254  loss_box_reg_stage2: 0.4445  loss_mask: 0.115  loss_rpn_cls: 0.003367  loss_rpn_loc: 0.03968    time: 0.7989  last_time: 0.8042  data_time: 0.0017  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:36:26 d2.utils.events]: \u001b[0m eta: 0:23:38  iter: 10219  total_loss: 1.255  loss_cls_stage0: 0.02425  loss_box_reg_stage0: 0.2351  loss_cls_stage1: 0.01057  loss_box_reg_stage1: 0.3433  loss_cls_stage2: 0.01795  loss_box_reg_stage2: 0.4966  loss_mask: 0.1159  loss_rpn_cls: 0.002433  loss_rpn_loc: 0.03443    time: 0.7989  last_time: 0.7965  data_time: 0.0018  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:36:42 d2.utils.events]: \u001b[0m eta: 0:23:21  iter: 10239  total_loss: 1.067  loss_cls_stage0: 0.02391  loss_box_reg_stage0: 0.2023  loss_cls_stage1: 0.01079  loss_box_reg_stage1: 0.2662  loss_cls_stage2: 0.01238  loss_box_reg_stage2: 0.3838  loss_mask: 0.1235  loss_rpn_cls: 0.003576  loss_rpn_loc: 0.03415    time: 0.7989  last_time: 0.7954  data_time: 0.0016  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:36:58 d2.utils.events]: \u001b[0m eta: 0:23:05  iter: 10259  total_loss: 1.071  loss_cls_stage0: 0.02489  loss_box_reg_stage0: 0.2032  loss_cls_stage1: 0.006638  loss_box_reg_stage1: 0.2706  loss_cls_stage2: 0.007179  loss_box_reg_stage2: 0.378  loss_mask: 0.1225  loss_rpn_cls: 0.002804  loss_rpn_loc: 0.02383    time: 0.7988  last_time: 0.7862  data_time: 0.0017  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:37:13 d2.utils.events]: \u001b[0m eta: 0:22:49  iter: 10279  total_loss: 1.531  loss_cls_stage0: 0.03795  loss_box_reg_stage0: 0.2511  loss_cls_stage1: 0.03138  loss_box_reg_stage1: 0.4036  loss_cls_stage2: 0.02329  loss_box_reg_stage2: 0.6002  loss_mask: 0.1386  loss_rpn_cls: 0.002561  loss_rpn_loc: 0.04424    time: 0.7988  last_time: 0.7851  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:37:29 d2.utils.events]: \u001b[0m eta: 0:22:33  iter: 10299  total_loss: 1.277  loss_cls_stage0: 0.02286  loss_box_reg_stage0: 0.2107  loss_cls_stage1: 0.01339  loss_box_reg_stage1: 0.3402  loss_cls_stage2: 0.02206  loss_box_reg_stage2: 0.4238  loss_mask: 0.1293  loss_rpn_cls: 0.004067  loss_rpn_loc: 0.04488    time: 0.7988  last_time: 0.7948  data_time: 0.0016  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:37:45 d2.utils.events]: \u001b[0m eta: 0:22:17  iter: 10319  total_loss: 1.054  loss_cls_stage0: 0.01694  loss_box_reg_stage0: 0.207  loss_cls_stage1: 0.006337  loss_box_reg_stage1: 0.2837  loss_cls_stage2: 0.01152  loss_box_reg_stage2: 0.3871  loss_mask: 0.1156  loss_rpn_cls: 0.00413  loss_rpn_loc: 0.03753    time: 0.7988  last_time: 0.8000  data_time: 0.0018  last_data_time: 0.0017   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:38:01 d2.utils.events]: \u001b[0m eta: 0:22:01  iter: 10339  total_loss: 1.124  loss_cls_stage0: 0.01551  loss_box_reg_stage0: 0.2216  loss_cls_stage1: 0.007523  loss_box_reg_stage1: 0.3022  loss_cls_stage2: 0.0123  loss_box_reg_stage2: 0.4204  loss_mask: 0.1114  loss_rpn_cls: 0.002404  loss_rpn_loc: 0.03004    time: 0.7988  last_time: 0.7945  data_time: 0.0015  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:38:17 d2.utils.events]: \u001b[0m eta: 0:21:45  iter: 10359  total_loss: 1.391  loss_cls_stage0: 0.03143  loss_box_reg_stage0: 0.2504  loss_cls_stage1: 0.01756  loss_box_reg_stage1: 0.3617  loss_cls_stage2: 0.02451  loss_box_reg_stage2: 0.5136  loss_mask: 0.1235  loss_rpn_cls: 0.00371  loss_rpn_loc: 0.03793    time: 0.7988  last_time: 0.7884  data_time: 0.0017  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:38:33 d2.utils.events]: \u001b[0m eta: 0:21:29  iter: 10379  total_loss: 1.041  loss_cls_stage0: 0.01865  loss_box_reg_stage0: 0.2021  loss_cls_stage1: 0.01161  loss_box_reg_stage1: 0.2685  loss_cls_stage2: 0.01764  loss_box_reg_stage2: 0.3677  loss_mask: 0.1076  loss_rpn_cls: 0.002384  loss_rpn_loc: 0.02178    time: 0.7988  last_time: 0.7917  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:38:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:38:49 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:38:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:38:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:38:49 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:38:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:38:49 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:38:49 d2.utils.events]: \u001b[0m eta: 0:21:13  iter: 10399  total_loss: 1.212  loss_cls_stage0: 0.02497  loss_box_reg_stage0: 0.2162  loss_cls_stage1: 0.008526  loss_box_reg_stage1: 0.3306  loss_cls_stage2: 0.01181  loss_box_reg_stage2: 0.4454  loss_mask: 0.1211  loss_rpn_cls: 0.002718  loss_rpn_loc: 0.02707    time: 0.7988  last_time: 0.8155  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:39:05 d2.utils.events]: \u001b[0m eta: 0:20:57  iter: 10419  total_loss: 1.243  loss_cls_stage0: 0.01901  loss_box_reg_stage0: 0.2172  loss_cls_stage1: 0.01527  loss_box_reg_stage1: 0.316  loss_cls_stage2: 0.02278  loss_box_reg_stage2: 0.5013  loss_mask: 0.1255  loss_rpn_cls: 0.002027  loss_rpn_loc: 0.03526    time: 0.7988  last_time: 0.8049  data_time: 0.0017  last_data_time: 0.0027   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:39:21 d2.utils.events]: \u001b[0m eta: 0:20:41  iter: 10439  total_loss: 1.057  loss_cls_stage0: 0.01691  loss_box_reg_stage0: 0.2077  loss_cls_stage1: 0.007244  loss_box_reg_stage1: 0.2711  loss_cls_stage2: 0.01254  loss_box_reg_stage2: 0.3576  loss_mask: 0.1016  loss_rpn_cls: 0.003668  loss_rpn_loc: 0.04304    time: 0.7988  last_time: 0.7965  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:39:37 d2.utils.events]: \u001b[0m eta: 0:20:25  iter: 10459  total_loss: 1.539  loss_cls_stage0: 0.03963  loss_box_reg_stage0: 0.2703  loss_cls_stage1: 0.02656  loss_box_reg_stage1: 0.4019  loss_cls_stage2: 0.02604  loss_box_reg_stage2: 0.4837  loss_mask: 0.1319  loss_rpn_cls: 0.006094  loss_rpn_loc: 0.03864    time: 0.7988  last_time: 0.7969  data_time: 0.0015  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:39:53 d2.utils.events]: \u001b[0m eta: 0:20:09  iter: 10479  total_loss: 1.216  loss_cls_stage0: 0.02294  loss_box_reg_stage0: 0.2068  loss_cls_stage1: 0.0184  loss_box_reg_stage1: 0.3058  loss_cls_stage2: 0.0211  loss_box_reg_stage2: 0.4286  loss_mask: 0.1206  loss_rpn_cls: 0.003887  loss_rpn_loc: 0.02549    time: 0.7988  last_time: 0.7890  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:40:09 d2.utils.events]: \u001b[0m eta: 0:19:53  iter: 10499  total_loss: 1.376  loss_cls_stage0: 0.02644  loss_box_reg_stage0: 0.2262  loss_cls_stage1: 0.009418  loss_box_reg_stage1: 0.3787  loss_cls_stage2: 0.01956  loss_box_reg_stage2: 0.561  loss_mask: 0.1213  loss_rpn_cls: 0.003063  loss_rpn_loc: 0.0252    time: 0.7988  last_time: 0.7929  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:40:25 d2.utils.events]: \u001b[0m eta: 0:19:37  iter: 10519  total_loss: 1.24  loss_cls_stage0: 0.02332  loss_box_reg_stage0: 0.2344  loss_cls_stage1: 0.01002  loss_box_reg_stage1: 0.3126  loss_cls_stage2: 0.0161  loss_box_reg_stage2: 0.4466  loss_mask: 0.1038  loss_rpn_cls: 0.002668  loss_rpn_loc: 0.03591    time: 0.7988  last_time: 0.7990  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:40:41 d2.utils.events]: \u001b[0m eta: 0:19:21  iter: 10539  total_loss: 1.1  loss_cls_stage0: 0.01663  loss_box_reg_stage0: 0.2186  loss_cls_stage1: 0.007249  loss_box_reg_stage1: 0.2748  loss_cls_stage2: 0.0129  loss_box_reg_stage2: 0.4241  loss_mask: 0.1085  loss_rpn_cls: 0.002723  loss_rpn_loc: 0.02327    time: 0.7988  last_time: 0.7942  data_time: 0.0016  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:40:57 d2.utils.events]: \u001b[0m eta: 0:19:05  iter: 10559  total_loss: 1.235  loss_cls_stage0: 0.01927  loss_box_reg_stage0: 0.2259  loss_cls_stage1: 0.01249  loss_box_reg_stage1: 0.3221  loss_cls_stage2: 0.01931  loss_box_reg_stage2: 0.4702  loss_mask: 0.1109  loss_rpn_cls: 0.004511  loss_rpn_loc: 0.03106    time: 0.7988  last_time: 0.7854  data_time: 0.0017  last_data_time: 0.0029   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:41:12 d2.utils.events]: \u001b[0m eta: 0:18:49  iter: 10579  total_loss: 1.264  loss_cls_stage0: 0.01757  loss_box_reg_stage0: 0.2426  loss_cls_stage1: 0.01435  loss_box_reg_stage1: 0.3438  loss_cls_stage2: 0.01218  loss_box_reg_stage2: 0.3833  loss_mask: 0.1404  loss_rpn_cls: 0.003722  loss_rpn_loc: 0.05637    time: 0.7988  last_time: 0.7949  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:41:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:41:28 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:41:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:41:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:41:28 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:41:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:41:28 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:41:28 d2.utils.events]: \u001b[0m eta: 0:18:33  iter: 10599  total_loss: 1.129  loss_cls_stage0: 0.01605  loss_box_reg_stage0: 0.2158  loss_cls_stage1: 0.006711  loss_box_reg_stage1: 0.2777  loss_cls_stage2: 0.01244  loss_box_reg_stage2: 0.4445  loss_mask: 0.1046  loss_rpn_cls: 0.003015  loss_rpn_loc: 0.0237    time: 0.7988  last_time: 0.7875  data_time: 0.0017  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:41:45 d2.utils.events]: \u001b[0m eta: 0:18:17  iter: 10619  total_loss: 1.19  loss_cls_stage0: 0.01798  loss_box_reg_stage0: 0.2178  loss_cls_stage1: 0.01163  loss_box_reg_stage1: 0.3041  loss_cls_stage2: 0.0103  loss_box_reg_stage2: 0.48  loss_mask: 0.1059  loss_rpn_cls: 0.002788  loss_rpn_loc: 0.02751    time: 0.7988  last_time: 0.8324  data_time: 0.0020  last_data_time: 0.0022   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:42:01 d2.utils.events]: \u001b[0m eta: 0:18:02  iter: 10639  total_loss: 1.205  loss_cls_stage0: 0.02386  loss_box_reg_stage0: 0.2163  loss_cls_stage1: 0.01001  loss_box_reg_stage1: 0.3052  loss_cls_stage2: 0.01019  loss_box_reg_stage2: 0.4305  loss_mask: 0.1175  loss_rpn_cls: 0.002336  loss_rpn_loc: 0.04188    time: 0.7988  last_time: 0.8207  data_time: 0.0021  last_data_time: 0.0024   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:42:17 d2.utils.events]: \u001b[0m eta: 0:17:46  iter: 10659  total_loss: 1.227  loss_cls_stage0: 0.03101  loss_box_reg_stage0: 0.2381  loss_cls_stage1: 0.0196  loss_box_reg_stage1: 0.311  loss_cls_stage2: 0.01717  loss_box_reg_stage2: 0.4429  loss_mask: 0.1108  loss_rpn_cls: 0.002358  loss_rpn_loc: 0.03871    time: 0.7988  last_time: 0.7880  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:42:33 d2.utils.events]: \u001b[0m eta: 0:17:30  iter: 10679  total_loss: 1.336  loss_cls_stage0: 0.03238  loss_box_reg_stage0: 0.2466  loss_cls_stage1: 0.02906  loss_box_reg_stage1: 0.3425  loss_cls_stage2: 0.01488  loss_box_reg_stage2: 0.46  loss_mask: 0.1154  loss_rpn_cls: 0.003437  loss_rpn_loc: 0.02418    time: 0.7988  last_time: 0.8082  data_time: 0.0024  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:42:49 d2.utils.events]: \u001b[0m eta: 0:17:14  iter: 10699  total_loss: 1.038  loss_cls_stage0: 0.02293  loss_box_reg_stage0: 0.1923  loss_cls_stage1: 0.00472  loss_box_reg_stage1: 0.2612  loss_cls_stage2: 0.009498  loss_box_reg_stage2: 0.3573  loss_mask: 0.09695  loss_rpn_cls: 0.001836  loss_rpn_loc: 0.02514    time: 0.7988  last_time: 0.8216  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:43:05 d2.utils.events]: \u001b[0m eta: 0:16:59  iter: 10719  total_loss: 1.251  loss_cls_stage0: 0.02642  loss_box_reg_stage0: 0.211  loss_cls_stage1: 0.01888  loss_box_reg_stage1: 0.3045  loss_cls_stage2: 0.02181  loss_box_reg_stage2: 0.4442  loss_mask: 0.1092  loss_rpn_cls: 0.003821  loss_rpn_loc: 0.03441    time: 0.7988  last_time: 0.7933  data_time: 0.0017  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:43:21 d2.utils.events]: \u001b[0m eta: 0:16:42  iter: 10739  total_loss: 1.148  loss_cls_stage0: 0.02519  loss_box_reg_stage0: 0.203  loss_cls_stage1: 0.005391  loss_box_reg_stage1: 0.2675  loss_cls_stage2: 0.01506  loss_box_reg_stage2: 0.3923  loss_mask: 0.1318  loss_rpn_cls: 0.002615  loss_rpn_loc: 0.02803    time: 0.7988  last_time: 0.7897  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:43:37 d2.utils.events]: \u001b[0m eta: 0:16:27  iter: 10759  total_loss: 1.122  loss_cls_stage0: 0.01224  loss_box_reg_stage0: 0.2009  loss_cls_stage1: 0.01065  loss_box_reg_stage1: 0.2885  loss_cls_stage2: 0.01328  loss_box_reg_stage2: 0.416  loss_mask: 0.1175  loss_rpn_cls: 0.002199  loss_rpn_loc: 0.03131    time: 0.7988  last_time: 0.7883  data_time: 0.0019  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:43:53 d2.utils.events]: \u001b[0m eta: 0:16:11  iter: 10779  total_loss: 1.081  loss_cls_stage0: 0.01799  loss_box_reg_stage0: 0.2318  loss_cls_stage1: 0.006438  loss_box_reg_stage1: 0.2711  loss_cls_stage2: 0.01424  loss_box_reg_stage2: 0.3734  loss_mask: 0.0972  loss_rpn_cls: 0.00202  loss_rpn_loc: 0.02603    time: 0.7988  last_time: 0.7895  data_time: 0.0018  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:44:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:44:09 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:44:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:44:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:44:09 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:44:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:44:09 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:44:09 d2.utils.events]: \u001b[0m eta: 0:15:55  iter: 10799  total_loss: 1.34  loss_cls_stage0: 0.02441  loss_box_reg_stage0: 0.2502  loss_cls_stage1: 0.01309  loss_box_reg_stage1: 0.3624  loss_cls_stage2: 0.008793  loss_box_reg_stage2: 0.5043  loss_mask: 0.1072  loss_rpn_cls: 0.002948  loss_rpn_loc: 0.02649    time: 0.7988  last_time: 0.8006  data_time: 0.0015  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:44:25 d2.utils.events]: \u001b[0m eta: 0:15:39  iter: 10819  total_loss: 1.16  loss_cls_stage0: 0.02518  loss_box_reg_stage0: 0.2196  loss_cls_stage1: 0.01254  loss_box_reg_stage1: 0.2748  loss_cls_stage2: 0.01339  loss_box_reg_stage2: 0.387  loss_mask: 0.09811  loss_rpn_cls: 0.002891  loss_rpn_loc: 0.02476    time: 0.7988  last_time: 0.8013  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:44:41 d2.utils.events]: \u001b[0m eta: 0:15:23  iter: 10839  total_loss: 0.9374  loss_cls_stage0: 0.0134  loss_box_reg_stage0: 0.1784  loss_cls_stage1: 0.006741  loss_box_reg_stage1: 0.2341  loss_cls_stage2: 0.00943  loss_box_reg_stage2: 0.338  loss_mask: 0.1089  loss_rpn_cls: 0.002526  loss_rpn_loc: 0.03053    time: 0.7988  last_time: 0.7932  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:44:57 d2.utils.events]: \u001b[0m eta: 0:15:07  iter: 10859  total_loss: 1.211  loss_cls_stage0: 0.0182  loss_box_reg_stage0: 0.2275  loss_cls_stage1: 0.01344  loss_box_reg_stage1: 0.3183  loss_cls_stage2: 0.01525  loss_box_reg_stage2: 0.473  loss_mask: 0.1063  loss_rpn_cls: 0.001932  loss_rpn_loc: 0.03403    time: 0.7988  last_time: 0.8087  data_time: 0.0017  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:45:13 d2.utils.events]: \u001b[0m eta: 0:14:51  iter: 10879  total_loss: 1.501  loss_cls_stage0: 0.01782  loss_box_reg_stage0: 0.2639  loss_cls_stage1: 0.01324  loss_box_reg_stage1: 0.403  loss_cls_stage2: 0.01853  loss_box_reg_stage2: 0.535  loss_mask: 0.1234  loss_rpn_cls: 0.005461  loss_rpn_loc: 0.03405    time: 0.7988  last_time: 0.7921  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:45:29 d2.utils.events]: \u001b[0m eta: 0:14:35  iter: 10899  total_loss: 1.11  loss_cls_stage0: 0.01582  loss_box_reg_stage0: 0.2101  loss_cls_stage1: 0.004843  loss_box_reg_stage1: 0.3113  loss_cls_stage2: 0.01566  loss_box_reg_stage2: 0.4216  loss_mask: 0.1114  loss_rpn_cls: 0.002308  loss_rpn_loc: 0.03234    time: 0.7988  last_time: 0.7945  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:45:45 d2.utils.events]: \u001b[0m eta: 0:14:19  iter: 10919  total_loss: 1.219  loss_cls_stage0: 0.02399  loss_box_reg_stage0: 0.223  loss_cls_stage1: 0.01009  loss_box_reg_stage1: 0.3129  loss_cls_stage2: 0.01266  loss_box_reg_stage2: 0.4404  loss_mask: 0.1191  loss_rpn_cls: 0.004571  loss_rpn_loc: 0.03848    time: 0.7988  last_time: 0.7967  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:46:01 d2.utils.events]: \u001b[0m eta: 0:14:04  iter: 10939  total_loss: 1.161  loss_cls_stage0: 0.01251  loss_box_reg_stage0: 0.2027  loss_cls_stage1: 0.01976  loss_box_reg_stage1: 0.2763  loss_cls_stage2: 0.02372  loss_box_reg_stage2: 0.4213  loss_mask: 0.1028  loss_rpn_cls: 0.001766  loss_rpn_loc: 0.03106    time: 0.7988  last_time: 0.8014  data_time: 0.0016  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:46:17 d2.utils.events]: \u001b[0m eta: 0:13:48  iter: 10959  total_loss: 1.204  loss_cls_stage0: 0.03349  loss_box_reg_stage0: 0.214  loss_cls_stage1: 0.01855  loss_box_reg_stage1: 0.3237  loss_cls_stage2: 0.01717  loss_box_reg_stage2: 0.4198  loss_mask: 0.1155  loss_rpn_cls: 0.003919  loss_rpn_loc: 0.03246    time: 0.7988  last_time: 0.8053  data_time: 0.0014  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:46:33 d2.utils.events]: \u001b[0m eta: 0:13:32  iter: 10979  total_loss: 1.066  loss_cls_stage0: 0.01528  loss_box_reg_stage0: 0.1994  loss_cls_stage1: 0.009992  loss_box_reg_stage1: 0.2888  loss_cls_stage2: 0.007441  loss_box_reg_stage2: 0.4207  loss_mask: 0.1164  loss_rpn_cls: 0.002433  loss_rpn_loc: 0.02786    time: 0.7988  last_time: 0.7959  data_time: 0.0017  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:46:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:46:49 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:46:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:46:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:46:49 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:46:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:46:49 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:46:49 d2.utils.events]: \u001b[0m eta: 0:13:16  iter: 10999  total_loss: 0.9924  loss_cls_stage0: 0.01582  loss_box_reg_stage0: 0.2035  loss_cls_stage1: 0.008872  loss_box_reg_stage1: 0.2423  loss_cls_stage2: 0.01777  loss_box_reg_stage2: 0.3887  loss_mask: 0.1092  loss_rpn_cls: 0.001743  loss_rpn_loc: 0.02494    time: 0.7988  last_time: 0.7878  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:47:05 d2.utils.events]: \u001b[0m eta: 0:13:00  iter: 11019  total_loss: 1.354  loss_cls_stage0: 0.01893  loss_box_reg_stage0: 0.2474  loss_cls_stage1: 0.01224  loss_box_reg_stage1: 0.3749  loss_cls_stage2: 0.01834  loss_box_reg_stage2: 0.4909  loss_mask: 0.1075  loss_rpn_cls: 0.005251  loss_rpn_loc: 0.03642    time: 0.7988  last_time: 0.8023  data_time: 0.0019  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:47:21 d2.utils.events]: \u001b[0m eta: 0:12:44  iter: 11039  total_loss: 1.026  loss_cls_stage0: 0.01786  loss_box_reg_stage0: 0.2108  loss_cls_stage1: 0.007535  loss_box_reg_stage1: 0.2584  loss_cls_stage2: 0.01472  loss_box_reg_stage2: 0.3837  loss_mask: 0.1065  loss_rpn_cls: 0.002526  loss_rpn_loc: 0.02516    time: 0.7988  last_time: 0.8117  data_time: 0.0017  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:47:37 d2.utils.events]: \u001b[0m eta: 0:12:28  iter: 11059  total_loss: 1.173  loss_cls_stage0: 0.02103  loss_box_reg_stage0: 0.2096  loss_cls_stage1: 0.0139  loss_box_reg_stage1: 0.309  loss_cls_stage2: 0.03326  loss_box_reg_stage2: 0.4167  loss_mask: 0.1231  loss_rpn_cls: 0.002674  loss_rpn_loc: 0.02874    time: 0.7988  last_time: 0.7895  data_time: 0.0018  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:47:53 d2.utils.events]: \u001b[0m eta: 0:12:12  iter: 11079  total_loss: 1.294  loss_cls_stage0: 0.02629  loss_box_reg_stage0: 0.223  loss_cls_stage1: 0.01132  loss_box_reg_stage1: 0.3293  loss_cls_stage2: 0.02321  loss_box_reg_stage2: 0.408  loss_mask: 0.1163  loss_rpn_cls: 0.002532  loss_rpn_loc: 0.03431    time: 0.7988  last_time: 0.7901  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:48:09 d2.utils.events]: \u001b[0m eta: 0:11:56  iter: 11099  total_loss: 1.167  loss_cls_stage0: 0.02139  loss_box_reg_stage0: 0.2209  loss_cls_stage1: 0.01162  loss_box_reg_stage1: 0.3132  loss_cls_stage2: 0.01434  loss_box_reg_stage2: 0.405  loss_mask: 0.1146  loss_rpn_cls: 0.001967  loss_rpn_loc: 0.02932    time: 0.7988  last_time: 0.7948  data_time: 0.0017  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:48:25 d2.utils.events]: \u001b[0m eta: 0:11:40  iter: 11119  total_loss: 1.054  loss_cls_stage0: 0.01367  loss_box_reg_stage0: 0.2067  loss_cls_stage1: 0.006142  loss_box_reg_stage1: 0.2541  loss_cls_stage2: 0.01046  loss_box_reg_stage2: 0.3792  loss_mask: 0.1136  loss_rpn_cls: 0.00316  loss_rpn_loc: 0.03058    time: 0.7988  last_time: 0.7871  data_time: 0.0017  last_data_time: 0.0027   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:48:41 d2.utils.events]: \u001b[0m eta: 0:11:25  iter: 11139  total_loss: 1.2  loss_cls_stage0: 0.01906  loss_box_reg_stage0: 0.2135  loss_cls_stage1: 0.009277  loss_box_reg_stage1: 0.3144  loss_cls_stage2: 0.02183  loss_box_reg_stage2: 0.4661  loss_mask: 0.1211  loss_rpn_cls: 0.002476  loss_rpn_loc: 0.03585    time: 0.7988  last_time: 0.7929  data_time: 0.0016  last_data_time: 0.0026   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:48:57 d2.utils.events]: \u001b[0m eta: 0:11:09  iter: 11159  total_loss: 0.8517  loss_cls_stage0: 0.01569  loss_box_reg_stage0: 0.1583  loss_cls_stage1: 0.004044  loss_box_reg_stage1: 0.223  loss_cls_stage2: 0.00809  loss_box_reg_stage2: 0.3276  loss_mask: 0.08715  loss_rpn_cls: 0.001993  loss_rpn_loc: 0.01693    time: 0.7988  last_time: 0.7902  data_time: 0.0017  last_data_time: 0.0027   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:49:13 d2.utils.events]: \u001b[0m eta: 0:10:53  iter: 11179  total_loss: 1.05  loss_cls_stage0: 0.02199  loss_box_reg_stage0: 0.2138  loss_cls_stage1: 0.01103  loss_box_reg_stage1: 0.2596  loss_cls_stage2: 0.0117  loss_box_reg_stage2: 0.3773  loss_mask: 0.1072  loss_rpn_cls: 0.003414  loss_rpn_loc: 0.02184    time: 0.7988  last_time: 0.7933  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:49:29 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:49:29 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:49:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:49:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:49:29 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:49:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:49:29 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:49:29 d2.utils.events]: \u001b[0m eta: 0:10:37  iter: 11199  total_loss: 1.034  loss_cls_stage0: 0.01842  loss_box_reg_stage0: 0.2149  loss_cls_stage1: 0.008205  loss_box_reg_stage1: 0.2799  loss_cls_stage2: 0.01584  loss_box_reg_stage2: 0.3903  loss_mask: 0.1124  loss_rpn_cls: 0.003279  loss_rpn_loc: 0.03786    time: 0.7988  last_time: 0.8039  data_time: 0.0017  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:49:45 d2.utils.events]: \u001b[0m eta: 0:10:21  iter: 11219  total_loss: 1.258  loss_cls_stage0: 0.02495  loss_box_reg_stage0: 0.2145  loss_cls_stage1: 0.01329  loss_box_reg_stage1: 0.3039  loss_cls_stage2: 0.01331  loss_box_reg_stage2: 0.454  loss_mask: 0.1139  loss_rpn_cls: 0.003815  loss_rpn_loc: 0.04787    time: 0.7988  last_time: 0.8070  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:50:01 d2.utils.events]: \u001b[0m eta: 0:10:05  iter: 11239  total_loss: 0.9952  loss_cls_stage0: 0.01567  loss_box_reg_stage0: 0.2129  loss_cls_stage1: 0.006387  loss_box_reg_stage1: 0.2849  loss_cls_stage2: 0.01354  loss_box_reg_stage2: 0.3641  loss_mask: 0.1217  loss_rpn_cls: 0.002862  loss_rpn_loc: 0.02695    time: 0.7988  last_time: 0.8069  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:50:17 d2.utils.events]: \u001b[0m eta: 0:09:49  iter: 11259  total_loss: 0.9843  loss_cls_stage0: 0.01829  loss_box_reg_stage0: 0.2097  loss_cls_stage1: 0.01209  loss_box_reg_stage1: 0.2448  loss_cls_stage2: 0.01254  loss_box_reg_stage2: 0.3022  loss_mask: 0.09751  loss_rpn_cls: 0.002766  loss_rpn_loc: 0.03527    time: 0.7988  last_time: 0.8033  data_time: 0.0015  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:50:33 d2.utils.events]: \u001b[0m eta: 0:09:34  iter: 11279  total_loss: 1.25  loss_cls_stage0: 0.02339  loss_box_reg_stage0: 0.2265  loss_cls_stage1: 0.01209  loss_box_reg_stage1: 0.332  loss_cls_stage2: 0.0203  loss_box_reg_stage2: 0.449  loss_mask: 0.1064  loss_rpn_cls: 0.003217  loss_rpn_loc: 0.02918    time: 0.7988  last_time: 0.7935  data_time: 0.0016  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:50:49 d2.utils.events]: \u001b[0m eta: 0:09:18  iter: 11299  total_loss: 1.148  loss_cls_stage0: 0.01898  loss_box_reg_stage0: 0.2241  loss_cls_stage1: 0.008754  loss_box_reg_stage1: 0.3117  loss_cls_stage2: 0.01839  loss_box_reg_stage2: 0.4433  loss_mask: 0.1228  loss_rpn_cls: 0.002633  loss_rpn_loc: 0.02506    time: 0.7988  last_time: 0.8201  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:51:05 d2.utils.events]: \u001b[0m eta: 0:09:02  iter: 11319  total_loss: 1.062  loss_cls_stage0: 0.021  loss_box_reg_stage0: 0.2111  loss_cls_stage1: 0.00864  loss_box_reg_stage1: 0.2952  loss_cls_stage2: 0.01717  loss_box_reg_stage2: 0.4023  loss_mask: 0.1064  loss_rpn_cls: 0.003123  loss_rpn_loc: 0.03266    time: 0.7988  last_time: 0.7902  data_time: 0.0016  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:51:21 d2.utils.events]: \u001b[0m eta: 0:08:46  iter: 11339  total_loss: 1.261  loss_cls_stage0: 0.03476  loss_box_reg_stage0: 0.2276  loss_cls_stage1: 0.01391  loss_box_reg_stage1: 0.3295  loss_cls_stage2: 0.01697  loss_box_reg_stage2: 0.4429  loss_mask: 0.1168  loss_rpn_cls: 0.003583  loss_rpn_loc: 0.037    time: 0.7988  last_time: 0.7941  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:51:37 d2.utils.events]: \u001b[0m eta: 0:08:30  iter: 11359  total_loss: 1.087  loss_cls_stage0: 0.02704  loss_box_reg_stage0: 0.209  loss_cls_stage1: 0.007294  loss_box_reg_stage1: 0.2618  loss_cls_stage2: 0.01846  loss_box_reg_stage2: 0.4012  loss_mask: 0.1055  loss_rpn_cls: 0.003899  loss_rpn_loc: 0.02663    time: 0.7988  last_time: 0.7915  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:51:53 d2.utils.events]: \u001b[0m eta: 0:08:14  iter: 11379  total_loss: 1.178  loss_cls_stage0: 0.01786  loss_box_reg_stage0: 0.2084  loss_cls_stage1: 0.009504  loss_box_reg_stage1: 0.3203  loss_cls_stage2: 0.0183  loss_box_reg_stage2: 0.4488  loss_mask: 0.121  loss_rpn_cls: 0.003116  loss_rpn_loc: 0.03016    time: 0.7988  last_time: 0.7976  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:52:09 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:52:09 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:52:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:52:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:52:09 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:52:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:52:09 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:52:09 d2.utils.events]: \u001b[0m eta: 0:07:58  iter: 11399  total_loss: 1.04  loss_cls_stage0: 0.01511  loss_box_reg_stage0: 0.2032  loss_cls_stage1: 0.005748  loss_box_reg_stage1: 0.2579  loss_cls_stage2: 0.01254  loss_box_reg_stage2: 0.4019  loss_mask: 0.1114  loss_rpn_cls: 0.002984  loss_rpn_loc: 0.02774    time: 0.7988  last_time: 0.8020  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:52:25 d2.utils.events]: \u001b[0m eta: 0:07:42  iter: 11419  total_loss: 1.233  loss_cls_stage0: 0.02546  loss_box_reg_stage0: 0.2043  loss_cls_stage1: 0.01152  loss_box_reg_stage1: 0.3132  loss_cls_stage2: 0.01259  loss_box_reg_stage2: 0.417  loss_mask: 0.1194  loss_rpn_cls: 0.001816  loss_rpn_loc: 0.02763    time: 0.7988  last_time: 0.7883  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:52:40 d2.utils.events]: \u001b[0m eta: 0:07:26  iter: 11439  total_loss: 1.235  loss_cls_stage0: 0.02363  loss_box_reg_stage0: 0.2112  loss_cls_stage1: 0.01001  loss_box_reg_stage1: 0.3156  loss_cls_stage2: 0.0154  loss_box_reg_stage2: 0.4721  loss_mask: 0.09762  loss_rpn_cls: 0.003081  loss_rpn_loc: 0.02951    time: 0.7988  last_time: 0.7966  data_time: 0.0016  last_data_time: 0.0031   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:52:57 d2.utils.events]: \u001b[0m eta: 0:07:10  iter: 11459  total_loss: 1.004  loss_cls_stage0: 0.02061  loss_box_reg_stage0: 0.2165  loss_cls_stage1: 0.00495  loss_box_reg_stage1: 0.2416  loss_cls_stage2: 0.01091  loss_box_reg_stage2: 0.3642  loss_mask: 0.1071  loss_rpn_cls: 0.001555  loss_rpn_loc: 0.03356    time: 0.7988  last_time: 0.7904  data_time: 0.0015  last_data_time: 0.0025   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:53:12 d2.utils.events]: \u001b[0m eta: 0:06:54  iter: 11479  total_loss: 1.33  loss_cls_stage0: 0.02278  loss_box_reg_stage0: 0.2322  loss_cls_stage1: 0.01823  loss_box_reg_stage1: 0.3113  loss_cls_stage2: 0.02519  loss_box_reg_stage2: 0.5032  loss_mask: 0.1247  loss_rpn_cls: 0.002014  loss_rpn_loc: 0.02511    time: 0.7988  last_time: 0.7846  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:53:28 d2.utils.events]: \u001b[0m eta: 0:06:38  iter: 11499  total_loss: 1.154  loss_cls_stage0: 0.01609  loss_box_reg_stage0: 0.211  loss_cls_stage1: 0.007989  loss_box_reg_stage1: 0.261  loss_cls_stage2: 0.01542  loss_box_reg_stage2: 0.4175  loss_mask: 0.1121  loss_rpn_cls: 0.002476  loss_rpn_loc: 0.03883    time: 0.7988  last_time: 0.7926  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:53:44 d2.utils.events]: \u001b[0m eta: 0:06:22  iter: 11519  total_loss: 1.219  loss_cls_stage0: 0.0243  loss_box_reg_stage0: 0.205  loss_cls_stage1: 0.01092  loss_box_reg_stage1: 0.3171  loss_cls_stage2: 0.01327  loss_box_reg_stage2: 0.4808  loss_mask: 0.1146  loss_rpn_cls: 0.001709  loss_rpn_loc: 0.02448    time: 0.7988  last_time: 0.8042  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:54:00 d2.utils.events]: \u001b[0m eta: 0:06:06  iter: 11539  total_loss: 1.08  loss_cls_stage0: 0.02863  loss_box_reg_stage0: 0.2148  loss_cls_stage1: 0.009123  loss_box_reg_stage1: 0.2834  loss_cls_stage2: 0.01448  loss_box_reg_stage2: 0.397  loss_mask: 0.1048  loss_rpn_cls: 0.002468  loss_rpn_loc: 0.0283    time: 0.7988  last_time: 0.8128  data_time: 0.0017  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:54:16 d2.utils.events]: \u001b[0m eta: 0:05:51  iter: 11559  total_loss: 1.023  loss_cls_stage0: 0.02252  loss_box_reg_stage0: 0.2169  loss_cls_stage1: 0.009062  loss_box_reg_stage1: 0.2811  loss_cls_stage2: 0.008937  loss_box_reg_stage2: 0.3867  loss_mask: 0.09191  loss_rpn_cls: 0.002317  loss_rpn_loc: 0.02512    time: 0.7988  last_time: 0.8011  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:54:32 d2.utils.events]: \u001b[0m eta: 0:05:35  iter: 11579  total_loss: 1.492  loss_cls_stage0: 0.02399  loss_box_reg_stage0: 0.2542  loss_cls_stage1: 0.008239  loss_box_reg_stage1: 0.4302  loss_cls_stage2: 0.02466  loss_box_reg_stage2: 0.5819  loss_mask: 0.1208  loss_rpn_cls: 0.003025  loss_rpn_loc: 0.03995    time: 0.7988  last_time: 0.7991  data_time: 0.0019  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:54:48 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:54:48 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:54:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:54:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:54:48 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:54:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:54:48 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:54:48 d2.utils.events]: \u001b[0m eta: 0:05:19  iter: 11599  total_loss: 1.189  loss_cls_stage0: 0.01414  loss_box_reg_stage0: 0.2182  loss_cls_stage1: 0.00781  loss_box_reg_stage1: 0.2751  loss_cls_stage2: 0.01199  loss_box_reg_stage2: 0.3983  loss_mask: 0.1108  loss_rpn_cls: 0.002605  loss_rpn_loc: 0.03269    time: 0.7988  last_time: 0.8032  data_time: 0.0017  last_data_time: 0.0028   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:55:04 d2.utils.events]: \u001b[0m eta: 0:05:03  iter: 11619  total_loss: 1.267  loss_cls_stage0: 0.01848  loss_box_reg_stage0: 0.2415  loss_cls_stage1: 0.01668  loss_box_reg_stage1: 0.3161  loss_cls_stage2: 0.02542  loss_box_reg_stage2: 0.4359  loss_mask: 0.1305  loss_rpn_cls: 0.003909  loss_rpn_loc: 0.03983    time: 0.7988  last_time: 0.7982  data_time: 0.0017  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:55:20 d2.utils.events]: \u001b[0m eta: 0:04:47  iter: 11639  total_loss: 1.366  loss_cls_stage0: 0.02893  loss_box_reg_stage0: 0.2297  loss_cls_stage1: 0.01582  loss_box_reg_stage1: 0.3746  loss_cls_stage2: 0.01056  loss_box_reg_stage2: 0.4585  loss_mask: 0.1179  loss_rpn_cls: 0.003957  loss_rpn_loc: 0.0298    time: 0.7988  last_time: 0.7958  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:55:36 d2.utils.events]: \u001b[0m eta: 0:04:31  iter: 11659  total_loss: 1.142  loss_cls_stage0: 0.03  loss_box_reg_stage0: 0.23  loss_cls_stage1: 0.01442  loss_box_reg_stage1: 0.2794  loss_cls_stage2: 0.01563  loss_box_reg_stage2: 0.4047  loss_mask: 0.1237  loss_rpn_cls: 0.006029  loss_rpn_loc: 0.03222    time: 0.7988  last_time: 0.7864  data_time: 0.0016  last_data_time: 0.0023   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:55:52 d2.utils.events]: \u001b[0m eta: 0:04:15  iter: 11679  total_loss: 1.35  loss_cls_stage0: 0.01941  loss_box_reg_stage0: 0.2335  loss_cls_stage1: 0.009452  loss_box_reg_stage1: 0.3629  loss_cls_stage2: 0.02268  loss_box_reg_stage2: 0.4897  loss_mask: 0.1291  loss_rpn_cls: 0.004477  loss_rpn_loc: 0.03521    time: 0.7988  last_time: 0.7996  data_time: 0.0018  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:56:08 d2.utils.events]: \u001b[0m eta: 0:03:59  iter: 11699  total_loss: 1.17  loss_cls_stage0: 0.02834  loss_box_reg_stage0: 0.2172  loss_cls_stage1: 0.009551  loss_box_reg_stage1: 0.2838  loss_cls_stage2: 0.0125  loss_box_reg_stage2: 0.3847  loss_mask: 0.1077  loss_rpn_cls: 0.008186  loss_rpn_loc: 0.04177    time: 0.7988  last_time: 0.8038  data_time: 0.0014  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:56:24 d2.utils.events]: \u001b[0m eta: 0:03:43  iter: 11719  total_loss: 1.242  loss_cls_stage0: 0.02054  loss_box_reg_stage0: 0.2223  loss_cls_stage1: 0.009742  loss_box_reg_stage1: 0.3193  loss_cls_stage2: 0.01873  loss_box_reg_stage2: 0.4682  loss_mask: 0.1088  loss_rpn_cls: 0.00272  loss_rpn_loc: 0.03178    time: 0.7988  last_time: 0.8008  data_time: 0.0015  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:56:40 d2.utils.events]: \u001b[0m eta: 0:03:27  iter: 11739  total_loss: 1.134  loss_cls_stage0: 0.0222  loss_box_reg_stage0: 0.2187  loss_cls_stage1: 0.02303  loss_box_reg_stage1: 0.2825  loss_cls_stage2: 0.01007  loss_box_reg_stage2: 0.4305  loss_mask: 0.1046  loss_rpn_cls: 0.003306  loss_rpn_loc: 0.01939    time: 0.7988  last_time: 0.8020  data_time: 0.0015  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:56:56 d2.utils.events]: \u001b[0m eta: 0:03:11  iter: 11759  total_loss: 1.184  loss_cls_stage0: 0.02296  loss_box_reg_stage0: 0.2144  loss_cls_stage1: 0.01056  loss_box_reg_stage1: 0.3034  loss_cls_stage2: 0.01183  loss_box_reg_stage2: 0.4248  loss_mask: 0.1115  loss_rpn_cls: 0.00389  loss_rpn_loc: 0.03514    time: 0.7988  last_time: 0.7923  data_time: 0.0014  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:57:12 d2.utils.events]: \u001b[0m eta: 0:02:55  iter: 11779  total_loss: 1.217  loss_cls_stage0: 0.01961  loss_box_reg_stage0: 0.2176  loss_cls_stage1: 0.009213  loss_box_reg_stage1: 0.2901  loss_cls_stage2: 0.01492  loss_box_reg_stage2: 0.4232  loss_mask: 0.1111  loss_rpn_cls: 0.004227  loss_rpn_loc: 0.02622    time: 0.7988  last_time: 0.7931  data_time: 0.0018  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:57:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 13:57:28 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 13:57:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 13:57:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 13:57:28 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 13:57:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 13:57:28 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "\u001b[32m[10/22 13:57:28 d2.utils.events]: \u001b[0m eta: 0:02:39  iter: 11799  total_loss: 1.255  loss_cls_stage0: 0.02397  loss_box_reg_stage0: 0.2126  loss_cls_stage1: 0.01221  loss_box_reg_stage1: 0.3191  loss_cls_stage2: 0.01901  loss_box_reg_stage2: 0.4769  loss_mask: 0.1161  loss_rpn_cls: 0.002841  loss_rpn_loc: 0.02345    time: 0.7988  last_time: 0.8017  data_time: 0.0016  last_data_time: 0.0015   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:57:44 d2.utils.events]: \u001b[0m eta: 0:02:23  iter: 11819  total_loss: 1.091  loss_cls_stage0: 0.01851  loss_box_reg_stage0: 0.2275  loss_cls_stage1: 0.004193  loss_box_reg_stage1: 0.3029  loss_cls_stage2: 0.01039  loss_box_reg_stage2: 0.4001  loss_mask: 0.1301  loss_rpn_cls: 0.007285  loss_rpn_loc: 0.03373    time: 0.7987  last_time: 0.7957  data_time: 0.0016  last_data_time: 0.0013   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:58:00 d2.utils.events]: \u001b[0m eta: 0:02:07  iter: 11839  total_loss: 1.17  loss_cls_stage0: 0.02591  loss_box_reg_stage0: 0.2135  loss_cls_stage1: 0.008873  loss_box_reg_stage1: 0.3004  loss_cls_stage2: 0.0104  loss_box_reg_stage2: 0.4993  loss_mask: 0.1018  loss_rpn_cls: 0.003079  loss_rpn_loc: 0.02756    time: 0.7987  last_time: 0.7991  data_time: 0.0015  last_data_time: 0.0016   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:58:16 d2.utils.events]: \u001b[0m eta: 0:01:51  iter: 11859  total_loss: 1.024  loss_cls_stage0: 0.0168  loss_box_reg_stage0: 0.1916  loss_cls_stage1: 0.01145  loss_box_reg_stage1: 0.2335  loss_cls_stage2: 0.01557  loss_box_reg_stage2: 0.3406  loss_mask: 0.1008  loss_rpn_cls: 0.01335  loss_rpn_loc: 0.03446    time: 0.7987  last_time: 0.8001  data_time: 0.0017  last_data_time: 0.0018   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:58:32 d2.utils.events]: \u001b[0m eta: 0:01:35  iter: 11879  total_loss: 1.144  loss_cls_stage0: 0.01842  loss_box_reg_stage0: 0.2274  loss_cls_stage1: 0.01091  loss_box_reg_stage1: 0.292  loss_cls_stage2: 0.01503  loss_box_reg_stage2: 0.431  loss_mask: 0.1133  loss_rpn_cls: 0.01321  loss_rpn_loc: 0.03874    time: 0.7987  last_time: 0.7833  data_time: 0.0014  last_data_time: 0.0014   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:58:48 d2.utils.events]: \u001b[0m eta: 0:01:19  iter: 11899  total_loss: 1.03  loss_cls_stage0: 0.03089  loss_box_reg_stage0: 0.187  loss_cls_stage1: 0.01399  loss_box_reg_stage1: 0.2509  loss_cls_stage2: 0.01141  loss_box_reg_stage2: 0.3618  loss_mask: 0.1028  loss_rpn_cls: 0.005609  loss_rpn_loc: 0.03056    time: 0.7987  last_time: 0.7979  data_time: 0.0016  last_data_time: 0.0019   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:59:04 d2.utils.events]: \u001b[0m eta: 0:01:03  iter: 11919  total_loss: 1.111  loss_cls_stage0: 0.02664  loss_box_reg_stage0: 0.216  loss_cls_stage1: 0.00712  loss_box_reg_stage1: 0.2572  loss_cls_stage2: 0.01648  loss_box_reg_stage2: 0.3957  loss_mask: 0.1102  loss_rpn_cls: 0.006651  loss_rpn_loc: 0.03017    time: 0.7987  last_time: 0.7930  data_time: 0.0016  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:59:20 d2.utils.events]: \u001b[0m eta: 0:00:47  iter: 11939  total_loss: 1.173  loss_cls_stage0: 0.02471  loss_box_reg_stage0: 0.2222  loss_cls_stage1: 0.009909  loss_box_reg_stage1: 0.3258  loss_cls_stage2: 0.01048  loss_box_reg_stage2: 0.3867  loss_mask: 0.0985  loss_rpn_cls: 0.003857  loss_rpn_loc: 0.02305    time: 0.7987  last_time: 0.7889  data_time: 0.0016  last_data_time: 0.0021   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:59:36 d2.utils.events]: \u001b[0m eta: 0:00:31  iter: 11959  total_loss: 1.394  loss_cls_stage0: 0.03278  loss_box_reg_stage0: 0.2256  loss_cls_stage1: 0.007333  loss_box_reg_stage1: 0.3644  loss_cls_stage2: 0.02514  loss_box_reg_stage2: 0.4872  loss_mask: 0.1195  loss_rpn_cls: 0.003388  loss_rpn_loc: 0.03171    time: 0.7987  last_time: 0.7956  data_time: 0.0015  last_data_time: 0.0012   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 13:59:52 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 11979  total_loss: 1.466  loss_cls_stage0: 0.0197  loss_box_reg_stage0: 0.2462  loss_cls_stage1: 0.01501  loss_box_reg_stage1: 0.3914  loss_cls_stage2: 0.0307  loss_box_reg_stage2: 0.547  loss_mask: 0.1301  loss_rpn_cls: 0.00362  loss_rpn_loc: 0.03338    time: 0.7987  last_time: 0.8117  data_time: 0.0016  last_data_time: 0.0020   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 14:00:08 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 11999  total_loss: 1.041  loss_cls_stage0: 0.02132  loss_box_reg_stage0: 0.1738  loss_cls_stage1: 0.00833  loss_box_reg_stage1: 0.247  loss_cls_stage2: 0.01365  loss_box_reg_stage2: 0.3606  loss_mask: 0.1118  loss_rpn_cls: 0.003058  loss_rpn_loc: 0.03739    time: 0.7987  last_time: 0.7926  data_time: 0.0016  last_data_time: 0.0028   lr: 0.01  max_mem: 1618M\n",
            "\u001b[32m[10/22 14:00:08 d2.engine.hooks]: \u001b[0mOverall training speed: 11998 iterations in 2:39:43 (0.7987 s / it)\n",
            "\u001b[32m[10/22 14:00:08 d2.engine.hooks]: \u001b[0mTotal training time: 2:39:56 (0:00:13 on hooks)\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 14:00:08 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 14:00:08 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 14:00:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 14:00:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 14:00:08 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 14:00:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 14:00:08 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
          ]
        }
      ],
      "source": [
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorBoard is running at http://localhost:6010/\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import os\n",
        "from tensorboard import program\n",
        "\n",
        "# Set the path for the output directory\n",
        "OUTPUT_DIR_PATH = r\"E:\\TA AINGGGG\\1. INI PALING FIXX\\Detectron-Cascade R-CNN\\my_dataset\\cascade_mask_rcnn_R_50_FPN_3x\\2024-10-22-11-18-32\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "if not os.path.exists(OUTPUT_DIR_PATH):\n",
        "    print(f\"Output directory does not exist: {OUTPUT_DIR_PATH}\")\n",
        "else:\n",
        "    # Start TensorBoard\n",
        "    tb = program.TensorBoard()\n",
        "    tb.configure(argv=[None, '--logdir', OUTPUT_DIR_PATH])\n",
        "    url = tb.launch()\n",
        "    print(f'TensorBoard is running at {url}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flInE1L-XTfx"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 20:29:13 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/22 20:29:13 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[10/22 20:29:13 d2.data.datasets.coco]: \u001b[0mLoaded 105 images in COCO format from E:/TA AINGGGG/1. INI PALING FIXX/Detectron-Mask_RCNN/dataset\\valid\\_annotations.coco.json\n",
            "\u001b[32m[10/22 20:29:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[10/22 20:29:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[10/22 20:29:13 d2.data.common]: \u001b[0mSerializing 105 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[10/22 20:29:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
            "\u001b[32m[10/22 20:29:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 105 batches\n",
            "\u001b[32m[10/22 20:29:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/105. Dataloading: 0.0003 s/iter. Inference: 0.2704 s/iter. Eval: 0.0020 s/iter. Total: 0.2726 s/iter. ETA=0:00:25\n",
            "\u001b[32m[10/22 20:29:34 d2.evaluation.evaluator]: \u001b[0mInference done 30/105. Dataloading: 0.0005 s/iter. Inference: 0.2720 s/iter. Eval: 0.0018 s/iter. Total: 0.2744 s/iter. ETA=0:00:20\n",
            "\u001b[32m[10/22 20:29:39 d2.evaluation.evaluator]: \u001b[0mInference done 48/105. Dataloading: 0.0006 s/iter. Inference: 0.2740 s/iter. Eval: 0.0019 s/iter. Total: 0.2766 s/iter. ETA=0:00:15\n",
            "\u001b[32m[10/22 20:29:48 d2.evaluation.evaluator]: \u001b[0mInference done 50/105. Dataloading: 0.0006 s/iter. Inference: 0.4569 s/iter. Eval: 0.0021 s/iter. Total: 0.4596 s/iter. ETA=0:00:25\n",
            "\u001b[32m[10/22 20:29:53 d2.evaluation.evaluator]: \u001b[0mInference done 68/105. Dataloading: 0.0008 s/iter. Inference: 0.4065 s/iter. Eval: 0.0021 s/iter. Total: 0.4096 s/iter. ETA=0:00:15\n",
            "\u001b[32m[10/22 20:29:58 d2.evaluation.evaluator]: \u001b[0mInference done 86/105. Dataloading: 0.0007 s/iter. Inference: 0.3778 s/iter. Eval: 0.0021 s/iter. Total: 0.3808 s/iter. ETA=0:00:07\n",
            "\u001b[32m[10/22 20:30:03 d2.evaluation.evaluator]: \u001b[0mInference done 105/105. Dataloading: 0.0007 s/iter. Inference: 0.3573 s/iter. Eval: 0.0021 s/iter. Total: 0.3602 s/iter. ETA=0:00:00\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.380897 (0.373809 s / iter per device, on 1 devices)\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.357252 s / iter per device, on 1 devices)\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.02 seconds.\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.834\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.966\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.936\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.760\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.865\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.864\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.864\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.781\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.889\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.910\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 83.374 | 96.637 | 93.620 | 75.964 | 86.515 | 87.255 |\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
            "| category     | AP   | category   | AP     |\n",
            "|:-------------|:-----|:-----------|:-------|\n",
            "| defect-crack | nan  | crack      | 83.374 |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "\u001b[32m[10/22 20:30:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
            "\u001b[32m[10/22 20:30:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
            "\u001b[32m[10/22 20:30:05 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
            "\u001b[32m[10/22 20:30:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.851\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.169\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.497\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.334\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.241\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.195\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.449\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.472\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.240\n",
            "\u001b[32m[10/22 20:30:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 33.279 | 85.085 | 16.906 | 49.654 | 33.430 | 24.059 |\n",
            "\u001b[32m[10/22 20:30:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n",
            "| category     | AP   | category   | AP     |\n",
            "|:-------------|:-----|:-----------|:-------|\n",
            "| defect-crack | nan  | crack      | 33.279 |\n",
            "OrderedDict([('bbox', {'AP': 83.37443254778265, 'AP50': 96.63657542224813, 'AP75': 93.6201710040804, 'APs': 75.96444048271638, 'APm': 86.51505240146705, 'APl': 87.25513064577207, 'AP-defect-crack': nan, 'AP-crack': 83.37443254778265}), ('segm', {'AP': 33.27924343478071, 'AP50': 85.08506775402249, 'AP75': 16.905730065954128, 'APs': 49.65429189454772, 'APm': 33.429696703531725, 'APl': 24.059405940594058, 'AP-defect-crack': nan, 'AP-crack': 33.27924343478071})])\n"
          ]
        }
      ],
      "source": [
        "# Mengimpor yang diperlukan dari Detectron2\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "import detectron2\n",
        "import pandas as pd\n",
        "\n",
        "# Siapkan evaluator\n",
        "evaluator = COCOEvaluator(\"my_dataset_valid\", cfg, False, output_dir=\"./output/\")\n",
        "\n",
        "# Membangun DataLoader untuk dataset validasi\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_valid\")\n",
        "\n",
        "# Eksekusi evaluasi\n",
        "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
        "\n",
        "# Menampilkan hasil evaluasi\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AP(bbox)</td>\n",
              "      <td>83.374433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AP50(bbox)</td>\n",
              "      <td>96.636575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AP75(bbox)</td>\n",
              "      <td>93.620171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>APs(bbox)</td>\n",
              "      <td>75.964440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>APm(bbox)</td>\n",
              "      <td>86.515052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>APl(bbox)</td>\n",
              "      <td>87.255131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>AP-defect-crack</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>AP-crack(bbox)</td>\n",
              "      <td>83.374433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>AP(segm)</td>\n",
              "      <td>33.279243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>AP50(segm)</td>\n",
              "      <td>85.085068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AP75(segm)</td>\n",
              "      <td>16.905730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>APs(segm)</td>\n",
              "      <td>49.654292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>APm(segm)</td>\n",
              "      <td>33.429697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>APl(segm)</td>\n",
              "      <td>24.059406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>AP-defect-crack</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>AP-crack(segm)</td>\n",
              "      <td>33.279243</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Metric      Value\n",
              "0          AP(bbox)  83.374433\n",
              "1        AP50(bbox)  96.636575\n",
              "2        AP75(bbox)  93.620171\n",
              "3         APs(bbox)  75.964440\n",
              "4         APm(bbox)  86.515052\n",
              "5         APl(bbox)  87.255131\n",
              "6   AP-defect-crack        NaN\n",
              "7    AP-crack(bbox)  83.374433\n",
              "8          AP(segm)  33.279243\n",
              "9        AP50(segm)  85.085068\n",
              "10       AP75(segm)  16.905730\n",
              "11        APs(segm)  49.654292\n",
              "12        APm(segm)  33.429697\n",
              "13        APl(segm)  24.059406\n",
              "14  AP-defect-crack        NaN\n",
              "15   AP-crack(segm)  33.279243"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Membangun DataFrame dari OrderedDict\n",
        "data = []\n",
        "\n",
        "for metric_type, metrics in results.items():\n",
        "    for metric_name, value in metrics.items():\n",
        "        data.append({\n",
        "            'Metric': f\"{metric_name}({metric_type})\" if metric_name != 'AP-defect-crack' else f\"{metric_name}\",\n",
        "            'Value': value\n",
        "        })\n",
        "\n",
        "df_metrics = pd.DataFrame(data)\n",
        "\n",
        "# Menampilkan DataFrame\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vsByFDFbQwLi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/22 14:38:40 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from my_dataset\\cascade_mask_rcnn_R_50_FPN_3x\\2024-10-22-14-38-04\\model_final.pth ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\anaconda3\\envs\\detectron_env\\lib\\site-packages\\fvcore-0.1.5.post20221221-py3.9.egg\\fvcore\\common\\checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
          ]
        }
      ],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hmAcBbpXX-Rh",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved prediction to: predicted_images\\prediction_0.png\n",
            "Saved prediction to: predicted_images\\prediction_1.png\n",
            "Saved prediction to: predicted_images\\prediction_2.png\n",
            "Saved prediction to: predicted_images\\prediction_3.png\n",
            "Saved prediction to: predicted_images\\prediction_4.png\n",
            "Saved prediction to: predicted_images\\prediction_5.png\n",
            "Saved prediction to: predicted_images\\prediction_6.png\n",
            "Saved prediction to: predicted_images\\prediction_7.png\n",
            "Saved prediction to: predicted_images\\prediction_8.png\n",
            "Saved prediction to: predicted_images\\prediction_9.png\n",
            "Saved prediction to: predicted_images\\prediction_10.png\n",
            "Saved prediction to: predicted_images\\prediction_11.png\n",
            "Saved prediction to: predicted_images\\prediction_12.png\n",
            "Saved prediction to: predicted_images\\prediction_13.png\n",
            "Saved prediction to: predicted_images\\prediction_14.png\n",
            "Saved prediction to: predicted_images\\prediction_15.png\n",
            "Saved prediction to: predicted_images\\prediction_16.png\n",
            "Saved prediction to: predicted_images\\prediction_17.png\n",
            "Saved prediction to: predicted_images\\prediction_18.png\n",
            "Saved prediction to: predicted_images\\prediction_19.png\n",
            "Saved prediction to: predicted_images\\prediction_20.png\n",
            "Saved prediction to: predicted_images\\prediction_21.png\n",
            "Saved prediction to: predicted_images\\prediction_22.png\n",
            "Saved prediction to: predicted_images\\prediction_23.png\n",
            "Saved prediction to: predicted_images\\prediction_24.png\n",
            "Saved prediction to: predicted_images\\prediction_25.png\n",
            "Saved prediction to: predicted_images\\prediction_26.png\n"
          ]
        }
      ],
      "source": [
        "# Define the directory containing the images\n",
        "image_dir = r\"E:\\TA AINGGGG\\dataset\\images\"\n",
        "output_dir = \"predicted_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "# Get a list of all image files in the directory\n",
        "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Process and save predictions\n",
        "for idx, image_file in enumerate(image_files):\n",
        "    img_path = os.path.join(image_dir, image_file)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Ensure the image is loaded\n",
        "    if img is None:\n",
        "        print(f\"Error loading image: {img_path}\")\n",
        "        continue\n",
        "    \n",
        "    outputs = predictor(img)\n",
        "    \n",
        "    visualizer = Visualizer(\n",
        "        img[:, :, ::-1],\n",
        "        metadata=train_metadata,\n",
        "        scale=0.8,\n",
        "        instance_mode=ColorMode.IMAGE_BW\n",
        "    )\n",
        "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    \n",
        "    # Save the resulting image to the output directory\n",
        "    output_file_path = os.path.join(output_dir, f\"prediction_{idx}.png\")\n",
        "    cv2.imwrite(output_file_path, out.get_image()[:, :, ::-1])\n",
        "    \n",
        "    print(f\"Saved prediction to: {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uYGk-zJz4mQF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved prediction to: predicted_images\\prediction_0.png\n",
            "Saved mask to: masks\\mask_0.png\n",
            "Saved prediction to: predicted_images\\prediction_1.png\n",
            "Saved mask to: masks\\mask_1.png\n",
            "Saved prediction to: predicted_images\\prediction_2.png\n",
            "Saved mask to: masks\\mask_2.png\n",
            "Saved prediction to: predicted_images\\prediction_3.png\n",
            "Saved mask to: masks\\mask_3.png\n",
            "Saved prediction to: predicted_images\\prediction_4.png\n",
            "Saved mask to: masks\\mask_4.png\n",
            "Saved prediction to: predicted_images\\prediction_5.png\n",
            "Saved mask to: masks\\mask_5.png\n",
            "Saved prediction to: predicted_images\\prediction_6.png\n",
            "Saved mask to: masks\\mask_6.png\n",
            "Saved prediction to: predicted_images\\prediction_7.png\n",
            "Saved mask to: masks\\mask_7.png\n",
            "Saved prediction to: predicted_images\\prediction_8.png\n",
            "Saved mask to: masks\\mask_8.png\n",
            "Saved prediction to: predicted_images\\prediction_9.png\n",
            "Saved mask to: masks\\mask_9.png\n",
            "Saved prediction to: predicted_images\\prediction_10.png\n",
            "Saved mask to: masks\\mask_10.png\n",
            "Saved prediction to: predicted_images\\prediction_11.png\n",
            "Saved mask to: masks\\mask_11.png\n",
            "Saved prediction to: predicted_images\\prediction_12.png\n",
            "Saved mask to: masks\\mask_12.png\n",
            "Saved prediction to: predicted_images\\prediction_13.png\n",
            "Saved mask to: masks\\mask_13.png\n",
            "Saved prediction to: predicted_images\\prediction_14.png\n",
            "Saved mask to: masks\\mask_14.png\n",
            "Saved prediction to: predicted_images\\prediction_15.png\n",
            "Saved mask to: masks\\mask_15.png\n",
            "Saved prediction to: predicted_images\\prediction_16.png\n",
            "Saved mask to: masks\\mask_16.png\n",
            "Saved prediction to: predicted_images\\prediction_17.png\n",
            "Saved mask to: masks\\mask_17.png\n",
            "Saved prediction to: predicted_images\\prediction_18.png\n",
            "Saved mask to: masks\\mask_18.png\n",
            "Saved prediction to: predicted_images\\prediction_19.png\n",
            "Saved mask to: masks\\mask_19.png\n",
            "Saved prediction to: predicted_images\\prediction_20.png\n",
            "Saved mask to: masks\\mask_20.png\n",
            "Saved prediction to: predicted_images\\prediction_21.png\n",
            "Saved mask to: masks\\mask_21.png\n",
            "Saved prediction to: predicted_images\\prediction_22.png\n",
            "Saved mask to: masks\\mask_22.png\n",
            "Saved prediction to: predicted_images\\prediction_23.png\n",
            "Saved mask to: masks\\mask_23.png\n",
            "Saved prediction to: predicted_images\\prediction_24.png\n",
            "Saved mask to: masks\\mask_24.png\n",
            "Saved prediction to: predicted_images\\prediction_25.png\n",
            "Saved mask to: masks\\mask_25.png\n",
            "Saved prediction to: predicted_images\\prediction_26.png\n",
            "Saved mask to: masks\\mask_26.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define the directory containing the images and output directories\n",
        "image_dir = r\"E:\\TA AINGGGG\\dataset\\images\"\n",
        "output_dir = \"predicted_images\"\n",
        "mask_output_dir = \"masks\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create directory for predicted images\n",
        "os.makedirs(mask_output_dir, exist_ok=True)  # Create directory for masks\n",
        "\n",
        "# Get a list of all image files in the directory\n",
        "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Process each image and save predictions and masks\n",
        "for idx, image_file in enumerate(image_files):\n",
        "    img_path = os.path.join(image_dir, image_file)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Ensure the image is loaded\n",
        "    if img is None:\n",
        "        print(f\"Error loading image: {img_path}\")\n",
        "        continue\n",
        "    \n",
        "    outputs = predictor(img)\n",
        "    \n",
        "    # Create a blank mask for the image\n",
        "    masks = outputs[\"instances\"].pred_masks  # Get the predicted masks\n",
        "\n",
        "    # Initialize a mask array\n",
        "    final_mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    # Combine all instance masks into the final_mask (optional)\n",
        "    for i in range(len(masks)):\n",
        "        single_mask = masks[i].cpu().numpy().astype(np.uint8)  # Convert to numpy array\n",
        "        final_mask = np.maximum(final_mask, single_mask)  # Combine masks\n",
        "\n",
        "    # Save the mask to output directory\n",
        "    mask_output_path = os.path.join(mask_output_dir, f\"mask_{idx}.png\")\n",
        "    cv2.imwrite(mask_output_path, final_mask * 255)  # Scale mask to 255 for visualization\n",
        "\n",
        "    # Visualize and save the original image with masks\n",
        "    visualizer = Visualizer(\n",
        "        img[:, :, ::-1],\n",
        "        metadata=MetadataCatalog.get(\"my_dataset_train\"),\n",
        "        scale=0.8,\n",
        "        instance_mode=ColorMode.IMAGE_BW\n",
        "    )\n",
        "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "    # Save the resulting image with masks\n",
        "    output_file_path = os.path.join(output_dir, f\"prediction_{idx}.png\")\n",
        "    cv2.imwrite(output_file_path, out.get_image()[:, :, ::-1])\n",
        "    \n",
        "    print(f\"Saved prediction to: {output_file_path}\")\n",
        "    print(f\"Saved mask to: {mask_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed mask_0.png:\n",
            "  Length of contour 0: 37.03 mm\n",
            "  Length of contour 1: 65.58 mm\n",
            "Processed mask_1.png:\n",
            "  Length of contour 0: 9.09 mm\n",
            "  Length of contour 1: 43.41 mm\n",
            "  Length of contour 2: 3.18 mm\n",
            "  Length of contour 3: 108.86 mm\n",
            "  Length of contour 4: 40.28 mm\n",
            "  Length of contour 5: 42.05 mm\n",
            "  Length of contour 6: 16.52 mm\n",
            "Processed mask_10.png:\n",
            "  Length of contour 0: 87.44 mm\n",
            "  Length of contour 1: 9.64 mm\n",
            "Processed mask_11.png:\n",
            "  Length of contour 0: 18.92 mm\n",
            "  Length of contour 1: 3.92 mm\n",
            "  Length of contour 2: 4.44 mm\n",
            "  Length of contour 3: 69.62 mm\n",
            "  Length of contour 4: 155.60 mm\n",
            "  Length of contour 5: 38.74 mm\n",
            "  Length of contour 6: 31.89 mm\n",
            "Processed mask_12.png:\n",
            "  Length of contour 0: 4.07 mm\n",
            "  Length of contour 1: 72.33 mm\n",
            "  Length of contour 2: 30.36 mm\n",
            "  Length of contour 3: 31.83 mm\n",
            "Processed mask_13.png:\n",
            "  Length of contour 0: 7.47 mm\n",
            "  Length of contour 1: 41.98 mm\n",
            "  Length of contour 2: 36.32 mm\n",
            "  Length of contour 3: 46.72 mm\n",
            "  Length of contour 4: 0.52 mm\n",
            "  Length of contour 5: 49.47 mm\n",
            "Processed mask_14.png:\n",
            "  Length of contour 0: 38.95 mm\n",
            "  Length of contour 1: 186.52 mm\n",
            "  Length of contour 2: 22.89 mm\n",
            "  Length of contour 3: 44.46 mm\n",
            "Processed mask_15.png:\n",
            "  Length of contour 0: 7.38 mm\n",
            "  Length of contour 1: 12.73 mm\n",
            "  Length of contour 2: 10.89 mm\n",
            "  Length of contour 3: 182.48 mm\n",
            "  Length of contour 4: 18.85 mm\n",
            "  Length of contour 5: 9.06 mm\n",
            "  Length of contour 6: 34.94 mm\n",
            "Processed mask_16.png:\n",
            "  Length of contour 0: 24.51 mm\n",
            "  Length of contour 1: 3.77 mm\n",
            "  Length of contour 2: 60.06 mm\n",
            "  Length of contour 3: 0.52 mm\n",
            "  Length of contour 4: 225.91 mm\n",
            "  Length of contour 5: 31.98 mm\n",
            "  Length of contour 6: 146.03 mm\n",
            "Processed mask_17.png:\n",
            "  Length of contour 0: 46.21 mm\n",
            "  Length of contour 1: 95.65 mm\n",
            "  Length of contour 2: 165.67 mm\n",
            "  Length of contour 3: 66.43 mm\n",
            "Processed mask_18.png:\n",
            "  Length of contour 0: 213.37 mm\n",
            "Processed mask_19.png:\n",
            "  Length of contour 0: 186.42 mm\n",
            "Processed mask_2.png:\n",
            "  Length of contour 0: 13.01 mm\n",
            "  Length of contour 1: 27.41 mm\n",
            "Processed mask_20.png:\n",
            "  Length of contour 0: 5.17 mm\n",
            "  Length of contour 1: 14.36 mm\n",
            "  Length of contour 2: 4.50 mm\n",
            "  Length of contour 3: 37.40 mm\n",
            "  Length of contour 4: 4.01 mm\n",
            "  Length of contour 5: 205.30 mm\n",
            "Processed mask_21.png:\n",
            "  Length of contour 0: 108.00 mm\n",
            "  Length of contour 1: 119.51 mm\n",
            "  Length of contour 2: 32.73 mm\n",
            "Processed mask_22.png:\n",
            "  Length of contour 0: 204.64 mm\n",
            "  Length of contour 1: 55.56 mm\n",
            "Processed mask_23.png:\n",
            "  Length of contour 0: 66.01 mm\n",
            "  Length of contour 1: 72.95 mm\n",
            "  Length of contour 2: 58.41 mm\n",
            "  Length of contour 3: 31.83 mm\n",
            "Processed mask_24.png:\n",
            "  Length of contour 0: 140.22 mm\n",
            "  Length of contour 1: 57.02 mm\n",
            "  Length of contour 2: 5.54 mm\n",
            "  Length of contour 3: 18.97 mm\n",
            "  Length of contour 4: 91.69 mm\n",
            "Processed mask_25.png:\n",
            "  Length of contour 0: 2.30 mm\n",
            "  Length of contour 1: 4.07 mm\n",
            "  Length of contour 2: 2.88 mm\n",
            "  Length of contour 3: 9.49 mm\n",
            "  Length of contour 4: 6.06 mm\n",
            "  Length of contour 5: 4.07 mm\n",
            "  Length of contour 6: 1.04 mm\n",
            "  Length of contour 7: 185.62 mm\n",
            "  Length of contour 8: 26.72 mm\n",
            "  Length of contour 9: 2.14 mm\n",
            "  Length of contour 10: 83.08 mm\n",
            "  Length of contour 11: 0.00 mm\n",
            "Processed mask_26.png:\n",
            "  Length of contour 0: 4.90 mm\n",
            "  Length of contour 1: 100.18 mm\n",
            "  Length of contour 2: 4.07 mm\n",
            "  Length of contour 3: 248.02 mm\n",
            "Processed mask_3.png:\n",
            "  Length of contour 0: 7.16 mm\n",
            "  Length of contour 1: 3.98 mm\n",
            "  Length of contour 2: 36.03 mm\n",
            "  Length of contour 3: 2.36 mm\n",
            "Processed mask_4.png:\n",
            "  Length of contour 0: 20.26 mm\n",
            "  Length of contour 1: 17.97 mm\n",
            "  Length of contour 2: 15.89 mm\n",
            "  Length of contour 3: 130.34 mm\n",
            "  Length of contour 4: 48.69 mm\n",
            "  Length of contour 5: 92.89 mm\n",
            "  Length of contour 6: 55.65 mm\n",
            "Processed mask_5.png:\n",
            "  Length of contour 0: 75.19 mm\n",
            "  Length of contour 1: 8.60 mm\n",
            "  Length of contour 2: 142.29 mm\n",
            "  Length of contour 3: 54.65 mm\n",
            "  Length of contour 4: 98.26 mm\n",
            "Processed mask_6.png:\n",
            "  Length of contour 0: 11.17 mm\n",
            "  Length of contour 1: 16.28 mm\n",
            "  Length of contour 2: 101.27 mm\n",
            "  Length of contour 3: 5.69 mm\n",
            "  Length of contour 4: 97.15 mm\n",
            "  Length of contour 5: 8.05 mm\n",
            "Processed mask_7.png:\n",
            "  Length of contour 0: 4.44 mm\n",
            "  Length of contour 1: 2.51 mm\n",
            "  Length of contour 2: 7.16 mm\n",
            "  Length of contour 3: 7.84 mm\n",
            "  Length of contour 4: 262.05 mm\n",
            "  Length of contour 5: 8.57 mm\n",
            "Processed mask_8.png:\n",
            "  Length of contour 0: 27.71 mm\n",
            "  Length of contour 1: 23.98 mm\n",
            "  Length of contour 2: 132.71 mm\n",
            "Processed mask_9.png:\n",
            "  Length of contour 0: 64.93 mm\n",
            "  Length of contour 1: 47.96 mm\n",
            "  Length of contour 2: 61.53 mm\n",
            "  Length of contour 3: 0.89 mm\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Directory paths\n",
        "input_dir = r'E:\\TA AINGGGG\\1. INI PALING FIXX\\Detectron-Cascade R-CNN\\masks'\n",
        "output_dir = r'E:\\TA AINGGGG\\1. INI PALING FIXX\\Detectron-Cascade R-CNN\\predict_length'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Conversion factor from pixels to mm\n",
        "pixel_to_mm = 0.26\n",
        "\n",
        "# Process each image in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.png'):\n",
        "        # Load the image\n",
        "        path_image = os.path.join(input_dir, filename)\n",
        "        image = cv2.imread(path_image)\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply binary thresholding\n",
        "        _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Initialize a variable to store lengths\n",
        "        lengths = []\n",
        "\n",
        "        # Iterate through contours to process each one\n",
        "        for idx, contour in enumerate(contours):\n",
        "            length = cv2.arcLength(contour, True)  # Calculate contour length in pixels\n",
        "            lengths.append(length)\n",
        "\n",
        "            # Initialize variables to find the furthest points\n",
        "            max_distance = 0\n",
        "            point1 = None\n",
        "            point2 = None\n",
        "\n",
        "            # Find two points that are furthest apart on the contour\n",
        "            for i in range(len(contour)):\n",
        "                for j in range(len(contour)):\n",
        "                    # Calculate distance between all pairs of points\n",
        "                    dist = np.linalg.norm(contour[i][0] - contour[j][0])\n",
        "                    if dist > max_distance:\n",
        "                        max_distance = dist\n",
        "                        point1 = tuple(contour[i][0])\n",
        "                        point2 = tuple(contour[j][0])\n",
        "\n",
        "            # Check if point1 and point2 are found\n",
        "            if point1 is not None and point2 is not None:\n",
        "                # Draw the line on the image between the two furthest points\n",
        "                cv2.line(image, point1, point2, (255, 0, 0), 2)  # Blue line\n",
        "                # Draw circles at the endpoints\n",
        "                cv2.circle(image, point1, 3, (0, 255, 0), -1)  # Green circle at point1\n",
        "                cv2.circle(image, point2, 3, (0, 255, 0), -1)  # Green circle at point2\n",
        "\n",
        "                # Convert length from pixels to mm\n",
        "                length_mm = length * pixel_to_mm\n",
        "                # Prepare text with the length in mm\n",
        "                text = f\"Length: {length_mm:.2f} mm\"  # Format the length to 2 decimal places\n",
        "\n",
        "                # Get the mid point for placing the text, with added y-offset for more spacing\n",
        "                mid_point = ((point1[0] + point2[0]) // 2, (point1[1] + point2[1]) // 2 - 10)  # Y offset\n",
        "        \n",
        "                # Use a smaller font size\n",
        "                cv2.putText(image, text, mid_point, cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)  # Smaller size and yellow text\n",
        "        \n",
        "        # Save the modified image with all contour lengths\n",
        "        output_filename = f\"length_{os.path.splitext(filename)[0]}.png\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "        cv2.imwrite(output_path, image)\n",
        "\n",
        "        # Print the lengths for confirmation\n",
        "        print(f\"Processed {filename}:\")\n",
        "        for i, length in enumerate(lengths):\n",
        "            length_mm = length * pixel_to_mm  # Convert to mm\n",
        "            print(f\"  Length of contour {i}: {length_mm:.2f} mm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined figure saved to results_inference\\combined_results_2_24_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_3_30_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_3_6_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_10_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_11_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_13_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_15_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_16_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_18_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_19_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_20_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_24_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_26_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_27_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_28_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_2_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_30_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_34_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_35_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_36_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_37_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_3_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_40_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_41_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_7_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_8_Crack.jpg\n",
            "Combined figure saved to results_inference\\combined_results_4_9_Crack.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Directory paths (update these as needed)\n",
        "input_directory = r'E:\\TA AINGGGG\\dataset\\images'\n",
        "annotation_directory = r'E:\\TA AINGGGG\\dataset\\labelme'\n",
        "mask_predict_directory = r'E:\\TA AINGGGG\\1. INI PALING FIXX\\Detectron-Cascade R-CNN\\masks'  # Replace with actual path\n",
        "length_predict_directory = r'E:\\TA AINGGGG\\1. INI PALING FIXX\\Detectron-Cascade R-CNN\\predict_length'\n",
        "\n",
        "# Create the results_inference directory if it does not exist\n",
        "output_directory = 'results_inference'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Get all image files in the input directory and sort them\n",
        "image_files = sorted([f for f in os.listdir(input_directory) if f.lower().endswith('.jpg')])\n",
        "\n",
        "# Function to process each .jpg image in the input directory\n",
        "for index, file_name in enumerate(image_files):\n",
        "    original_image_path = os.path.join(input_directory, file_name)\n",
        "\n",
        "    # Construct the mask and length prediction file names using the index\n",
        "    mask_predict_image_path = os.path.join(mask_predict_directory, f'mask_{index}.png')\n",
        "    length_predict_image_path = os.path.join(length_predict_directory, f'length_mask_{index}.png')\n",
        "    annotation_file_path = os.path.join(annotation_directory, file_name.replace('.jpg', '.json'))\n",
        "\n",
        "    # Load the original image\n",
        "    try:\n",
        "        original_image = cv2.imread(original_image_path)\n",
        "        if original_image is None:\n",
        "            print(f\"Error: Could not load the original image from {original_image_path}\")\n",
        "            continue  # Skip processing if the image can't be loaded\n",
        "\n",
        "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    except PermissionError:\n",
        "        print(f\"Permission denied for {original_image_path}\")\n",
        "        continue\n",
        "\n",
        "    # Load the mask prediction image\n",
        "    mask_predict_image = cv2.imread(mask_predict_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if mask_predict_image is None:\n",
        "        print(f\"Error: Could not load the mask prediction image from {mask_predict_image_path}\")\n",
        "\n",
        "    # Load the length prediction image\n",
        "    length_predict_image = cv2.imread(length_predict_image_path)\n",
        "    if length_predict_image is None:\n",
        "        print(f\"Error: Could not load the length prediction image from {length_predict_image_path}\")\n",
        "    else:\n",
        "        length_predict_image = cv2.cvtColor(length_predict_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Load the annotation from LabelMe JSON file\n",
        "    try:\n",
        "        with open(annotation_file_path, 'r') as f:\n",
        "            annotation_data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: JSON annotation file not found at {annotation_file_path}\")\n",
        "        annotation_data = None\n",
        "\n",
        "    # Create the ground truth mask from the annotation\n",
        "    if annotation_data and original_image is not None:\n",
        "        height, width = original_image.shape[:2]\n",
        "        ground_truth_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "        for shape in annotation_data['shapes']:\n",
        "            points = np.array(shape['points'], dtype=np.int32)\n",
        "            cv2.fillPoly(ground_truth_mask, [points], 255)\n",
        "    else:\n",
        "        ground_truth_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "    # Plot all the images in a single row and save the figure to the output directory\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # Title with the input file name\n",
        "    fig.suptitle(f'{file_name}', fontsize=16, fontweight='bold', color='blue')\n",
        "\n",
        "    if original_image is not None:\n",
        "        axs[0].imshow(original_image)\n",
        "        axs[0].set_title('Original Image', color='blue')\n",
        "    else:\n",
        "        axs[0].text(0.5, 0.5, 'Image not found', fontsize=12, ha='center')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow(ground_truth_mask, cmap='gray')\n",
        "    axs[1].set_title('Ground Truth', color='blue')\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    if mask_predict_image is not None:\n",
        "        axs[2].imshow(mask_predict_image, cmap='gray')\n",
        "        axs[2].set_title('Mask Prediction', color='blue')\n",
        "    else:\n",
        "        axs[2].text(0.5, 0.5, 'Mask not found', fontsize=12, ha='center')\n",
        "    axs[2].axis('off')\n",
        "\n",
        "    if length_predict_image is not None:\n",
        "        axs[3].imshow(length_predict_image)\n",
        "        axs[3].set_title('Length Prediction', color='blue')\n",
        "    else:\n",
        "        axs[3].text(0.5, 0.5, 'Image not found', fontsize=12, ha='center')\n",
        "    axs[3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the combined figure to the results_inference directory\n",
        "    combined_output_path = os.path.join(output_directory, f'combined_results_{file_name}')\n",
        "    plt.savefig(combined_output_path)\n",
        "    print(f\"Combined figure saved to {combined_output_path}\")\n",
        "\n",
        "    plt.close(fig)  # Close the figure to free memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **HASIL COMBINE**"
      ]
    },
    {
      "attachments": {
        "combined_results_4_24_Crack-2.jpg": {
          "image/jpeg": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![combined_results_4_24_Crack-2.jpg](attachment:combined_results_4_24_Crack-2.jpg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "detectron_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
